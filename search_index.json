[["index.html", "Stochastyczne modele układów oddziałujących 2024 Notatni do wykładu Sylabus Skrócony plan wykładu Szczegółowy plan wykładu", " Stochastyczne modele układów oddziałujących 2024 Notatni do wykładu Piotr Dyszewski 2024-11-22 Sylabus Dane dotyczące przedmiotu Nazwa przedmiotu: Stochastyczne modele systemów oddziałujących Jednostka oferująca przedmiot: Instytut Matematyczny Założenia: Teoria prawdopodobieństwa 2 (28-MT-S-tTPrawd2) Strona www: https://sites.google.com/site/piotrdyszewski/teaching/SMUO Forma zajęć: wykład + ćwiczenia Punkty ECTS: 6 Skrócony plan wykładu W trakcie wykładu poruszymy następujące zagadnienia: A) Procesy Markowa: łańcuchy Markowa w czasie ciągłym, procesy Fellera, półgrupy operatorów, generatory infinitezymalne, martyngały, rozkłady stacjonarne, ruch Browna i procesy pokrewne B) Układy cząstek: konstrukcja, ergodyczność, model głosowania, model epidemii, exclusion process Podstawowa literatura do wykładu: Liggett, Thomas Milton. Continuous time Markov processes: an introduction. Vol. 113. American Mathematical Soc., 2010. Liggett, Thomas Milton, and Thomas M. Liggett. Interacting particle systems. Vol. 2. New York: Springer, 1985. Liggett, Thomas M. Stochastic interacting systems: contact, voter and exclusion processes. Vol. 324. Springer Science &amp; Business Media, 2013. Szczegółowy plan wykładu Wstępny plan tematów poruszanych na poszczególnych wykładach: Łańcuchy Markowa w czasie ciągłym Procesy Fellera, półgrupy i generatory Od procesu do półgrupy i generatora Od generatora do procesu Konstrukcje generatorów, zagadnienie martyngałowe, rozkłady stacjonarne Procesy dualne Zaburzenia ruchu Browna Konstrukcja systemów oddziałujących Ergodyczność systemów oddziałujących Kilka narzędzi Model głosowania: przypadek rekurencyjny Model głosowania: przypadek tranzytywny Model epidemii: reprezentacja graficzna i addytywność Model epidemii na drzewie jednorodnym Exclusion process Efekty kształcenia Po wykładzie student: Formułuje podstawowe obiekty z zakresu teorii procesów Markowa (A); Podaje przykłady stochastycznych systemów oddziałujących wraz z ich podstawowymi własnościami (B); Formułuje związki między generatorami, półgrupami oraz procesami Markowa (A); Analizuje dowody prostych twierdzeń z wykładu z uzasadnieniem poszczególnych ich założeń (A); Formułuje główne twierdzenia teorii procesów Markowa (A); Analizuje dowody najważniejszych twierdzeń z wykładu z uzasadnieniem poszczególnych ich założeń (A, B); Formułuje główne twierdzenia z zakresu teorii systemów oddziałujących (B); Stosuje teorię procesów Markowa w przykładach (A). Sposób weryfikacji efektów kształcenia Na zaliczenie składać się będą: Aktywność na ćwiczeniach; Zadania domowe. Metody i kryteria oceniania Zaliczenie ćwiczeń na podstawie zadań domowych i aktywności w czasie zajęć. Ocena z egzaminu wystawiona jest na podstawie egzaminu ustnego. Warunkiem zaliczenia przedmiotu jest: Uzyskanie 30% punktów za zadania stanowiące bieżącą weryfikację efektów kształcenia; Uzyskanie pozytywnej oceny z egzaminu stanowiącego końcową weryfikację efektów kształcenia. Kryteria ocen: (dst) student realizuje punkty 1-2 efektów kształcenia (db) student realizuje punkty 1-5 efektów kształcenia (bdb) student realizuje punkty 1-8 efektów kształcenia Wrocław, wrzesień 2024 Piotr Dyszewski "],["wykład-1-łańcuchy-markowa-w-czasie-ciągłym.html", "Wykład 1: Łańcuchy Markowa w czasie ciągłym Podstawowe definicje", " Wykład 1: Łańcuchy Markowa w czasie ciągłym 2024-10-03 Piotr Dyszewski Celem tego rozdziału jest skonstruowanie procesów Markowa w czasie ciągłym na przeliczalnym (lub skończonym) zbiorze \\(S\\) w oparciu o jego opis infinitezymalny. W następnym rozdziale zbadamy problem konstrukcji dla procesów na bardziej ogólnej przestrzeni stanów. Na razie ograniczamy naszą uwagę do bardziej konkretnej sytuacji przeliczalnego \\(S\\). W tym przypadku często używa się słowa “łańcuch” zamiast “proces”. Przypomnijmy, że łańcuchem Markowa w czasie dyskretnym nazywamy proces stochastyczny \\(\\{X_n\\}_{n \\in \\mathbb{N}}\\) takie, że dla dowolnego \\(n \\in \\mathbb{N}\\) i dowolnych \\(s_0, s_1, \\ldots, s_n \\in S\\) takich, że \\[ \\mathbb{P}\\left[ X_{n-1}=s_{n-1}, \\: X_{n-2}=s_{n-2}, \\ldots , X_0=s_0 \\right]&gt;0 \\] zachodzi \\[ \\mathbb{P}\\left[ X_n=s_n \\: | \\: X_{n-1}=s_{n-1}, \\: X_{n-2}=s_{n-2}, \\ldots , X_0=s_0 \\right]= \\mathbb{P}\\left[ X_n=s_n \\: | \\: X_{n-1}=s_{n-1} \\right]. \\] Powyższa własność jest bardzo często przytaczana jako wyjściowa definicja łańcucha Markowa. Mimo swojej prostoty, która ułatwia czytelnikom pierwsze zetknięcie z własnością Markowa, własność ta ma jedną wadę, która ujawnia się przy bardziej zaawansowanych rozważaniach teoretycznych. Jeżeli chcemy badać tylko procesy na przeliczalnej przestrzeni stanów, to jedno naturalne uogólnienie ma następującą formę. Proces stochastyczny w czasie ciągłym \\(\\{X(t)\\}_{t \\in \\mathbb{R}_+}\\) nazwiemy łańcuchem Markowa w czasie ciągłym na przeliczalnej przestrzeni stanów \\(S\\), jeżeli dla dowolnego \\(n\\) i dowolnych \\(0 \\leq t_0 &lt; t_1 &lt; \\ldots &lt; t_n\\) i dowolnych \\(s_0, s_1, \\ldots, s_n \\in S\\) takich, że \\[ \\mathbb{P}\\left[ X(t_{n-1})=s_{n-1}, \\: X(t_{n-2})=s_{n-2}, \\ldots , X(t_0)=s_0 \\right]&gt;0 \\] zachodzi \\[ \\mathbb{P}\\left[ X(t_n)=s_n \\: | \\: X(t_{n-1})=s_{n-1}, \\: X(t_{n-2})=s_{n-2}, \\ldots , X(t_0)=s_0 \\right]= \\mathbb{P}\\left[ X(t_n)=s_n \\: | \\: X(t_{n-1})=s_{n-1} \\right]. \\] Powyższa własność nie jest zbyt przydatna, jeżeli chcemy badać procesy na nieprzeliczalnej przestrzeni stanów. Dla bardzo wielu naturalnych obiektów zmienne losowe \\(X(t)\\) w badanym przez nas procesie mogą mieć rozkład ciągły. Oznacza to, że warunek powyższy nie jest spełniony dla dowolnego wyboru parametrów. W celu znalezienia bardziej elastycznego warunku zauważmy, że własność Markowa dla jednorodnego łańcucha Markowa \\(\\{X_n\\}_{n \\in \\mathbb{N}}\\) w czasie dyskretnym z macierzą przejścia \\[ p(i,j) = \\mathbb{P}[X_1=j \\: | \\: X_0=i] \\] zapisuje się jako \\[ \\mathbb{P}[X_n = j \\: | \\: \\mathcal{F}_n] = p(X_n,j), \\] gdzie \\(\\mathcal{F}_n = \\sigma(X_0, X_1, \\ldots, X_n)\\). Dokładne uzasadnienie powyższej własności pozostawiamy jako zadanie. Podobnie, dla dowolnego \\(m \\in \\mathbb{N}\\), \\[ \\mathbb{P}[X_{n+m} = j \\: | \\: \\mathcal{F}_n] = p^{(m)}(X_n,j), \\] gdzie \\((p^{(m)(i,j) })_{i, j \\in S}\\) jest \\(m\\)-tą potęgą macierzy przejścia \\[ p^{(m)}(i,j) = \\mathbb{P}[X_m=j \\: | \\: X_0=i]. \\] Oznacza to, że dla dowolnej funkcji mierzalnej \\(f \\colon S \\to \\mathbb{R}\\), \\[ \\mathbb{E}[ f(X_{n+m}) \\: | \\: \\mathcal{F}_n] = \\sum_{s \\in S} f(s) p^{(m)}(X_n, s). \\] Powyższa definicja względnie łatwo zapisuje się w czasie ciągłym \\[ \\mathbb{E}[ f(X_{s+t}) \\: | \\: \\mathcal{F}_s] = \\sum_{x\\in S} f(x) p^{(t)}(X_n, x), \\] gdzie \\[ p^{(t)}(y,x) = \\mathbb{P}[X_t=x | X_0=y]. \\] Relacja powyższa daje się zapisać w przypadku nieprzeliczalnej przestrzeni stanów jako \\[ \\mathbb{E}[ f(X_{s+t}) \\: | \\: \\mathcal{F}_s] = \\int_S f(x) p^{(t)}(X_n, \\mathrm{d} x), \\] gdzie \\[ p^{(t)}(y, \\mathrm{d} x) = \\mathbb{P}[X_t \\in \\mathrm{d} x | X_0=y] \\] jest rozkładem \\(X_t\\) pod warunkiem \\(\\{X_0=y\\}\\). To niesie ze sobą kolejne problemy, ponieważ jak wcześniej zauważyliśmy \\(\\{X_0=y\\}\\) może być zdarzeniem o prawdopodobieństwie zero. Przedstawione podejście jest do uratowania pod kątem formalnym przez odniesienie się do regularnych rozkładów warunkowych. Zamiast tego podejdziemy do problemu od innej strony. Naszym punktem wyjścia będzie odpowiednia rodzina miar. Jak zobaczymy wkrótce, to podejście będzie również opierało się o odpowiednik powyższej relacji. Oznacza to, że w rezultacie będziemy opisywali tę samą klasę procesów stochastycznych bez konieczności obchodzenia się z warunkowaniem po zdarzeniach niemożliwych. Podstawowe definicje Zaczynamy od prezentacji definicji trzech obiektów, na których się skupimy w tym rozdziale. Przypomnijmy, że będziemy definiować łańcuchy Markowa w czasie ciągłym na dyskretnej przestrzeni stanów \\(S\\). Topologia na \\(S\\) to oczywiście topologia dyskretna, względem której wszystkie funkcje są ciągłe. Głównym obiektem naszych badań będą procesy stochastyczne. Ze względów technicznych pracować będziemy na bardzo konkretnej przestrzeni zdarzeń elementarnych. Niech \\(\\Omega\\) będzie zbiorem prawostronnie ciągłych funkcji \\(\\omega \\colon [0, \\infty) \\to S\\) ze skończoną liczbą skoków w dowolnym skończonym przedziale czasowym. Dla każdego \\(t \\in \\mathbb{R}_+ = [0, +\\infty)\\) rozważmy funkcję \\(X_t \\colon \\Omega \\to S\\) zadaną przez \\[ X_t(\\omega) = \\omega(t). \\] Niech \\(\\sigma\\)-ciało \\(\\mathcal{F}\\) na \\(\\Omega\\) będzie najmniejszym takim, że odwzorowanie \\(\\omega \\mapsto \\omega(t)\\) jest mierzalne dla każdego \\(t \\in \\mathbb{R}_+ = [0, +\\infty)\\). Niech wreszcie dla \\(s \\in \\mathbb{R}_+\\) oznaczmy przez \\(\\theta_s\\) odwzorowanie \\(\\Omega \\to \\Omega\\) zadane przez \\[ \\theta_s(\\omega)(t) = \\omega(t+s). \\] W szczególności \\(X_t \\circ \\theta_s = X_{t+s}\\). O odwzorowaniu \\(\\theta_s\\) można myśleć jak o przesunięciu czasu o \\(s\\). Zamiast utożsamiać własność Markowa z procesem stochastycznym \\(X=\\{X(t)\\}_{t \\in \\mathbb{R}_+}\\), utożsamimy ją z rodziną miar probabilistycznych na \\(\\Omega\\), względem której \\(X\\) będzie procesem Markowa. Definicja 1 Łańcuchem Markowa w czasie ciągłym na przestrzeni stanów \\(S\\) nazywamy parę uporządkowaną \\(( \\mathbf{P}, \\mathbb{F})\\) taką, że (ŁM1) \\(\\mathbb{F}=(\\mathcal{F}_t)_{t \\in \\mathbb{R}_+}\\) jest filtracją względem której \\(X = (X(t))_{t \\in \\mathbb{R}_+}\\) jest adaptowalny i \\(\\mathcal{F}_t \\subseteq \\mathcal{F}\\) dla każdego \\(t \\in \\mathbb{R}_+\\). (ŁM2) \\(\\mathbf{P}= \\{ \\mathbf{P}_x\\}_{x \\in S}\\). Dla każdego \\(x \\in S\\), \\(\\mathbf{P}_x\\) jest miarą probabilistyczną na \\(\\Omega\\) taką, że \\[ \\mathbf{P}_x[X_0 = x] = \\mathbf{P}_x[\\omega \\in \\Omega : \\omega(0)=x]= 1. \\] (ŁM3) Spełniona jest własność Markowa \\[ \\mathbf{E}_x[Y \\circ \\theta_s | \\mathcal{F}_s] = \\mathbf{E}_{X(s)}[Y] \\text{ p.n. } \\mathbf{P}_x \\] dla wszystkich \\(x \\in S\\) i wszystkich ograniczonych mierzalnych \\(Y\\) na \\(\\Omega\\). W powyższej definicji \\(\\mathbf{E}_x\\) jest wartością oczekiwaną odpowiadającą mierze probabilistycznej \\(\\mathbf{P}_x\\), czyli \\[ \\mathbf{E}_x[Y] = \\int_\\Omega Y(\\omega) \\: \\mathbf{P}_x(\\mathrm{d} \\omega). \\] O zmiennej losowej \\(Y \\colon \\Omega \\to \\mathbb{R}\\) można myśleć jak o statystyce całej trajektorii procesu \\(\\{X(t)\\}_{t \\in \\mathbb{R}_+}\\). Załóżmy, że \\(Y\\) jest postaci \\[ Y(\\omega) = f(\\omega(t_1), \\omega(t_2), \\ldots , \\omega(t_n)) = f(X(t_1), X(t_2), \\ldots, X(t_n)) \\] dla pewnej mierzalnej i ograniczonej funkcji \\(f \\colon \\mathbb{R}^n \\to \\mathbb{R}\\) (dla ćwiczenia warto sprawdzić, że \\(Y\\) powyższej postaci jest istotnie zmienną losową, tj. jest mierzalna względem \\(\\mathcal{F}\\)). Wówczas \\[ Y\\circ \\theta_s(\\omega) = f(\\omega(t_1+s), \\omega(t_2+s), \\ldots , \\omega(t_n+s)) = f(X(t_1+s), X(t_2+s), \\ldots, X(t_n+s)). \\] Własność Markowa w Definicji mówi zatem to, co powinna. Rozkład wektora losowego \\((X(t_1+s), X(t_2+s), \\ldots, X(t_n+s))\\) pod warunkiem \\(\\mathcal{F}_s\\) jest taki sam, jak rozkład wektora \\((X&#39;(t_1), X&#39;(t_2), \\ldots, X&#39;(t_n))\\) dla \\(X&#39;=\\{X&#39;(t)\\}_{t\\in \\mathbb{R}_+}\\) będącym niezależną kopią \\(X\\), zapoczątkowaną w \\(X_0&#39;=X(s)\\). Naszym nadrzędnym celem będzie sprowadzenie powyższej definicji do bardziej przystępnych terminów. Zanim jednak do tego przejdziemy, rozważmy następujący przykład. Przykład 1 Niech \\(N=(N_t)_{t \\in \\mathbb{R}_+}\\) będzie jednorodnym procesem Poissona z intensywnością \\(\\lambda &gt;0\\) określonym na przestrzeni probabilistycznej \\((\\Sigma, \\mathcal{G}, \\mathbb{P})\\). Przypomnijmy, że oznacza to, że \\(t \\mapsto N_t\\) jest prawostronnie ciągła oraz dla każdego \\(t&gt;0\\) zmienna losowa \\(N_t\\) ma rozkład Poissona z parametrem \\(\\lambda&gt;0\\), \\[ \\mathbb{P}[N_t=k] = e^{-\\lambda} \\frac{\\lambda^k}{k!}, \\] i wreszcie dla \\(t&gt;s\\geq 0\\), zmienna \\(N_t - N_s\\) jest niezależna od \\(\\sigma\\)-ciała \\(\\mathcal{F}_s^N = \\sigma(N_r : r \\leq s)\\). Uzasadnimy, że \\(N\\) jest procesem Markowa w sensie przyjętej przez nas definicji. Przestrzenią stanów jest \\(S = \\mathbb{N}\\). Niech \\(\\mathbf{P}_x[A] = \\mathbb{P}[N + x \\in A]\\) dla \\(A \\in \\mathcal{F}\\). Tutaj \\(x+N\\) oznacza funkcję \\(t \\mapsto N_t + x\\). Pokażemy teraz, że spełniona jest własność Markowa. Dla \\(A \\in \\mathcal{F}_s\\) mamy \\[ \\mathbf{E}_x[Y \\circ \\theta_s \\cdot \\mathbf{1}_{A}] = \\mathbb{E}[Y(N \\circ \\theta_s + x) \\mathbf{1}_{\\{N \\in A\\}}] = \\mathbb{E}[Y(N \\circ \\theta_s - N_s + N_s + x) \\mathbf{1}_{\\{N \\in A\\}}]. \\] Skoro \\(A \\in \\mathcal{F}_s\\), to \\(\\{N \\in A\\} \\in \\mathcal{F}_s^N\\) (zadanie). Skoro \\(N \\circ \\theta_s - N_s = (N_{t+s} - N_s)_{t \\in \\mathbb{R}_+}\\) jest niezależny od \\(\\mathcal{F}_s^N\\), to \\[ \\mathbb{E}[Y(N \\circ \\theta_s - N_s + N_s + x) \\mathbf{1}_{\\{N \\in A\\}} | \\mathcal{F}_s] = \\mathbf{1}_{\\{N \\in A\\}} \\cdot y(N_s + x), \\] gdzie \\[ y(k) = \\mathbb{E}[Y(N \\circ \\theta_s - N_s + k)] = \\mathbf{E}_k[Y]. \\] Podsumowując, \\[ \\mathbf{E}_x[Y \\circ \\theta_s \\cdot \\mathbf{1}_{A}] = \\mathbb{E}[y(N_s + x) \\mathbf{1}_{\\{N \\in A\\}}] = \\mathbf{E}_x[y(X(s)) \\mathbf{1}_A] \\] odwołując się teraz do definicji warunkowej wartości oczekiwanej \\[ \\mathbf{E}_x[Y \\circ \\theta_s | \\mathcal{F}_s] = y(X(s)) = \\mathbf{E}_{X(s)}[Y]. \\] Powyższy przykład pokazuje, że uzasadnienie własności Markowa wprost z definicji nie jest najprostszym zadaniem. Przekonamy się w przyszłości, że taka forma jest przydatna do teoretycznych rozważań. Mimo to przyda się nam bardziej przystępny sposób mówienia o łańcuchach Markowa w czasie ciągłym. Odnosząc się do czasu dyskretnego, korzystać będziemy z funkcji przejścia. Definicja 2 Funkcją przejścia nazywamy rodzinę odwzorowań \\(p = (p_t)_{t \\in \\mathbb{R}_+}\\), gdzie \\(p_t \\colon S \\times S \\to [0,1]\\) zdefiniowanych dla \\(t \\ge 0\\) takich, że \\[ p_t(x, y) \\ge 0, \\quad \\sum_{y \\in S} p_t(x, y) = 1, \\quad \\lim_{t \\to 0} p_t(x, x) = p_0(x, x) = 1, \\] spełniających równania Chapmana-Kolmogorowa \\[ p_{s+t}(x, y) = \\sum_{z \\in S} p_s(x, z)p_t(z, y). \\] Interpretacją wartości \\(p_t(x,y)\\) jest prawdopodobieństwo, że w czasie \\(t\\) proces przejdzie ze stanu \\(x\\) do stanu \\(y\\). Innymi słowy, \\[ p_t(x,y) = \\mathbf{P}_x[X_t = y]. \\] Jak się niebawem przekonamy, dzięki własności Markowa pozwala ona jednoznacznie wyznaczyć rozkład procesu, tj. jednoznacznie wyznaczyć miarę \\(\\mathbf{P}_x\\). Przykład 2 Rozważmy \\(S = \\mathbb{N}\\), \\(\\lambda &gt; 0\\) oraz \\(p_t \\colon S \\times S \\to [0,1]\\) zadane przez \\[ p_t(x, y) = e^{-\\lambda t} \\frac{(\\lambda t)^{y - x}}{(y - x)!} \\mathbf{1}_{\\{y \\geq x\\}}. \\] Wówczas \\(p\\) jest funkcją przejścia. Wystarczy zauważyć, że \\(p_t(x)\\) to prawdopodobieństwo, że zmienna losowa o rozkładzie Poissona z parametrem \\(\\lambda t\\) jest równa \\(y - x\\). Równania Chapmana-Kołmogorowa wynikają z następującej własności rozkładu Poissona: jeżeli niezależne zmienne losowe \\(X\\) i \\(Y\\) mają rozkłady Poissona odpowiednio z parametrami \\(\\lambda t\\) i \\(\\lambda s\\), to \\(X + Y\\) ma rozkład Poissona z parametrem \\(\\lambda (t + s)\\). Twierdzenie 1 Dla łańcucha Markowa \\((\\mathbf{P}, \\mathbb{F})\\) połóżmy \\[ p_t(x, y) = \\mathbf{P}_x[X_t = y] \\] dla \\(t \\ge 0\\) oraz \\(x, y \\in S\\). Wówczas: \\(p_t(x, y)\\) jest funkcją przejścia, \\(p_t(x, y)\\) określa miary \\(\\mathbf{P}_x\\) jednoznacznie. Proof. Najpierw pokażemy, że \\(\\lim_{t \\to 0} p_t(x, x) = 1\\). Przez prawostronną ciągłość ścieżek, \\(\\tau = \\inf \\{ t &gt; 0 : X_t \\neq X_0 \\} &gt; 0\\) \\(\\mathbf{P}_x\\)-p.w. dla dowolnego \\(x\\) z \\(S\\). Ponieważ \\[p_t(x, x) \\ge \\mathbf{P}_x[\\tau &gt; t] \\to 1\\] przy \\(t \\to 0\\), to istotnie \\(\\lim_{t \\to 0} p_t(x, x) = 1\\). Równania Chapmana-Kołmogorowa wynikają z własności Markowa. Aby to zobaczyć, rozważmy \\(Y = 1_{\\{ X(t)=y \\}}\\). Własność Markowa zapisuje się jako \\[ \\mathbf{P}_x[X_{s+t} = y | \\mathcal{F}_s] = \\mathbf{P}_{X(s)}[X_t = y] = p_t(X(s), y) \\text{ p.n. } \\mathbf{P}_x. \\] Biorąc wartości oczekiwane względem \\(\\mathbf{P}_x\\) w tej tożsamości, otrzymujemy równania Chapmana-Kołmogorowa: \\[ \\mathbf{P}_x[X_{t+s}=y] = \\mathbf{E}_x \\left[\\mathbf{P}_x[X_{s+t} = y | \\mathcal{F}_s] \\right] = \\mathbf{E}_x\\left[ p_t(X(s), y) \\right] = \\sum_{z \\in S } p_t(z, y) p_s(x, z). \\] To dowodzi a. b. Użyjemy własności Markowa wielokrotnie, aby otrzymać \\[ \\mathbf{P}_x[X(t_1) = x_1, \\ldots, X(t_n) = x_n] = p_{t_1}(x, x_1) p_{t_2 - t_1}(x_1, x_2) \\cdots p_{t_n - t_{n-1}}(x_{n-1}, x_n) \\] dla \\(0 &lt; t_1 &lt; \\ldots &lt; t_n\\) oraz \\(x_1, \\ldots, x_n \\in S\\). Aby to zobaczyć oznaczmy \\[ \\mathcal{H}_{n-1} = \\{ X(t_1) = x_1, \\ldots, X(t_{n-1}) = x_{n-1} \\} \\in \\mathcal{F}_{t_{n-1}}. \\] Z własności Markowa \\[ \\mathbf{P}_x[X(t_1) = x_1, \\ldots, X(t_n) = x_n | \\mathcal{F}_{t_{n-1}}] = \\mathbf{P}_x[\\mathcal{H}_{n-1}, X(t_n) = x_n | \\mathcal{F}_{t_{n-1}}] \\] \\[ = \\mathbf{1}_{\\mathcal{H}_{n-1}} \\mathbf{P}_x[X(t_n) = x_n | \\mathcal{F}_{t_{n-1}}] = \\mathbf{1}_{\\mathcal{H}_{n-1}} \\mathbf{P}_x[X \\circ \\theta_{t_{n-1}} (t_n - t_{n-1}) = x_n | \\mathcal{F}_{t_{n-1}}] \\] \\[ = \\mathbf{1}_{\\mathcal{H}_{n-1}} \\mathbf{P}_{X(t_{n-1})}[X (t_n - t_{n-1}) = x_n] = \\mathbf{1}_{\\mathcal{H}_{n-1}} p_{t_n - t_{n-1}}(x_{n-1}, x_n). \\] Biorąc wartości oczekiwane, \\[ \\mathbf{P}_x[X(t_1) = x_1, \\ldots, X(t_n) = x_n] = \\mathbf{P}_x[\\mathcal{H}_{n-1}] p_{t_n - t_{n-1}}(x_{n-1}, x_n). \\] Postulowaną równość otrzymujemy przez iterację powyższej procedury. Udowodniona właśnie równość uzasadnia, że funkcja przejścia określa rozkłady skończenie wymiarowe \\(\\mathbf{P}_x\\). Określa również pełną miarę \\(\\mathbf{P}_x\\), ponieważ miary prawdopodobieństwa na \\((\\Omega, \\mathcal{F})\\) są określane przez ich skończenie wymiarowe rozkłady w świetle twierdzenia \\(\\pi - \\lambda\\). Aby to zobaczyć, załóżmy, że \\(\\mu\\) oraz \\(\\nu\\) to dwie takie miary, które mają te same skończenie wymiarowe rozkłady, i niech \\(\\mathcal{P}\\) będą skończenie wymiarowymi zbiorami w \\((\\Omega, \\mathcal{F})\\) oraz \\[ \\mathcal{L} = \\{ A \\in \\mathcal{F} : \\mu(A) = \\nu(A) \\}. \\] Wówczas \\(\\mathcal{L}\\) jest \\(\\lambda\\)-układem zawierającym \\(\\pi\\)-układ \\(\\mathcal{P}\\). Przez twierdzenie \\(\\pi - \\lambda\\), \\(\\sigma(\\mathcal{P}) \\subseteq \\mathcal{L}\\). Skoro \\(\\sigma(\\mathcal{P}) = \\mathcal{F}\\), to \\(\\mu(A) = \\nu(A)\\) dla wszystkich \\(A \\in \\mathcal{F}\\). Skoro \\(X\\) jest elementem \\(\\Omega\\), zbioru funkcji prawostronnie ciągłych o wartościach w przeliczalnym \\(S\\), to \\(X\\) jest funkcją kawałkami stałą. W rezultacie do opisu \\(X\\) wystarczy sprecyzować, w jaki sposób \\(X\\) zmienia wartość. Opis ten jest dokonywany w kategoriach \\(Q\\)-macierzy. Definicja 3 \\(Q\\)-macierzą nazywamy macierz \\((q(x, y))_{x,y \\in S}\\) liczb rzeczywistych indeksowanych przez \\(x, y \\in S\\), które spełniają \\[ q(x, y) \\ge 0 \\text{ dla } x \\neq y \\quad \\text{oraz} \\quad \\sum_y q(x, y) = 0. \\] Ponieważ wyrazy diagonalne są niedodatnie i odgrywają specjalną rolę, naturalne jest użycie specjalnego oznaczenia dla nich: \\[ c(x) = -q(x, x). \\] Przejście od funkcji przejścia do \\(Q\\)-macierzy jest trudniejsze niż przejście od procesu do funkcji przejścia i wymaga dokonania dodatkowych założeń. Zaczynamy od kilku własności, które obowiązują dla wszystkich funkcji przejścia. Twierdzenie 2 Załóżmy, że \\(p\\) jest funkcją przejścia. Wówczas \\(p_t(x, x) &gt; 0\\) dla wszystkich \\(t \\ge 0\\) oraz \\(x \\in S\\). Jeśli \\(p_t(x, x) = 1\\) dla pewnego \\(t &gt; 0\\) oraz \\(x \\in S\\), wtedy \\(p_t(x, x) = 1\\) dla wszystkich \\(t &gt; 0\\) oraz tego \\(x\\). Dla każdego \\(x, y \\in S\\), \\(p_t(x, y)\\) jest jednostajnie ciągła w \\(t\\). Dokładniej, \\[ |p_t(x, y) - p_s(x, y)| \\leq 1 - p_{|t-s|}(x, x). \\] Proof. Najpierw zauważmy, że \\(p_t(x, x) &gt; 0\\) dla małych \\(t\\) zgodnie z przyjętą Definicją 2 Z równań Chapmana-Kołmogorowa, \\[ p_{s+t}(x, x) \\geq p_s(x, x)p_t(x, x), \\] więc ścisła dodatniość rozciąga się na wszystkie \\(t\\). Użyjmy równania Chapmana-Kołmogorowa raz jeszcze, aby napisać \\[ p_{s+t}(x, x) \\leq p_s(x, x)p_t(x, x) + [1 - p_s(x, x)] = 1 - p_s(x, x)[1 - p_t(x, x)]. \\] Zatem, jeśli \\(p_{s+t}(x, x) = 1\\), to \\(p_t(x, x) = 1\\), ponieważ \\(p_s(x, x) &gt; 0\\) z części a. Stąd \\(\\{ t \\ge 0 : p_t(x, x) = 1 \\}\\) jest przedziałem zaczynającym się od 0. Z dowodu części a. wynika, że musi to być cała dodatnia oś. Ponownie użyjmy równania Chapmana-Kołmogorowa, aby napisać \\[ p_{t+s}(x, y) - p_t(x, y) = p_t(x, y)[p_s(x, x) - 1] + \\sum_{z \\neq x} p_s(x, z)p_t(z, y). \\] Pierwszy składnik po prawej stronie jest niedodatni, a drugi jest nieujemny. Wartość bezwzględna każdego z nich nie jest większa niż \\(1 - p_s(x, x)\\). To pociąga postulowaną nierówność, która z kolei pociąga jednostajną ciągłość. Twierdzenie 3 Załóżmy, że \\(p\\) jest funkcją przejścia. Dla każdego \\(x\\), prawostronna pochodna \\[ c(x) = -q(x, x) = - \\left. \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, x) \\right|_{t=0} \\in [0, \\infty] \\] istnieje i spełnia \\[ p_t(x, x) \\ge e^{-c(x)t}. \\] Jeśli \\(c(x) &lt; \\infty\\), wtedy dla tego \\(x\\) i dla wszystkich \\(y \\neq x\\), prawostronna pochodna \\[ q(x, y) = \\left. \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, y) \\right|_{t=0} \\in [0, \\infty) \\] istnieje oraz \\[ \\sum_{y \\in S} q(x, y) \\leq 0. \\] Jeśli dla pewnego \\(x \\in S\\), \\(c(x) &lt; \\infty\\) i \\(\\sum_y q(x, y) = 0\\), to \\(p_t(x, y)\\) jest różniczkowalna w sposób ciągły względem \\(t\\) dla tego \\(x\\) i każdego \\(y\\), oraz spełnia równania retrospektywne Kołmogorowa: \\[ \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, y) = \\sum_z q(x, z)p_t(z, y). \\] Proof. Niech \\(f(t) = - \\log p_t(x, x)\\). Wówczas \\(f\\) jest dobrze określona i jednostajnie ciągła. Jest ona też subaddytywna. Zatem, zgodnie z Lematem Fekete, \\[ c(x) = \\lim_{t \\to 0} \\frac{f(t)}{t} \\in [0, \\infty] \\] i spełnia \\(f(t) \\leq c(x)t\\). Przypuśćmy, że \\(c(x) &lt; \\infty\\). Z dowiedzionej właśnie nierówności, \\[ 1 - p_t(x, x) \\leq 1 - e^{-c(x)t} \\leq c(x)t, \\] i stąd \\[ \\sum_{y : y \\neq x} \\frac{p_t(x, y)}{t} \\leq c(x). \\] Zatem, \\[ \\limsup_{t \\to 0} \\frac{p_t(x, y)}{t} &lt; \\infty \\tag{1} \\] dla \\(y \\neq x\\). Niech \\(q(x, y)\\) będzie wartością powyższej granicy górnej. Aby pokazać, że granica rzeczywiście istnieje, weźmy \\(\\delta &gt; 0\\) i dodatnią liczbę całkowitą \\(n\\). Macierz \\(p_{\\delta}(x, y)\\) można traktować jako prawdopodobieństwa przejścia dla dyskretnego łańcucha Markowa na \\(S\\), a wtedy zgodnie z równaniami Chapmana-Kołmogorowa, odpowiadające \\(n\\)-krokowe prawdopodobieństwa przejścia są dane przez \\(p_{n\\delta}(x, y)\\). Rozkładając zdarzenie, że ten łańcuch znajduje się w \\(y\\) w czasie \\(n\\) zgodnie z czasem pierwszej wizyty w \\(y\\), mamy dla \\(y \\neq x\\), \\[ p_{n\\delta}(x, y) \\geq \\sum_{k=0}^{n-1} p_{k\\delta}^k(x, x) p_{\\delta}(x, y)p_{(n-k-1)\\delta}(y, y). \\] Stąd \\[ \\frac{p_{n\\delta}(x, y)}{n\\delta} \\geq \\frac{p_{\\delta}(x, y)}{\\delta} e^{-c(x)n\\delta} \\inf_{0 \\leq s \\leq n\\delta} p_s(y, y). \\] Teraz niech \\(\\delta \\downarrow 0\\) wzdłuż ciągu realizującego granicę górną w (1), tak że \\[ \\frac{p_{\\delta}(x, y)}{\\delta} \\to q(x, y). \\] Wybierzmy teraz \\(n \\to \\infty\\) tak, że \\(n\\delta \\to t\\). Wówczas \\[ \\frac{p_t(x, y)}{t} \\geq q(x, y)e^{-c(x)t} \\inf_{0 \\leq s \\leq t} p_s(y, y) \\] dla \\(t &gt; 0\\). Zatem, \\[ \\liminf_{t \\to 0} \\frac{p_t(x, y)}{t} \\geq q(x, y). \\] Postulowana nierówność wynika teraz z lematu Fatou. Napiszmy \\[ \\frac{p_{t+s}(x, y) - p_t(x, y)}{s} = \\sum_z \\left[ \\frac{p_s(x, z) - p_0(x, z)}{s} - q(x, z) \\right] p_t(z, y). \\] Każdy wyraz w sumie dąży do \\(0\\), gdy \\(s \\downarrow 0\\), zgodnie z pierwszymi dwoma częściami twierdzenia. Zatem musimy kontrolować ogony sumy. Weźmy skończony zbiór \\(T \\subset S\\) zawierający \\(x\\) i zauważmy, że \\[ \\sum_{z \\notin T} \\left| \\frac{p_s(x, z)}{s} - q(x, z) \\right| p_t(z, y) \\leq \\sum_{z \\notin T} \\frac{p_s(x, z)}{s} + \\sum_{z \\notin T} q(x, z) \\] \\[ = s^{-1} \\left[ 1 - \\sum_{z \\in T} p_s(x, z) \\right] - \\sum_{z \\in T} q(x, z) \\to -2 \\sum_{z \\in T} q(x, z) \\] gdy \\(s \\downarrow 0\\). Granicę po prawej stronie można uczynić dowolnie małą, wybierając \\(T\\) duże, ponieważ \\(\\sum_z q(x, z) = 0\\). Zatem prawa strona dąży do zera, gdy \\(s \\downarrow 0\\). To dowodzi, że prawostronne pochodne \\(p_t(x, y)\\) istnieją i spełniają zadaną równość. Aby zobaczyć, że obustronne pochodne rzeczywiście istnieją, wystarczy zauważyć, że prawa strona jest ciągła w \\(t\\) i użyć faktu, że funkcja ciągła z ciągłą prawostronną pochodną jest różniczkowalna. "],["wykład-2-mocna-własność-markowa.html", "Wykład 2: Mocna własność Markowa Czasy zatrzymania Charakteryzacja", " Wykład 2: Mocna własność Markowa 2024-10-10 Piotr Dyszewski Pojęcie czasu zatrzymania odgrywa kluczową rolę w teorii procesów stochastycznych. Są to losowe momenty adaptowalne do z góry zadanej filtracji. Jest to kluczowa koncepcj w silnej własności Markowa. Będziemy korzystać z ciągłej filtracji \\(\\mathbb{F}=( \\mathcal{F}_t )_{t \\in \\mathbb{R}_+}\\). Czasy zatrzymania Przypomnijmy, że w czasie dyskretnym definicja czasu zatrzymania \\(\\tau\\) jest taka, że \\(\\{ \\tau = n \\} \\in \\mathcal{F}_n\\) dla każdego naturalnego \\(n\\). Jest to równoważne z warunkiem, że \\(\\{ \\tau \\leq n \\} \\in \\mathcal{F}_n\\) dla każdego naturalnego \\(n\\). W czasie ciągłym ta równoważność nie zachodzi, ponieważ \\([0, \\infty)\\) nie jest przeliczalne. Warunek analogiczny do tego drugiego jest naturalny do użycia w czasie ciągłym, ponieważ zazwyczaj zdarzenie \\(\\{ \\tau = t \\}\\) ma zerowe prawdopodobieństwo dla każdego \\(t\\). Definicja 4 Zmienna losowa \\(\\tau : \\Omega \\to [0, \\infty]\\) nazywana jest \\(\\mathbb{F}\\)-czasem zatrzymania, jeśli \\(\\{ \\tau \\leq t \\} \\in \\mathcal{F}_t\\) dla każdego \\(t \\geq 0\\). W niektórych kontekstach filtracja \\(\\mathbb{F}\\), z którą pracujemy, jest na tyle regularna, że ułatwia to weryfikację, czy zmienna jest czasem zatrzymania. Definicja 5 Powiemy, że filtracja \\(\\mathbb{F} = (\\mathcal{F}_t)_{t \\in \\mathbb{R}_+}\\) jest prawostronnie ciągła, jeżeli \\[ \\mathcal{F}_t = \\mathcal{F}_{t_+}, \\qquad \\text{gdzie} \\qquad \\mathcal{F}_{t+} := \\bigcap_{s &gt; t} \\mathcal{F}_s \\] dla każdego \\(t \\in \\mathbb{R}_+\\). Zadanie 1 Załóżmy, że filtracja \\(\\mathbb{F}\\) jest prawostronnie ciągła. Wówczas \\(\\tau\\) jest czasem zatrzymania wtedy i tylko wtedy, gdy \\(\\{ \\tau &lt; t \\} \\in \\mathcal{F}_t\\) dla każdego \\(t \\in \\mathbb{R}_+\\). Zadanie 2 Pokaż, że jeśli \\(\\tau_1\\) i \\(\\tau_2\\) są czasami zatrzymania, to również \\(\\tau_1 \\wedge \\tau_2\\), \\(\\tau_1 \\vee \\tau_2\\) i \\(\\tau_1 + \\tau_2\\) są czasami zatrzymania. Zadanie 3 Udowodnij, że jeśli \\(\\{\\tau_n\\}_{n \\in \\mathbb{N}}\\) jest ciągiem czasów zatrzymania, które maleją do \\(\\tau\\), to \\(\\tau\\) jest czasem zatrzymania. Własność Markowa dotyczy warunkowej wartości oczekiwanej względem \\(\\mathcal{F}_s\\) dla ustalonego \\(s\\). Mocna własność Markowa jest analogiczna, ale warunkowanie odbywa się względem \\(\\sigma\\)-algebry \\(\\mathcal{F}_\\tau\\), gdzie \\(\\tau\\) jest czasem stopu. Składa się ona ze zdarzeń, które są określone przez przeszłość aż do czasu \\(\\tau\\). Definicja 6 Dla czasu zatrzymania \\(\\tau\\) kładziemy \\[ \\mathcal{F}_\\tau = \\left\\{ A \\in \\mathcal{F} : A \\cap \\{ \\tau \\leq t \\} \\in \\mathcal{F}_t \\text{ dla każdego } t \\in \\mathbb{R}_+ \\right\\}. \\] Zadanie 4 Pokaż, że: \\(\\mathcal{F}_\\tau\\) jest \\(\\sigma\\)-algebrą, Załóżmy, że \\(\\mathbb{F}\\) jest prawostronnie ciągła. Pokaż, że \\[ \\mathcal{F}_\\tau = \\{ A : A \\cap \\{ \\tau &lt; t \\} \\in \\mathcal{F}_t \\text{ dla każdego } t \\geq 0 \\}. \\] Oto niektóre podstawowe własności \\(\\mathcal{F}_\\tau\\). Twierdzenie 4 Jeśli \\(\\tau\\), \\(\\tau_n\\), \\(n \\in \\mathbb{N}\\) są czasasmi zatrzymania, to: \\(\\tau\\) jest mierzalny względem \\(\\mathcal{F}_\\tau\\). Jeśli \\(\\tau_n \\downarrow \\tau\\), to \\(\\mathcal{F}_\\tau = \\bigcap_n \\mathcal{F}_{\\tau_n}\\). \\(\\tau_1 \\leq \\tau_2\\) implikuje \\(\\mathcal{F}_{\\tau_1} \\subseteq \\mathcal{F}_{\\tau_2}\\). Proof. Zadanie. Mocna własność Markowa Twierdzenie 5 Niech \\((\\mathbb{P}, \\mathbb{F})\\) będzie łańcuchem Markowa na przeliczalnej przestrzeni stanów \\(S\\). Załóżmy, że \\(Y\\) jest ograniczoną zmienną losową oraz że \\(\\tau\\) jest czasem zatrzymania. Wówczas dla każdego \\(x \\in S\\), \\[\\begin{equation} \\mathbf{E}_x [ Y \\circ \\theta_\\tau | \\mathcal{F}_\\tau] = \\mathbf{E}_{X(\\tau)} [Y] \\quad \\mathbf{P}_x-\\text{prawie na pewno} \\quad \\text{na} \\quad \\{\\tau &lt; \\infty\\}. \\tag{2} \\end{equation}\\] Silna własność Markowa jest zazwyczaj używana w następujący sposób: Przemnóż równość (2) przez \\(\\mathbf{1}_{\\{\\tau &lt; \\infty\\}}\\), a następnie zastosuj \\(\\mathbf{E}_x\\). Wynik, z uwzględnieniem, że \\(\\{\\tau &lt; \\infty\\} \\in \\mathcal{F}_\\tau\\), to: \\[ \\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_{\\{\\tau &lt; \\infty\\}} \\right] = \\mathbf{E}_x \\left[ \\mathbf{E}_{X(\\tau)} \\left[ Y \\right] \\mathbf{1}_{\\{ \\tau &lt; \\infty\\}} \\right]. \\] Proof (Twierdzenia 5). Najpierw załóżmy, że \\(\\tau\\) przyjmuje wartości z przeliczalnego zbioru \\(0 \\leq t_1 \\leq t_2 \\leq \\dots\\) oraz \\(\\infty\\). W tym przypadku silna własność Markowa sprowadza się do własności Markowa, jak teraz pokażemy. Zauważmy, że prawa strona (2) jest mierzalna względem \\(\\mathcal{F}_\\tau\\). Musimy więc sprawdzić, że jeśli \\(A \\in \\mathcal{F}_\\tau\\) oraz \\(A \\subseteq \\{\\tau &lt; \\infty\\}\\), to \\[ \\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_A \\right] = \\mathbf{E}_x \\left[ \\mathbf{E}_{X(\\tau)} \\left[ Y \\right] \\mathbf{1}_A \\right]. \\] Aby to wykazać, napiszmy: \\[ \\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_A \\right] = \\sum_{n \\in \\mathbb{N}} \\mathbf{E}_x \\left[ Y \\circ \\theta_{t_n} \\mathbf{1}_{A \\cap \\{\\tau = t_n\\}} \\right] = \\sum_{n \\in \\mathbb{N}} \\mathbf{E}_x \\left[ \\mathbf{E}_{X(t_n)} \\left[ Y \\right] \\mathbf{1}_{ A \\cap \\{\\tau = t_n\\}} \\right] =\\mathbf{E}_x\\left[ \\mathbf{E}_{X(\\tau)}[Y] \\mathbf{1}_A\\right]. \\] W drugim kroku skorzystaliśmy z własności Markowa, ponieważ \\[ A \\cap \\{\\tau = t_n\\} \\in \\mathcal{F}_{t_n} \\] zgodnie z definicją \\(\\mathcal{F}_\\tau\\). W drugim kroku uzasadnimy tezę dla dowolnych \\(\\tau\\) i \\(Y\\) postaci \\[\\begin{equation} Y(\\omega) = \\prod_{j=1}^m f_j(\\omega(t_j)), \\tag{3} \\end{equation}\\] dla pewnego \\(m \\in \\mathbb{N}\\), \\(t_1, \\ldots, t_m \\in \\mathbb{R}_+\\) oraz ograniczonych funkcji \\(f_1, \\ldots, f_m \\colon S \\to \\mathbb{R}\\). Czas \\(\\tau\\) przybliżamy go od góry czasami stopu \\(\\{\\tau_n\\}_{n \\in \\mathbb{N}}\\) zdefiniowanymi przez \\[ \\tau_n = \\frac{k + 1}{2^n} \\quad \\text{jeśli} \\quad \\frac{k}{2^n} \\leq \\tau &lt; \\frac{k + 1}{2^n}. \\] dla dostatecznie dużych \\(k\\). Weźmy teraz \\(A \\in \\mathcal{F}_\\tau \\subseteq \\mathcal{F}_{\\tau_k}\\) takie, że \\(A \\subseteq \\{\\tau &lt; \\infty\\}\\). Z pierwszej części dowodu, \\[ \\mathbf{E}_x \\left[ Y \\circ \\theta_{\\tau_k} \\mathbf{1}_A \\right] = \\mathbf{E}_x\\left[ \\mathbf{E}_{X(\\tau_k)}[Y] \\mathbf{1}_A \\right]. \\] Musimy przejść do granicy, gdy \\(k \\to \\infty\\). Po prawej stronie, \\(\\tau_k \\downarrow \\tau\\) i z prawostronnej ciągłości \\(X(\\tau_k) \\to X(\\tau)\\) w \\(S\\), czyli \\(X(\\tau_k) = X(\\tau)\\) dla dostatecznie dużych \\(k\\). Po lewej stronie, napiszmy \\[\\begin{equation*} \\left( Y \\circ \\theta_{\\tau_k} \\right)(\\omega) = \\prod_{m=1}^n f_m(\\omega(t_m + \\tau_k)) \\to \\prod_{m=1}^n f_m(\\omega(t_m + \\tau)) = \\left( Y \\circ \\theta_\\tau \\right)(\\omega). \\end{equation*}\\] kiedy \\(k \\to \\infty\\), dzięki prawostronnej ciągłości ścieżek. To pokazuje tezę: \\[ \\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_A \\right] = \\mathbf{E}_x \\left[ \\mathbf{E}_{X(\\tau)}[Y] \\mathbf{1}_A \\right]. \\] W ostatnim kroku dowodu pokażemy tezę dla dowolnego \\(Y\\). Dla dowolnego \\(m \\in \\mathbb{N}\\) oraz \\(t_1, \\ldots, t_m \\in \\mathbb{R}_+\\) rozważmy \\(\\pi_{t_1, \\ldots, t_m} \\colon \\Omega \\to \\mathbb{R}^m\\) dane wzorem \\[\\begin{equation*} \\pi_{t_1, \\ldots, t_m}(\\omega) = (\\omega(t_1), \\omega(t_2), \\ldots, \\omega(t_m)). \\end{equation*}\\] Wówczas dla dowolnych \\(A_1, A_2, \\ldots, A_m \\subseteq S\\), \\[\\begin{equation*} \\pi_{t_1, \\ldots, t_m}^{-1}[A_1\\times A_2 \\times \\ldots \\times A_m ] = \\{ \\omega \\in \\Omega \\: : \\: \\omega(t_1) \\in A_1, \\ldots, \\omega(t_m) \\in A_m \\}. \\end{equation*}\\] Rozważmy teraz \\(\\pi\\)-układ \\[\\begin{equation*} \\mathcal{B} = \\left\\{ \\pi_{t_1, \\ldots, t_m}^{-1}[A_1 \\times A_2 \\times \\ldots \\times A_m ] \\: : \\: m \\in \\mathbb{N}, t_1, \\ldots , t_m \\in \\mathbb{R}_+, \\: A_1, \\ldots , A_m \\subseteq S \\right\\}. \\end{equation*}\\] oraz \\(\\lambda\\)-układ \\[\\begin{equation*} \\mathcal{L} = \\left\\{ G \\in \\mathcal{F} \\: : \\: \\mathbf{P}_x \\left[ \\theta_\\tau \\in G, A \\right] = \\mathbf{E}_x \\left[ \\mathbf{P}_{X(\\tau)} \\left[ G \\right] \\mathbf{1}_{ A } \\right] \\text{ dla } A \\in \\mathcal{F}_\\tau, \\: A \\subseteq \\{\\tau &lt; \\infty\\} \\right\\}. \\end{equation*}\\] Aproksymując \\(\\mathbf{1}_{\\pi_{t_1, \\ldots, t_m}^{-1}[A_1 \\times \\ldots \\times A_m]}\\) zmiennymi \\(Y\\) postaci (3), dostajemy \\(\\mathcal{B} \\subseteq \\mathcal{L}\\). Z lematu o \\(\\pi\\)-\\(\\lambda\\) układach mamy \\(\\mathcal{F} = \\sigma(\\mathcal{B}) \\subseteq \\mathcal{L}\\). Czyli dla każdego \\(G \\in \\mathcal{F}\\), \\[\\begin{equation*} \\mathbf{P}_x \\left[ \\theta_\\tau \\in G, A \\right] = \\mathbf{E}_x \\left[ \\mathbf{P}_{X(\\tau)} \\left[ G \\right] \\mathbf{1}_{A} \\right] \\end{equation*}\\] dla każdego \\(A \\in \\mathcal{F}_\\tau\\) takiego, że \\(A \\subseteq \\{\\tau &lt; \\infty\\}\\). Jest to równoważne naszej tezie dla \\(Y = \\mathbf{1}_G\\). Z liniowości teza jest zatem prawdziwa dla każdego \\(Y\\) przyjmującego skończenie wiele wartości. Zastosowanie standardowego twierdzenia granicznego dowodzi tezy dla dowolnego ograniczonego \\(Y\\). Charakteryzacja Zauważmy, że każda funkcja \\(\\omega \\in \\Omega\\) musi być następującego typu: Istnieje \\(t_1 \\in (0, \\infty]\\), taki że \\(\\omega(t) = \\omega(0)\\) dla każdego \\(t \\in [0, t_1)\\), następnie, jeśli \\(t_1 &lt; \\infty\\), istnieje \\(t_2 \\in (t_1, \\infty]\\) taki, że \\(\\omega(t) = \\omega(t_1) \\neq \\omega(0)\\) dla każdego \\(t \\in [t_1, t_2)\\), i tak dalej. Powyższe czasy \\(t_1, t_2, \\ldots\\) zależą oczywiście od wyboru \\(\\omega\\). Dla każdego \\(\\omega \\in \\Omega\\), istnieje zatem ciąg \\[ T_0(\\omega) = 0 &lt; T_1(\\omega) \\leq T_2(\\omega) \\leq T_3(\\omega) \\leq \\dots \\leq \\infty, \\] taki, że \\(X_t(\\omega) = X_0(\\omega)\\) dla każdego \\(t \\in [0, T_1(\\omega))\\) oraz dla każdej liczby całkowitej \\(i \\geq 1\\), warunek \\(T_i(\\omega) &lt; \\infty\\) implikuje \\(T_i(\\omega) &lt; T_{i+1}(\\omega)\\), \\(X_{T_i(\\omega)}(\\omega) \\neq X_{T_{i-1}(\\omega)}(\\omega)\\) i \\(X_t(\\omega) = X_{T_i(\\omega)}(\\omega)\\) dla każdego \\(t \\in [T_i(\\omega), T_{i+1}(\\omega))\\). Co więcej, \\(T_n(\\omega) \\uparrow \\infty\\), gdy \\(n \\to \\infty\\). Nietrudno jest sprawdzić, że \\(T_0, T_1, T_2, \\dots\\) są czasami stopu. Na przykład, \\[ \\{ T_1 \\leq t \\} = \\{X(t) \\neq X(0)\\} \\cup \\bigcup_{q \\in (0, 1) \\cap \\mathbb{Q}} \\{ X_q \\neq X_0 \\} \\in \\mathcal{F}_t. \\] Przypomnijmy, że dla \\(\\lambda &gt; 0\\), dodatnia zmienna losowa \\(U\\) ma rozkład wykładniczy z parametrem \\(\\lambda\\), jeśli \\(\\mathbb{P}[U &gt; r] = e^{-\\lambda r}\\) dla każdego \\(r \\geq 0\\). W poniższym lemacie przyjmujemy konwencję, że zmienna losowa wykładnicza o parametrze \\(0\\) jest równa \\(\\infty\\) prawie na pewno. Lemma 1 Niech \\(x \\in S\\). Istnieje rzeczywista liczba \\(c(x) \\geq 0\\), taka że zmienna losowa \\(T_1\\) ma rozkład wykładniczy z parametrem \\(c(x)\\) pod \\(\\mathbf{P}_x\\). C o więcej, jeśli \\(c(x) &gt; 0\\), to \\(T_1\\) i \\(X_{T_1}\\) są niezależne pod \\(\\mathbf{P}_x\\). Proof. Niech \\(s, t \\geq 0\\). Mamy \\[ \\mathbf{P}_x[T_1 &gt; s + t] = \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 &gt; s\\}} \\Phi \\circ \\theta_s], \\] gdzie \\(\\Phi(\\omega) = \\mathbf{1}_{\\{\\omega(r)=\\omega(0), \\, \\forall r \\in [0,t]\\}}\\). Używając własności Markowa, dostajemy \\[\\begin{equation*} \\mathbf{P}_x[T_1 &gt; s + t] = \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 &gt; s\\}} \\mathbf{E}_x[\\Phi]] = \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 &gt; s\\}} \\mathbf{P}_x[T_1 &gt; t]] = \\mathbf{P}_x[T_1 &gt; s] \\mathbf{P}_x[T_1 &gt; t], \\end{equation*}\\] co implikuje, że \\(T_1\\) ma rozkład wykładniczy pod \\(\\mathbf{P}_x\\). Załóżmy teraz, że \\(c(x) &gt; 0\\). Wówczas \\(T_1 &lt; \\infty\\), \\(\\mathbf{P}_x\\) prawie na pewno. Dla każdego \\(t \\geq 0\\) i \\(y \\in S\\), \\[\\begin{equation*} \\mathbf{P}_x[T_1 &gt; t, X_{T_1} = y] = \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 &gt; t\\}} \\Psi \\circ \\theta_t ], \\end{equation*}\\] gdzie dla \\(\\omega \\in \\Omega\\), \\(\\Psi(\\omega) = 0\\) jeśli \\(\\omega\\) jest stałe, a w przeciwnym razie \\(\\Psi(\\omega) = \\mathbf{1}_{\\{ \\gamma_1(\\omega) = y\\}}\\), gdzie \\(\\gamma_1(\\omega)\\) jest wartością \\(\\omega\\) po jego pierwszym skoku. Zatem mamy \\[\\begin{multline*} \\mathbf{P}_x[T_1 &gt; t, X_{T_1} = y] = \\mathbf{E}_x [\\mathbf{1}_{\\{T_1 &gt; t\\}} \\mathbf{E}_x[\\Psi ]] = \\\\ \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 &gt; t\\}} \\mathbf{P}_x[X_{T_1} = y]] = \\mathbf{P}_x[T_1 &gt; t] \\mathbf{P}_x[X_{T_1} = y], \\end{multline*}\\] co daje pożądaną niezależność. Punkty, dla których \\(c(x) = 0\\), są stanami pochłaniającymi dla procesu Markowa, w tym sensie, że \\(\\mathbf{P}_x[X_t = x, \\, \\forall t \\geq 0] = 1\\). Dla każdych \\(x, y \\in S\\) definiujemy \\[\\begin{equation*} \\Pi(x, y) = \\left\\{ \\begin{array}{cc} \\mathbf{P}_x[X_{T_1} = y] &amp; c(x)&gt;0 \\\\ \\delta_x(y) &amp; c(x)=0 \\end{array}\\right. \\end{equation*}\\] Zauważmy, że \\(\\Pi(x, \\cdot)\\) jest miarą prawdopodobieństwa na \\(S\\). Twierdzenie 6 Niech \\((\\mathbf{P}, \\mathbb{F})\\) będzie łańcuchem Markowa w czasie ciągłym takim, że \\(\\sup_{x \\in S} c(x) &lt; \\infty\\). Wówczas \\[\\begin{equation*} \\frac{\\mathrm{d}}{\\mathrm{d} t} \\left. \\mathbb{P}_x[X_t=y]\\right|_{t=0} = c(x) \\Pi(x,y). \\end{equation*}\\] Proof. Jeśli \\(c(x) = 0\\), to \\(\\mathbf{P}_x[X_t=x] = \\mathbf{P}_x[X_0 =x]=1\\), i stąd \\[\\begin{equation*} \\lim_{t \\to 0} \\frac{\\mathbf{P}_x[X_t=x] - 1}{t} = 0. \\end{equation*}\\] Załóżmy teraz, że \\(c(x) &gt; 0\\). Najpierw zauważmy, że \\[\\begin{equation} \\mathbf{P}_x[T_2 \\leq t] = O(t^2) \\tag{4} \\end{equation}\\] gdy \\(t \\to 0\\). Rzeczywiście, używając silnej własności Markowa w \\(T_1\\), \\[\\begin{equation*} \\mathbf{P}_x[T_2 \\leq t] \\leq \\mathbf{P}x[T_1 \\leq t, T_2 \\leq T_1 + t] = \\mathbf{E}x[\\mathbf{1}_{\\{T_1 \\leq t\\}} \\mathbf{E}_{X_{T_1}}[T_1 \\leq t]], \\end{equation*}\\] i możemy oszacować \\[\\begin{equation*} \\mathbf{P}_{X_{T_1}}[T_1 \\leq t] \\leq \\sup_{y \\in S} \\mathbf{P}_y[T_1 \\leq t] \\leq \\sup_{y \\in S} c(y), \\end{equation*}\\] co daje oczekiwany wynik, ponieważ mamy również \\(\\mathbf{P}_x[T_1 \\leq t] \\leq c(x)t\\). Z (4) wynika, że \\[\\begin{multline*} \\mathbf{P}_x[X_t=y] = \\mathbf{P}_x[X_t=y, \\: T_1 &gt; t] + \\mathbf{P}_x[X_{T_1}=y, \\: T_1 \\leq t] + O(t^2) \\\\ = \\delta_{x}(y) e^{-c(x)t} + \\left(1 - e^{-c(x)t}\\right)\\Pi(x,y) + O(t^2), \\end{multline*}\\] używając niezależności \\(T_1\\) i \\(X_{T_1}\\) oraz definicji \\(\\Pi(x, y)\\). Dochodzimy do wniosku, że skoro \\(\\mathbf{P}_x[X_0=y] = \\delta_x(y)\\), to \\[\\begin{equation*} \\frac{\\mathbf{P}_x[X_t=y] - \\mathbf{P}_x[X_0=y]}{t} \\to -c(x)\\delta_{x}(y) + c(x)\\Pi(x,y). \\end{equation*}\\] co kończy dowód. Kolejne twierdzenie dostarcza pełnego opisu ścieżek procesu \\(X\\) pod \\(\\mathbf{P}_x\\). Dla uproszczenia zakładamy, że nie ma stanów pochłaniających, ale czytelnik łatwo rozszerzy stwierdzenie na przypadek ogólny. Twierdzenie 7 Zakładamy, że \\(c(y) &gt; 0\\) dla każdego \\(y \\in S\\) i że \\(\\sup_{y \\in S} c(y) &lt; \\infty\\). Niech \\(x \\in S\\). Wówczas, \\(\\mathbf{P}_x\\) p.n., czasy skoku \\(T_1 &lt; T_2 &lt; T_3 &lt; \\dots\\) są skończone, a ciąg \\(X_0, X_{T_1}, X_{T_2}, \\dots\\) pod \\(\\mathbf{P}_x\\) jest dyskretnym łańcuchem Markowa z macierzą przejścia \\(\\Pi\\) rozpoczętym w \\(x\\). Ponadto, pod warunkiem \\((X_0, X_{T_1}, X_{T_2}, \\dots)\\), zmienne losowe \\(T_1 - T_0, T_2 - T_1, \\dots\\) są niezależne, a dla każdej liczby całkowitej \\(i \\geq 0\\), rozkład warunkowy \\(T_{i+1} - T_i\\) jest wykładniczy z parametrem \\(c(X_{T_i})\\). Proof. Zastosowanie silnej własności Markowa pokazuje, że wszystkie czasy stopu \\(T_1, T_2, \\dots\\) są skończone \\(\\mathbf{P}_x\\)-p.n. Następnie, niech \\(y, z \\in S\\), a \\(f_1, f_2 \\colon S \\to \\mathbb{R}\\). Używając silnej własności Markowa w \\(T_1\\): \\[\\begin{multline*} \\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_1} = y\\}} f_1(T_1) 1_{\\{X_{T_2} = z\\}} f_2(T_2 - T_1)] = \\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_1} = y\\}} f_1(T_1) \\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_2} = z\\}} f_2(T_2 - T_1)]] \\\\ = \\Pi(x, y) \\Pi(y, z) \\int_0^\\infty e^{-c(x)s_1} f_1(s_1)\\mathrm{d} s_1 \\int_0^\\infty e^{-c(y)s_2} f_2(s_2) \\mathrm{d} s_2. \\end{multline*}\\] Postępując indukcyjnie, otrzymujemy dla każdych \\(y_1, \\dots, y_p \\in S\\) oraz \\(f_1, \\dots, f_p \\colon S \\to \\mathbb{R}\\): \\[\\begin{multline*} \\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_1} = y_1\\}} \\mathbf{1}_{\\{X_{T_2} = y_2\\}} \\dots \\mathbf{1}_{\\{X_{T_p} = y_p\\}} f_1(T_1) f_2(T_2 - T_1) \\dots f_p(T_p - T_{p-1})] \\\\ = \\Pi(x, y_1) \\Pi(y_1, y_2) \\dots \\Pi(y_{p-1}, y_p) \\prod_{i=1}^p \\left( \\int_0^\\infty e^{-c(y_{i-1})s} f_i(s) \\mathrm{d} s\\right), \\end{multline*}\\] gdzie \\(y_0 = x\\). Z powyższego twierdzenia wynika charakteryzacja łańcucha Markowa w terminach \\(Q\\)-macierzy. Przez \\(\\mathbb{F}^X = (\\mathcal{F}_t^X)_t\\) oznaczać będziemy najmniejszą możliwą filtrację, tj. \\[\\begin{equation*} \\mathcal{F}_t = \\sigma(X_s \\: : \\: s \\leq t). \\end{equation*}\\] Wniosek 1 Niech \\(q = (q(x,y))_{x,y\\in S}\\) będzie \\(Q\\)-macierzą taką, że \\(\\sup_{x \\in S}|q(x,x)| &lt; \\infty\\). Wówczas istnieje jedyna rodzina miar \\(\\mathbf{P}\\) taka, że \\((\\mathbf{P}, \\mathbb{F}^X)\\) jest łańcuchem Markowa stowarzyszonym z \\(Q\\)-macierzą \\(q\\). "],["wykład-3-procesy-i-półgrupy-fellera.html", "Wykład 3: procesy i półgrupy Fellera Proces Półgrupa", " Wykład 3: procesy i półgrupy Fellera 2024-10-17 Piotr Dyszewski W tym rozdziale \\(S\\) jest ośrodkową, lokalnie zwartą przestrzenią metryczną, a \\(C(S)\\) jest przestrzenią ciągłych funkcji rzeczywistych na \\(S\\). Przez \\(C_0(S)\\) oznaczać będziemy klasę funkcji z \\(C(S)\\) znikających w nieskończoności. Dokładniej \\(C_0(S)\\) to zbiór funkcji \\(f\\) z \\(C(S)\\) takich, że dla każdego dodatniego \\(\\epsilon\\) istnieje zwarty \\(K \\subseteq S\\) taki, że \\(|f(x)| \\leq \\epsilon\\) dla \\(x \\in S \\setminus K\\). Zauważmy, że jeżeli \\(S\\) jest zwarta, to \\(C_0(S) = C(S)\\). Dodatkowo każda \\(f\\) z \\(C_0(S)\\) jest jednostajnie ciągła, tj. dla każdego dodatniego \\(\\epsilon\\) istnieje dodatnia \\(\\delta\\), taka, że dla każdych \\(x , y\\in S\\), \\[\\begin{equation*} \\mathrm{d}(x,y) &lt; \\delta \\quad \\Rightarrow \\quad |f(x)-f(y)| &lt;\\epsilon. \\end{equation*}\\] Tutaj \\(\\mathrm{d}\\) jest metryką na \\(S\\). W obu przestrzeniach \\(C(S)\\) i \\(C_0(S)\\) używamy normy jednostajnej \\[ \\|f\\| = \\sup_{x \\in S} |f(x)|, \\] co czyni \\(C_0(S)\\) przestrzenią Banacha. Głównym powodem stosowania ciągłych funkcji zanikających w nieskończoności zamiast ograniczonych ciągłych funkcji w przypadku lokalnie zwartym jest to, że jednostajna ciągłość jest wymagana w wielu argumentach. Ograniczone funkcje ciągłe nie są zwykle jednostajnie ciągłe, podczas gdy ciągłe funkcje zanikające na nieskończoności są. Innym powodem jest to, że \\(C_0(S)\\) jest ośrodkowa, co nie jest ogólnie prawdziwe dla przestrzeni wszystkich ograniczonych ciągłych funkcji na \\(S\\). Proces Zaczynamy od opisu składników potrzebnych do definicji głównego obiektu zainteresowania w tym rozdziale. Konstrukcja będzie analogiczna do łańcuchów Markowa w czasie ciągłym. Niech \\(\\Omega = D[0, \\infty)\\) będzie zbiorem funkcji prawostronnie ciągłych$ \\(\\omega : [0, \\infty) \\to S\\) z lewymi granicami w każdym punkcie. Tak jak poprzednio dla \\(s, t \\in \\mathbb{R}_+\\) połóżmy też \\[ X_t(\\omega) = \\omega(t) \\text{ oraz } (\\theta_s \\omega)(t) = \\omega(t + s). \\] Niech \\(\\mathcal{F}\\) będzie najmniejszym \\(\\sigma\\)-ciałem podzbiorów \\(\\Omega\\) względem którego wszystkie \\(X_t\\) dla \\(t \\in \\mathbb{R}_+\\) są mierzalne. Definicja 7 Procesem Fellera na \\(S\\) nazywamy parę uporządkowaną \\((\\mathbf{P}, \\mathbb{F})\\) taką, że (PF1) \\(\\mathbf{P}=\\{\\mathbf{P}_x\\}_{x \\in S}\\), gdzie dla każdego \\(x \\in S\\), \\(\\mathbf{P}_x\\) jest miarą probabilistyczną na \\((\\Omega, \\mathcal{F})\\) taką, że \\[\\begin{equation} \\mathbf{P}_x[X_0 = x] = \\mathbf{P}_x [\\omega \\: : \\: \\omega(0)=x] = 1. \\tag{5} \\end{equation}\\] (PF2) \\(\\mathbb{F}=\\{\\mathcal{F}_t\\}_{t \\in \\mathbb{R}_+}\\) jest filtracją na \\(\\Omega\\), względem której zmienne losowe \\(X(t)\\) są adaptowane. (PF3) Odwzorowanie \\[\\begin{equation} x \\mapsto \\mathbf{E}_x \\left[f(X_t) \\right] \\text{ jest w } C_0(S) \\text{ dla wszystkich } f \\in C_0(S) \\text{ i } t \\geq 0. \\tag{6} \\end{equation}\\] (PF4) Spełniona jest własność Markowa \\[\\begin{equation} \\mathbf{E}_x\\left[Y \\circ \\theta_s \\mid \\mathcal{F}_s\\right] = \\mathbf{E}_{X(s)}\\left[Y\\right] \\quad \\mathbf{P}_x\\text{-prawie wszędzie } \\tag{7} \\end{equation}\\] dla wszystkich \\(x \\in S\\) oraz wszystkich ograniczonych mierzalnych \\(Y\\) na \\(\\Omega\\). Własność (6) znana jest jako własność Fellera. Innym sposobem przedstawienia części ciągłości, który wydaje się całkiem naturalny, jest to, że \\(x_n \\to x\\) w \\(S\\) implikuje, że rozkład \\(X\\) dla procesu rozpoczynającego się w \\(x_n\\) zbiega się słabo do tego dla procesu rozpoczynającego się w \\(x\\). Własność Fellera (razem z prawostronną ciągłością trajektorii) implikuje silną własność Markowa. Twierdzenie 8 Każdy proces Fellera ma silną własność Markowa. Jeżeli \\((\\mathbf{P}, \\mathbb{F})\\) jest procesem Fellera, to dla każdej ograniczonej zmiennej \\(Y \\colon \\Omega \\to \\mathbb{R}\\) oraz \\(\\mathbb{F}\\)-czasu zatrzymania \\(\\tau\\) i każdego \\(x\\), \\[ \\mathbf{E}_x [Y \\circ \\theta_\\tau \\mid \\mathcal{F}_\\tau] = \\mathbf{E}_{X(\\tau)} \\left[ Y \\right]\\quad \\text{prawie na pewno } \\mathbf{P}_x \\] na zdarzeniu \\(\\{\\tau &lt; \\infty\\}\\). Zadanie 5 Niech \\((\\mathbf{P}, \\mathbb{F})\\) będzie procesem Fellera. Pokaż, że odwzorowanie \\[\\begin{equation} x \\mapsto \\mathbf{E}_x \\left[ \\prod_{j=1}^n f_j(X_{t_j}) \\right] \\tag{8} \\end{equation}\\] jest ciągłe dla dowolnego \\(n\\), dowolnych \\(t_1, \\ldots, t_n \\in \\mathbb{R}\\) oraz dowolnych \\(f_1, \\ldots, f_n \\in C_0(S)\\). Proof (Twierdzenia 8). Rozumowanie przebiega identycznie jak w przypadku łańcuchów Markowa w czasie ciągłym. W miejscu, w którym wymagana jest ciągłość odwzorowań \\(x \\mapsto \\mathbf{E}_x[Y]\\) należy powołać się na tezę Zadania 5. Przykład 3 Niech \\(B = (B_t)_{t \\in \\mathbb{R}_+}\\) będzie standardowym ruchem Browna określonym na przestrzeni probabilistycznej \\((\\Sigma, \\mathcal{G}, \\mathbb{P})\\). Przypomnijmy, że oznacza to, że \\(B_0=0\\) \\(\\mathbb{P}\\)-p.w. Dla dowolnych \\(t,s \\in \\mathbb{R}_+\\), \\(t\\geq s\\) zmienna \\(B_t-B_s\\) ma rozkład normalny \\(\\mathcal{N}(0,t-s)\\) o średniej zero i wariancji \\(t-s\\). Dla dowolnych \\(t,s \\in \\mathbb{R}_+\\), \\(t\\geq s\\) zmienna \\(B_t-B_s\\) jest niezależna od sigma ciała \\(\\mathcal{G}_s^B = \\sigma(B_r \\: : \\: r \\leq s)\\). Odwzorowanie \\(t \\mapsto B_t\\) jest ciągłe. Pokażemy, że ruch Browna jest procesem Fellera w myśl przyjętej przez nas definicji. Połóżmy \\(\\mathcal{F}_t = \\sigma(X_s \\: : \\: s \\leq t)\\). Niech \\(S = \\mathbb{R}\\). Dla \\(x \\in S\\) zdefiniujmy \\(\\mathbf{P}_x\\) jako rozkład ruchu Browna (rozumianego jako funkcji \\(\\mathbb{R}_+ \\to \\mathbb{R}\\)) zapoczątkowanego w punkcie \\(x\\), dokładniej dla \\(A \\in \\mathcal{F}\\) niech \\(\\mathbf{P}_x[A] = \\mathbb{P}[B+x \\in A]\\). Tutaj przez \\(B+x\\) rozumiemy funkcję \\(t \\mapsto B_t+x\\). Wylosowanie ścieżki \\(\\omega\\) z rozkładu \\(\\mathbf{P}_x\\) jest równoważne z wylosowaniem trajektorii ruchu Browna zapoczątkowanego w \\(x\\). Spełniona jest własność (PF1), ponieważ \\[\\begin{equation*} \\mathbf{P}_x[X_0=x] = \\mathbb{P}[B_0+x=x]=1. \\end{equation*}\\] Własność (PF2) jest spełniona wprost z definicji filtracji \\(\\mathbb{F}\\). Aby uzasadnić własność Fellera (PF3) ustalmy \\(f \\in C_0(S)\\). Ciągłość \\[\\begin{equation*} x \\mapsto \\mathbf{E}_x\\left[ f(X_t) \\right] = \\mathbb{E}[f(B_t+x)] \\end{equation*}\\] wynika z ciągłości \\(f\\) oraz twierdzenia o zbieżności ograniczonej. Aby uzasadnić, że powyższe odwzorowanie jest klasy \\(C_0(S)\\) należy pokazać, że \\[\\begin{equation*} \\lim_{|x| \\to \\infty} \\mathbf{E}_x[f(X_t)] =0. \\end{equation*}\\] Wystarczy w tym celu rozważyć oszacowanie \\[\\begin{equation*} \\left|\\mathbb{E}[f(B_t+x)]\\right| \\leq \\|f \\| \\mathbb{P}[|B_t| &gt;|x|/2] + \\sup_{|y| &gt;|x|/2}|f(y)|. \\end{equation*}\\] Oba składniki po prawej stronie zbiegają do zera, przy czy zbieżność tego drugiego wynika z \\(f \\in C_0(\\mathbb{R})\\). Własność Markowa uzasadniamy dokładnie w taki sam sposób, w jaki zrobiliśmy to dla procesu Poissona w Przykładzie @ref{exm:2-poisson}. Zadanie 6 Niech \\(S=\\mathbb{Z}\\). Pokaż, że łańcuch Markowa w czasie ciągłym \\((\\mathbf{P}, \\mathbb{F})\\) jest procesem Fellera wtedy i tylko wtedy, gdy dla każdego \\(y \\in S\\) i każdego \\(t \\in \\mathbb{R}_+\\), \\[\\begin{equation*} \\lim_{|x| \\to \\infty} \\mathbf{P}_x[X_t=y]=0. \\end{equation*}\\] Półgrupa Chcemy teraz przedstawić odpowiednik funkcji przejścia na nieprzeliczalnej przestrzeni stanów. W naturalny sposób nasuwa się rozważenie rozkładów \\(\\mathbf{P}_x[X_t \\in \\mathrm{d} y]\\). Jednak na dłuższą metę język rozkładów jest nieporęczny. O wiele bardziej praktyczny jest język półgrup. Aby umotywować następną definicję, rozważmy przeliczalną przestrzeń stanów \\(S_0\\) oraz funkcję przejścia \\(p\\) na \\(S_0\\). Funkcję przejścia można zakodować w kategoriach rodziny operatorów \\[\\begin{equation} T_tf(x) = \\sum_{y\\in S_0} p_t(x, y)f(y), \\tag{9} \\end{equation}\\] dla \\(f \\in C_0(S_0)\\). Jasne jest, że znając \\(T_t\\), a więc znając wartości \\(T_tf\\) dla wszystkich \\(f \\in C_0(S_0)\\), znamy też funkcję przejścia \\(p_t(x,y)\\). Wykorzystując równiania Chapmana-Kołmogorowa dostajemy dla \\(s,t\\geq 0\\), \\[\\begin{multline*} T_{s+t} f(x) = \\sum_{y \\in S_0} p_{t+s}(x,y) f(y) = \\sum_{y \\in S_0} \\sum_{z \\in S_0} p_t(x,z)p_s(z,y)f(y) \\\\ \\sum_{z \\in S_0} p_t(x,z)\\sum_{y\\in S_0}p_s(z,y)f(y) =\\sum_{z \\in S_0} p_t(x,z) (T_sf)(z) = T_t(T_s(f))(x). \\end{multline*}\\] Wszystkie powyższe manipulacje są dozwolone ponieważ \\(f \\in C_0(S_0)\\) jest ograniczona. Powyższa tożsamość zapisuje się jako \\(T_t T_s = T_t\\circ T_s = T_{t+s}\\). Oznacza to, że \\((T_{t})_{t \\geq 0}\\) tworzą półgrupę. Definicja 7 Półgrupa Fellera to rodzina ciągłych operatorów liniowych \\(T=\\{T_t\\}_{t \\in \\mathbb{R}_+}\\) na \\(C_0(S)\\) spełniających następujące własności: \\(T_0f = f\\) dla wszystkich \\(f \\in C_0(S)\\). Dla każdego \\(f \\in C_0(S)\\), \\(\\lim_{t \\to 0} T_tf = f\\) w \\(C_0(S)\\). \\(T_{t+s}f = T_sT_tf\\) dla każdego \\(f \\in C_0(S)\\). \\(T_tf \\geq 0\\) dla każdego nieujemnego \\(f \\in C_0(S)\\). Istnieje rodzina \\(f_n \\in C_0(S)\\), \\(n\\in \\mathbb{N}\\) taka, że \\(\\sup_n \\|f_n\\| &lt; \\infty\\), oraz \\(T_tf_n\\) zbiega punktowo do \\(1\\) dla każdego \\(t \\geq 0\\). Część c. to analogia równań Chapmana-Kolmogorowa i nazywana jest własnością półgrupy. Jedną z jej konsekwencji jest to, że \\(T(t)\\) i \\(T(s)\\) komutują, tj. \\(T_tT_s=T_{t+s}=T_{s+t}=T_sT_t\\). Z części d. i e. wynika, że \\(\\|T(t)f\\| \\leq \\|f\\|\\) dla wszystkich \\(f \\in C_0(S)\\), tak więc każdy \\(\\|T\\|\\leq 1\\). Własność b. jest znana jako mocna ciągłość. Wraz z c. i własnością kontrakcji, implikuje to, że funkcja \\(t \\mapsto T(t)f\\) z \\([0, \\infty)\\) do \\(C_0(S)\\) jest ciągła. Oto ważny przykład - półgrupa Gaussa–Weierstrassa. Część b. tego ćwiczenia ilustruje powody przyjmowania funkcji w \\(C_0(S)\\) zanikających na nieskończoności. Zadanie 7 Niech \\(S = \\mathbb{R}\\) i \\(B=(B_t)_{t \\in \\mathbb{R}_+}\\) będzie ruchem Browna. Pokaż, że \\(T_t\\) zdefiniowane przez \\[ T_tf(x) = \\mathbb{E} [f(B_t+x)] \\] jest półgrupą Fellera. Wyjaśnij, dlaczego \\(T\\) nie jest mocno ciągła jako półgrupa operatorów na klasie \\(C_b(S)\\) ograniczonych funkcji z \\(C(S)\\). W tym rozdziale konieczne będzie całkowanie funkcji ciągłych przyjmujących wartości w \\(C(S)\\) względem \\(t\\). Rachunek takich funkcji jest analogiczny do rachunku funkcji rzeczywistych. W tym duchu wiążemy z półgrupą jej transformata Laplace’a \\[\\begin{equation} U(\\alpha)f = \\int_0^\\infty e^{-\\alpha t} T_tf \\, \\mathrm{d} t, \\quad \\alpha &gt; 0, \\tag{10} \\end{equation}\\] która nazywana jest rezolwentą półgrupy. Funkcję \\(U(\\alpha)f\\) można interpretować jako całkę Bochnera pojawiającą się po prawej stronie (10). Można też równoważnie myśleć, że jest to funkcja \\(S \\to \\mathbb{R}\\) zadana przez \\[\\begin{equation*} U(\\alpha)f(x) = \\int_0^\\infty e^{-\\alpha t} T_tf(x) \\, \\mathrm{d} t, \\quad x \\in S. \\end{equation*}\\] W każdym razie całka w (10) jest dobrze określona, ponieważ funkcja \\(t \\mapsto e^{-\\alpha t} T_tf\\) jest ciągła oraz \\[ \\|e^{-\\alpha t} T_tf\\| \\leq e^{-\\alpha t} \\|f\\|. \\] Zauważmy też, że \\(U(\\alpha)\\) jest operatorem liniowym na \\(C_0(S)\\) i spełnia \\[ \\|U(\\alpha)f\\| \\leq \\|f\\|/\\alpha. \\] Zadanie 8 Pokaż, że dla każdego \\(f \\in C_0(S)\\), \\[ \\lim_{\\alpha \\to \\infty} \\alpha U(\\alpha) f = f. \\] Własność półgrupy przekłada się na następującą użyteczną relację, znaną jako równanie rezolwenty: \\[\\begin{equation} U(\\alpha) - U(\\beta) = (\\beta - \\alpha) U(\\alpha) U(\\beta). \\tag{11} \\end{equation}\\] Aby to sprawdzić, weźmy \\(\\alpha \\neq \\beta\\) i zapiszmy \\[\\begin{multline} U(\\alpha) U(\\beta) f = \\int_0^\\infty e^{-\\alpha t} T_t U(\\beta) f \\, \\mathrm{d} t = \\int_0^\\infty e^{-\\alpha t} \\left( \\int_0^\\infty e^{-\\beta s} T_t T_s f \\, \\mathrm{d} s \\right) \\mathrm{d} t\\\\ = \\int_0^\\infty \\int_0^r e^{-\\alpha t} e^{-\\beta (r-t)} \\, \\mathrm{d} t T_r f\\mathrm{d} r = \\int_0^\\infty \\frac{e^{-\\alpha r} - e^{-\\beta r}}{\\beta - \\alpha} T_r f \\mathrm{d} r. \\tag{12} \\end{multline}\\] Jedną z konsekwencji (11) jest to, że \\(U(\\alpha)\\) i \\(U(\\beta)\\) komutują. "],["wykład-4-generatory.html", "Wykład 4: Generatory Od procesu do półgrupy i generatora", " Wykład 4: Generatory 2024-10-24 Piotr Dyszewski Do tej pory te definicje powinny być dość intuicyjne. Kolejna definicja może wydawać się mniej oczywista, jednak okazuje się być odpowiednim odpowiednikiem definicji macierzy \\(Q\\). Aby nakreślić analogię, biorąc macierz \\(Q\\) na przeliczalnym zbiorze \\(S_0\\), niech \\(p\\) będzie funkcją przejścia zadaną jako \\[\\begin{equation*} p_t = e^{tq} = \\sum_{k=0}^\\infty \\frac{t^k}{k!} q^k. \\end{equation*}\\] Z tą funkcją wiążemy półgrupę \\[\\begin{equation*} T_t f(x) = \\sum_{y \\in S_0} p_t(x,y) f(y), \\end{equation*}\\] a także rezolwentę \\[\\begin{equation*} U(\\alpha) f(x) = \\int_0^\\infty e^{-\\alpha t} T_t f(x) \\, \\mathrm{d}t = \\int_0^\\infty e^{-\\alpha t} e^{tq} f(x) \\, \\mathrm{d}t = \\int_0^\\infty e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t \\, f(x). \\end{equation*}\\] Ostatnie przejście wynika z faktu, że mamy tutaj do czynienia z mnożeniem wektora przez macierz. Zauważmy, że \\[\\begin{equation*} (q - \\alpha I) \\int_0^\\infty e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t = \\int_0^\\infty (q - \\alpha I) e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t = \\int_0^\\infty \\frac{\\mathrm{d}}{\\mathrm{d}t} e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t = -I. \\end{equation*}\\] Oznacza to, że \\[\\begin{equation*} U(\\alpha) f(x) = (\\alpha I - q)^{-1} f(x). \\end{equation*}\\] Z własności rezolwenty wiemy, że \\[\\begin{equation*} \\left\\| (I - \\frac{1}{\\alpha} q)^{-1} \\right\\| = \\|\\alpha U(\\alpha)\\| \\leq 1. \\end{equation*}\\] Ostatnia własność rezolwenty, z której tutaj skorzystaliśmy, wynika z kontraktywności operatorów w półgrupie \\(T\\) (\\(\\|T_t\\| \\leq 1\\)). Definicja 8 Generator infinitezymalny na \\(C_0(S)\\) to para uporządkowana \\((L, \\mathcal{D}(L))\\) taka, że: (GI1) \\(\\mathcal{D}(L)\\) jest gęstą podprzestrzenią liniową \\(C_0(S)\\). (GI2) \\(L \\colon \\mathcal{D}(L) \\to C_0(S)\\) jest operatorem liniowym. (GI3) Jeśli \\(f \\in \\mathcal{D}(L)\\), \\(\\lambda \\geq 0\\), i \\(f - \\lambda L f = g\\), to \\[ \\inf_{x \\in S} f(x) \\geq \\inf_{x \\in S} g(x). \\] (GI4) \\(\\mathcal{R}(I - \\lambda L) = C_0(S)\\) dla wszystkich dostatecznie małych \\(\\lambda &gt; 0\\). (GI5) Dla dostatecznie małych dodatnich \\(\\lambda\\) istnieje ciąg \\(f_n \\in \\mathcal{D}(L)\\) (który może zależeć od \\(\\lambda\\)) taki, że \\(g_n = f_n - \\lambda L f_n\\) spełnia warunek \\(\\sup_n \\|g_n\\| &lt; \\infty\\), i zarówno \\(f_n\\), jak i \\(g_n\\) zbiega punktowo do \\(1\\). Zauważmy, że własność (GI3) ma następującą konsekwencję: \\[\\begin{equation} f \\in \\mathcal{D}(L), \\lambda \\geq 0, f - \\lambda L f = g \\implies \\|f\\| \\leq \\|g\\|. \\tag{13} \\end{equation}\\] Aby to zobaczyć, napiszmy: \\[ \\inf_{x \\in S} g(x) \\leq \\inf_{x \\in S} f(x) \\leq \\sup_{x \\in S} f(x) \\leq \\sup_{x \\in S} g(x), \\] gdzie ostatnia nierówność wynika z (GI3), gdy zastąpimy \\(f\\) i \\(g\\) odpowiednio przez \\(-f\\) i \\(-g\\). Oznacza to, że operator \\(I - \\lambda L\\) jest różnowartościowy. Rzeczywiście, dla \\(f - \\lambda L f = g = h - \\lambda L h\\), mamy \\(\\|f - h\\| \\leq \\|g - g\\| = 0\\). Tak więc, dla dostatecznie małych dodatnich \\(\\lambda\\), \\((I - \\lambda L)^{-1}\\) jest dobrze określoną kontrakcją, która odwzorowuje funkcje nieujemne na funkcje nieujemne. Ponieważ Definicja @ref{def:3-12} jest dość abstrakcyjna, pomocne może być rozważenie następującego przykładu, który okazuje się być generatorem procesu na prostej, poruszającego się w prawo z jednostkową prędkością. Zauważmy, że najtrudniejszą własnością do sprawdzenia jest (GI4). Zazwyczaj tak bywa. Zadanie 9 Przypuśćmy, że \\(S = \\mathbb{R}\\), \\[ \\mathcal{D}(L) = \\{f \\in C_0(\\mathbb{R}) : f&#39; \\in C_0(\\mathbb{R})\\}, \\] oraz \\(L f = f&#39;\\). Pokaż, że para \\((L, \\mathcal{D}(L))\\) jest generatorem infinitezymalnym. Od procesu do półgrupy i generatora Oto pierwszy krok w przejściu od procesu Fellera do jego generatora. Twierdzenie 9 Niech dany będzie proces Fellera \\((\\mathbf{P}, \\mathbb{F})\\). Dla \\(t \\geq 0\\) zdefiniujmy \\[\\begin{equation} T_t f(x) = \\mathbf{E}_x [f(X(t))] \\tag{14} \\end{equation}\\] dla \\(f \\in C_0(S)\\). Wtedy \\(T = (T_t)_{t \\geq 0}\\) jest półgrupą Fellera. Proof. Własności a., d. i e. z Definicji 7 są natychmiastowe. Własność półgrupy c. wynika z własności Markowa: \\[ T_{s + t} f(x) = \\mathbf{E}_x f(X(s + t)) = \\mathbf{E}_x [ \\mathbf{E}_{X(s)} f(X(t)) | \\mathcal{F}_s] = \\mathbf{E}_x [T_t f(X(s))] = T_s T_t f(x). \\] Zbieżność punktowa w b. wynika z ciągłości ścieżek i ciągłości \\(f\\). Aby sprawdzić wymaganą jednostajność w tej zbieżności, użyjemy rezolwenty \\[\\begin{equation*} U(\\alpha) f(x) = \\int_0^\\infty e^{-\\alpha t} T_t f(x) \\, \\mathrm{d}t = \\mathbf{E}_x \\left[ \\int_0^\\infty e^{-\\alpha t} f(X_t) \\, \\mathrm{d}t \\right]. \\end{equation*}\\] W dowodzie równania rezolwenty (11), użyliśmy wspomnianej jednostajności, ponieważ całki były interpretowane jako całki funkcji o wartościach w \\(C_0(S)\\). Jednak te same obliczenia stosują się do równania rezolwenty bez tej jednostajności, jeśli całki są interpretowane jako zwykłe całki dla ustalonego \\(x\\). Aby uzasadnić zamianę kolejności całkowania, zauważmy, że \\(T_t f(x)\\) jest jednostajnie ograniczone, prawostronnie ciągłe w \\(t\\) dla każdego \\(x\\), a także ciągłe w \\(x\\) dla każdego \\(t\\), zatem jest wspólnie mierzalne względem \\(x\\) i \\(t\\). Zbiór \\(\\mathcal{L} = \\mathcal{R}(U(\\alpha))\\) jest niezależny od \\(\\alpha\\). Z równania rezolwenty mamy bowiem \\[\\begin{equation*} U(\\alpha)f = U(\\beta) \\left(f + (\\beta - \\alpha) U(\\beta) f \\right). \\end{equation*}\\] Jeśli \\(f = U(\\alpha)g \\in \\mathcal{L}\\), to \\[ T_t f = \\int_0^\\infty e^{-\\alpha s} T_{s + t} g \\, \\mathrm{d}s = \\int_t^\\infty e^{-\\alpha(r - t)} T_r g \\, \\mathrm{d}r, \\] co zbiega jednostajnie do \\(f\\) gdy \\(t \\downarrow 0\\). Ponieważ każde \\(T_t\\) jest kontrakcją, mamy \\(\\|T_t f - f\\| \\to 0\\) dla wszystkich \\(f\\) w mocnym domknięciu \\({\\rm cl}(\\mathcal{L})\\). Pokażemy teraz, że wspomniane mocne domknięcie jest równe słabemu domknięciu: \\[\\begin{equation*} {\\rm wcl}(\\mathcal{L}) = \\left\\{ f \\in C_0(S) \\: : \\: \\exists \\{f_n\\}_n \\subseteq \\mathcal{L}, \\: x(f_n) \\to x(f), \\: \\forall x \\in C_0(S)^* \\right\\}. \\end{equation*}\\] Skoro zbieżność w normie implikuje zbieżność słabą, to \\({\\rm cl}(\\mathcal{L}) \\subseteq {\\rm wcl}(\\mathcal{L})\\). Weźmy \\(f \\notin {\\rm cl}(\\mathcal{L})\\). Ponieważ \\(\\mathcal{L}\\) jest wypukły, z twierdzenia Hahna-Banacha istnieje funkcjonał liniowy \\(\\mu\\) oddzielający \\(f\\) od \\({\\rm cl}(\\mathcal{L})\\), taki że \\[\\begin{equation*} \\mu(f) &lt; \\inf_{g \\in {\\rm cl}(\\mathcal{L})} \\mu(g). \\end{equation*}\\] Funkcjonał ten dowodzi, że \\(f \\notin {\\rm wcl}(\\mathcal{L})\\). Pozostaje pokazać, że słabe domknięcie \\(\\mathcal{L}\\) jest równe całemu \\(C_0(S)\\). Jest tak, ponieważ \\(\\alpha U(\\alpha) f\\) zbiega punktowo do \\(f\\) gdy \\(\\alpha \\to \\infty\\) dla każdego \\(f \\in C_0(S)\\). Następnie zobaczymy, jak przejść od półgrupy do generatora. Twierdzenie 10 Przypuśćmy, że \\((T_t)_{t \\geq 0}\\) jest półgrupą Fellera. Zdefiniujmy \\[\\begin{equation} Lf = \\lim_{t \\downarrow 0} \\frac{T(t)f - f}{t} \\tag{15} \\end{equation}\\] dla \\(f\\) z \\[ \\mathcal{D}(L) = \\{f \\in C(S) : \\text{przy $t \\to 0$ granica } (T_tf-f)/t \\text{ istnieje}\\}. \\] Wtedy para \\((L, \\mathcal(L))\\) jest generatorem infinitezymalnym. Ponadto: Dla dowolnego \\(g \\in C_0(S)\\) oraz \\(\\alpha &gt; 0\\), \\[\\begin{equation} f = \\alpha U(\\alpha)g \\text{ wtedy i tylko wtedy, gdy } f \\in \\mathcal{D}(L) \\text{ i spełnia } f - \\alpha^{-1} Lf = g. \\tag{16} \\end{equation}\\] Jeśli \\(f \\in \\mathcal{D}(L)\\), to \\(T_t f \\in \\mathcal{D}(L)\\) dla wszystkich \\(t \\geq 0\\), jest funkcją ciągłą, różniczkowalną względem \\(t\\), i spełnia \\[\\begin{equation} \\frac{\\mathrm{d}}{\\mathrm{d}t} T_t f = T_t Lf = L T_t f. \\tag{17} \\end{equation}\\] Proof. Przypuśćmy, że \\(f = \\alpha U(\\alpha)g\\) dla pewnego \\(\\alpha &gt; 0\\) oraz \\(g \\in C_0(S)\\). Korzystając z własności półgrupy i zmieniając zmienne jak w dowodzie Twierdzenia @ref{thm:3-15}, mamy: \\[\\begin{align*} \\frac{T_t f - f}{t} &amp;= \\alpha \\frac{e^{\\alpha t} - 1}{t} \\int_t^{\\infty} e^{-\\alpha s} T_s g \\, \\mathrm{d}s - \\alpha \\frac{1}{t} \\int_0^t e^{-\\alpha s} T_s g \\, \\mathrm{d}s \\\\ &amp;\\to \\alpha^2 U(\\alpha) g - \\alpha g = \\alpha f - \\alpha g, \\end{align*}\\] gdy \\(t \\downarrow 0\\). Przy przejściu do granicy skorzystaliśmy z własności b. z Definicji @ref{def:3-1}. To dowodzi jednej implikacji w (16), jak również (GI4) w Definicji 8. Ponieważ \\(\\alpha U(\\alpha)g \\in \\mathcal{D}(L)\\) i \\(\\alpha U(\\alpha)g \\to g\\) gdy \\(\\alpha \\to \\infty\\), zbiór \\(\\mathcal{D}(L)\\) jest gęsty w \\(C_0(S)\\). To uzasadnia (GI1). Dla \\(t &gt; 0\\) oraz \\(f \\in \\mathcal{D}(L)\\) zdefiniujmy \\[\\begin{equation*} g_t = \\left(1 + \\frac{\\lambda}{t}\\right) f - \\frac{\\lambda}{t} T(t)f = f - \\frac{\\lambda}{t}(T(t)f - f). \\end{equation*}\\] Wtedy \\(\\lim_{t \\downarrow 0} g_t = f - \\lambda \\mathcal{L}f\\) i \\[ (1 + \\lambda/t)\\inf_{x \\in S} f(x) \\geq \\frac{\\lambda}{t} \\inf_{x \\in S} T(t) f(x) + \\inf_{x \\in S} g_t(x) \\geq \\frac{\\lambda}{t} \\inf_{x \\in S} f(x) + \\inf_{x \\in S} g_t(x), \\] więc własność (GI3) z Definicji 8 jest spełniona. Drugą nierówność pozostawiamy jako zadanie. Teraz przypuśćmy, że \\(f - \\alpha^{-1} L f = g\\) dla pewnego \\(f \\in \\mathcal{D}(L)\\) oraz \\(\\alpha &gt; 0\\). Przez dowiedzioną już implikację w (16), \\(h = \\alpha U(\\alpha) g\\) spełnia \\(h - \\alpha^{-1} L h = g\\), więc \\(f = h\\) z (13). Aby sprawdzić własność (GI5) z Definicji 8, przypuśćmy, że \\(g_n \\in C(S)\\) spełniają \\(\\sup_n \\|g_n\\| &lt; \\infty\\), oraz że \\(g_n\\) i \\(T(t) g_n\\) są zbieżne punktowo do \\(1\\) dla każdego \\(t\\). Zdefiniujmy \\(f_n \\in \\mathcal{D}(L)\\) przez \\(g_n = f_n - \\lambda L f_n\\). Wtedy \\(f_n = \\alpha U(\\alpha) g_n\\) z (16). Ponieważ \\(T(t) g_n \\to 1\\) punktowo, to \\(f_n \\to 1\\) punktowo przez definicję \\(U(\\alpha)\\) oraz twierdzenie o zbieżności ograniczonej. Aby udowodnić punkt (b) twierdzenia, zauważmy, że \\[ \\frac{\\mathrm{d}}{\\mathrm{d}t} T(t) f = \\lim_{s \\to 0} \\frac{T(t+s)f - T(t)f}{s} = \\lim_{s \\to 0} T(t) \\left( \\frac{T(s)f - f}{s} \\right) = T(t) \\mathcal{L} f = \\mathcal{L} T(t) f, \\] pod warunkiem, że którakolwiek z granic istnieje, ponieważ wyrażenia w środku granic są identyczne. Środkowa granica rzeczywiście istnieje, ponieważ \\(f \\in \\mathcal{D}(L)\\) oraz \\(T(t)\\) jest kontrakcją. W związku z tym pozostałe granice również istnieją. Skoro istnieje trzecia granica, to \\(T(t)f \\in \\mathcal{D}(L)\\) oraz (17) zachodzi. Środkowe wyrażenie w (17) jest ciągłe względem \\(t\\), więc \\(T(t)f\\) jest ciągłe i różniczkowalne. "],["wykład-5-od-generatora-do-półgrupy.html", "Wykład 5: od generatora do półgrupy O notacji słów kilka Od generatora do procesu", " Wykład 5: od generatora do półgrupy 2024-10-31 Piotr Dyszewski O notacji słów kilka Twierdzenie 11 Przy oznaczeniach Twierdzenia @\\(\\ref{thm:3-16}\\), dla \\(f \\in C_0(S)\\) oraz \\(t &gt; 0\\), \\[ \\lim_{n \\to \\infty} \\left(I - \\frac{t}{n} L\\right)^{-n} f = T_tf. \\] Proof. Sprawdzamy indukcyjnie, że \\[ \\left(I - \\alpha^{-1} L\\right)^{-n} f = \\alpha^n U^n(\\alpha)f = \\int_0^{\\infty} \\alpha^n s^{n-1} e^{-\\alpha s} T(s)f \\, \\mathrm{d}s. \\] Stąd \\[\\begin{equation} \\left( I - \\frac{t}{n} L \\right)^{-n} f = \\mathbb{E} T \\left( (\\xi_1 + \\cdots + \\xi_n) t/n \\right) f, \\tag{18} \\end{equation}\\] gdzie \\(\\xi_1, \\xi_2, \\ldots\\) są niezależnymi zmiennymi o standardowym rozkładzie wykładniczym. Zauważmy, że funkcja \\((s, x) \\to T(s)f(x)\\) jest ciągła, więc wartość oczekiwana w (18) jest dobrze określona. Jeśli \\(f \\in \\mathcal{D}(L)\\), to \\[\\begin{equation*} \\frac{\\mathrm{d}}{\\mathrm{d}t} T_t f = T_t Lf \\end{equation*}\\] co w notacji całkowej zapisuje się jako \\[\\begin{equation*} T_tf-T_sf = \\int_s^t T_r Lf \\mathrm{d}r. \\end{equation*}\\] Skoro \\(\\|T_r \\| \\leq 1\\), to ostatnia nierówność implikuje, że \\[ \\| T(t)f - T(s)f \\| \\leq \\| Lf \\| |t - s|. \\] Wracając teraz do (18) otrzymujemy \\[ \\left\\| \\left( I - \\frac{t}{n} L \\right)^{-n} f - T(t)f \\right\\| \\leq t \\| Lf \\| \\mathbb{E} \\left| \\frac{\\xi_1 + \\cdots + \\xi_n}{n} - 1 \\right|. \\] Rezultat dla \\(f \\in \\mathcal{D}(L)\\) wynika teraz z prawa wielkich liczb. Jest on prawdziwy dla wszystkich \\(f \\in C_0(S)\\), ponieważ wszystkie rozważane operatory są kontrakcjami. Remark. Formalnie, \\(T_t = \\exp(tL)\\). Kiedy \\(L\\) jest ograniczone, istnieją przynajmniej trzy sposoby definiowania tego wykładnika: \\[ \\sum_{k=0}^\\infty \\frac{(tL)^k}{k!}, \\quad \\lim_{n \\to \\infty} \\left(I + \\frac{t}{n} L\\right)^n, \\quad \\text{ i } \\quad \\lim_{n \\to \\infty} \\left(I - \\frac{t}{n} L\\right)^{-n}. \\] Ostatni z nich jest jedynym, który ma sens w przypadku nieograniczonym. Teraz rozważmy kilka przykładów. Zadanie 10 Rozważmy proces Fellera polegający na jednostajnym ruchu w prawo. Niech \\(S=\\mathbb{R}\\) i niech \\(\\mathbf{P}_x\\) jest punktową masą na ścieżce \\(\\omega_x\\) danej przez \\(\\omega_x(t) = x + t\\), lub równoważnie, proces z półgrupą \\(T(t)f(x) = f(x + t)\\). Pokaż, że generatorem \\(L\\) tego procesu jest ten opisany w Zadaniu~\\(\\ref{zad:3:13}\\). Upewnij się, że dziedzina dana jest dokładnie przez dziedzinę \\(L\\). Przykład 4 W przypadku ruchu Browna, można zweryfikować, że \\[ U(\\lambda) f(x) = \\int u_\\lambda(y - x) f(y) \\, dy \\] gdzie \\[ u_\\lambda(y - x) = \\int_0^\\infty (2 \\pi t)^{-1/2} \\exp\\left(-\\frac{|y - x|^2}{2t} - \\lambda t\\right) \\, \\mathrm{d}t = \\frac{1}{\\sqrt{2 \\lambda}} \\exp(-|y - x| \\sqrt{2 \\lambda}). \\] Elegancki sposób na uzyskanie tej ostatniej równości polega na użyciu wzoru \\(\\mathbb{E}[e^{-\\lambda T_a}] = e^{-a \\sqrt{2 \\lambda}}\\) dla transformaty Laplace’a czasu trafienia \\(a &gt; 0\\) przez rzeczywisty ruch Browna rozpoczęty z \\(0\\). Różniczkując względem \\(\\lambda\\), otrzymujemy \\[ \\mathbb{E}\\left[T_a e^{-\\lambda T_a}\\right] = \\frac{a}{\\sqrt{2 \\lambda}} e^{-a \\sqrt{2 \\lambda}}, \\] i używając gęstości \\(T_a\\), aby przekształcić \\(\\mathbb{E}[T_a e^{-\\lambda T_a}]\\), dokładnie znajdujemy całkę, która pojawia się w obliczeniach \\(u_\\lambda(y - x)\\). Wiemy, że półgrupa operatorów związana z ruchem Browna jest półgrupą Fellera. Znajdziemy jej generator \\(L\\). Widzieliśmy, że dla każdego \\(\\lambda &gt; 0\\) i \\(f \\in C_0(\\mathbb{R})\\), \\[ U(\\lambda) f(x) = \\int \\frac{1}{\\sqrt{2 \\lambda}} \\exp(-\\sqrt{2 \\lambda} |y - x|) f(y) \\, \\mathrm{d}y. \\] Jeśli \\(h \\in \\mathcal{D}(L)\\), wiemy, że istnieje \\(f \\in C_0(\\mathbb{R})\\) takie, że \\(h = U(\\lambda) f\\). Przyjmując \\(\\lambda = 1/2\\), mamy \\[ h(x) = \\int \\exp(-|y - x|) f(y) \\, \\mathrm{d}y. \\] Różniczkując pod znakiem całki (pozostawiamy uzasadnienie jako zadanie), otrzymujemy, że \\(h\\) jest różniczkowalna na \\(\\mathbb{R}\\), i \\[ h&#39;(x) = \\int \\operatorname{sgn}(y - x) \\exp(-|y - x|) f(y) \\, \\mathrm{d}y \\] gdzie \\(\\operatorname{sgn}(z) = 1_{\\{z &gt; 0\\}} - 1_{\\{z &lt; 0\\}}\\) (wartość \\(\\operatorname{sgn}(0)\\) jest nieistotna). Pokażmy również, że \\(h&#39;\\) jest różniczkowalna na \\(\\mathbb{R}\\). Niech \\(x_0 \\in \\mathbb{R}\\). Następnie, dla \\(x &gt; x_0\\), \\[\\begin{align*} h&#39;(x) - h&#39;(x_0) = &amp; \\int_{\\mathbb{R}} \\left( \\operatorname{sgn}(y - x) \\exp(-|y - x|) - \\operatorname{sgn}(y - x_0) \\exp(-|y - x_0|) \\right) f(y) \\, \\mathrm{d}y \\\\ =&amp; \\int_{x_0}^x \\left( -\\exp(-|y - x|) - \\exp(-|y - x_0|) \\right) f(y) \\, \\mathrm{d}y \\\\ &amp;+ \\int_{\\mathbb{R} \\setminus [x_0, x]} \\operatorname{sgn}(y - x_0) \\left( \\exp(-|y - x|) - \\exp(-|y - x_0|) \\right) f(y) \\, \\mathrm{d}y. \\end{align*}\\] Wynika stąd, że \\[ \\frac{h&#39;(x) - h&#39;(x_0)}{x - x_0} \\xrightarrow{x \\downarrow x_0} -2f(x_0) + h(x_0). \\] Otrzymujemy tę samą granicę, gdy \\(x \\uparrow x_0\\), i stąd uzyskujemy, że \\(h\\) jest dwukrotnie różniczkowalna oraz \\(h&#39;&#39; = -2f + h\\). Z drugiej strony, ponieważ \\(h = U(1/2)f\\), \\[ \\left(\\frac{1}{2} - L\\right) h = f \\] stąd \\(Lh = -f + h/2 = h&#39;&#39;/2\\). Podsumowując, uzyskaliśmy, że \\[ \\mathcal{D}(L) \\subset \\{ h \\in C^2(\\mathbb{R}) : h \\text{ i } h&#39;&#39; \\in C_0(\\mathbb{R}) \\} \\] i że, jeśli \\(h \\in \\mathcal{D}(L)\\), mamy \\(Lh = h&#39;&#39;/2\\). Ostatnie zawieranie jest w rzeczywistości równością. Aby to zobaczyć, możemy postąpić następująco. Jeśli \\(g\\) jest funkcją dwukrotnie różniczkowalną taką, że \\(g\\) oraz \\(g&#39;&#39;\\) należą do \\(C_0(\\mathbb{R})\\), wtedy przyjmujemy \\(f = \\frac{1}{2}(g - g&#39;&#39;) \\in C_0(\\mathbb{R})\\), a więc \\(h = U(1/2)f \\in \\mathcal{D}(L)\\). Z poprzedniego argumentu wynika, że \\(h\\) jest dwukrotnie różniczkowalna oraz \\(h&#39;&#39; = -2f + h\\). Stąd \\((h - g)&#39;&#39; = h - g\\). Ponieważ funkcja \\(h - g\\) należy do \\(C_0(\\mathbb{R})\\), musi zanikać tożsamościowo, co daje \\(g = h \\in \\mathcal{D}(L)\\). Patrząc na Zadanie 9 i Przykład 4, można by się zastanawiać, czy pochodne wyższego rzędu mogą być generatorami infinitezymalnymi. Odpowiedź brzmi, że nie mogą. Główny problem polega na tym, że gdy gładka funkcja osiąga minimum w punkcie wewnętrznym swojej dziedziny, pierwsza pochodna jest zerowa, a druga pochodna jest tam nieujemna. Nic nie można powiedzieć o znakach innych pochodnych w tym miejscu. Zadanie 11 Pokaż, że nie istnieje generator prawdopodobieństwa, którego ograniczenie do gładkich funkcji jest dane przez \\(Lf = f&#39;&#39;&#39;\\). Od generatora do procesu W całym tym rozdziale \\(L\\) będzie generatorem infinitezymalnym. Naszym pierwszym zadaniem jest skonstruowanie odpowiadającej półgrupy prawdopodobieństwa. Aby to zrobić, wprowadzamy aproksymację \\(L_\\epsilon\\) do \\(L\\) dla małego dodatniego \\(\\epsilon\\) przez \\[ L_\\epsilon = L(I - \\epsilon L)^{-1}. \\] Zauważmy, że jest to dobrze zdefiniowane z definicji generatora infinitezymalnego, ponieważ \\(\\mathcal{R}(I - \\epsilon L) = \\mathcal{D}(L)\\). To łatwo zobaczyć z \\[ f - \\epsilon L f = g \\quad \\text{jest równoważne} \\quad f = (I - \\epsilon L)^{-1} g. \\] Ponadto \\[ \\|L_\\epsilon g\\| = \\|L f\\| \\leq \\frac{\\|f\\| + \\|g\\|}{\\epsilon} \\leq \\frac{2}{\\epsilon} \\|g\\|, \\] więc \\(L_\\epsilon\\) jest ograniczonym operatorem. To pozwala zdefiniować \\(T_\\epsilon(t)\\) przez \\[ T_\\epsilon(t) = e^{tL_\\epsilon} = \\sum_{n=0}^{\\infty} \\frac{t^n L_\\epsilon^n}{n!}. \\] Zadanie 12 (a) Pokaż, że dla każdego \\(f \\in C_0(S)\\), \\[\\begin{equation} (I - \\epsilon L)^{-1} f - \\epsilon L f = f. \\tag{19} \\end{equation}\\] (b) Użyj części (a), aby pokazać, że \\(L_\\epsilon\\) jest generatorem infinitezymalnym oraz że \\(T_\\epsilon(t)\\) jest półgrupą prawdopodobieństwa, której generatorem jest \\(L_\\epsilon\\) w sensie Twierdzenia 10. Twierdzenie 12 Dla \\(f \\in C_0(S)\\), \\[ T(t)f = \\lim_{\\epsilon \\to 0} T_\\epsilon(t) f \\] jednostajnie na ograniczonych przedziałach \\(t\\). Definiuje to półgrupę Fellera, której generatorem jest \\(L\\) w sensie Twierdzenia 10. Najpierw sprawdzamy, że \\(L_\\epsilon\\) i \\(L_\\delta\\) komutują dla \\(\\epsilon, \\delta &gt; 0\\). Wynika to z @ref{eq:3-19} oraz \\[ (I - \\epsilon L)^{-1} (I - \\delta L)^{-1} = (I - \\delta L)^{-1} (I - \\epsilon L)^{-1}, \\] co jest prawdziwe, ponieważ \\[ (I - \\epsilon L)^{-1} (I - \\delta L)^{-1} f = g \\quad \\text{jest równoważne} \\quad f = g - (\\epsilon + \\delta)Lg + \\epsilon \\delta L^2 g, \\] co jest symetryczne w \\(\\epsilon\\) i \\(\\delta\\). "],["wykład-6-od-generatora-przez-półgrupę-do-procesu.html", "Wykład 6: od generatora przez półgrupę do procesu", " Wykład 6: od generatora przez półgrupę do procesu 2024-11-07 Piotr Dyszewski W całym tym rozdziale \\((L, \\mathcal{D}(L))\\) będzie generatorem infinitezymalnym. Naszym pierwszym zadaniem jest skonstruowanie odpowiadającej mu półgrupy Fellera. Aby to zrobić, wprowadzamy aproksymację \\(L_\\epsilon\\) zadaną przez \\[L_\\epsilon = L(I - \\epsilon L)^{-1}.\\] Zauważmy, że \\(L_\\epsilon\\) jest dobrze określone ponieważ \\[f \\in \\mathcal{D}(L), \\: f - \\epsilon L f = g \\quad \\text{jest równoważne} \\quad f = (I - \\epsilon L)^{-1} g.\\] Ponadto, przy oznaczeniach jak wyżej, \\[\\|L_\\epsilon g\\| = \\|L f\\| \\leq \\frac{\\|f\\| + \\|g\\|}{\\epsilon} \\leq \\frac{2}{\\epsilon} \\|g\\|\\], więc \\(L_\\epsilon\\) jest ograniczonym operatorem. To pozwala zdefiniować \\(T_\\epsilon(t)\\) przez \\[T_\\epsilon(t) = e^{tL_\\epsilon} = \\sum_{n=0}^{\\infty} \\frac{t^n L_\\epsilon^n}{n!}.\\] Zadanie 13 Pokaż, że dla każdego \\(f \\in C_0(S)\\), \\[\\begin{equation} (I - \\epsilon L)^{-1} f - \\epsilon L_\\epsilon f = f. (\\#eq:3-19) \\end{equation}\\] Użyj części (a), aby pokazać, że \\(L_\\epsilon\\) jest generatorem infinitezymalny,, oraz że \\(T_\\epsilon(t)\\) jest półgrupą Fellera, której generatorem jest \\(L_\\epsilon\\). Twierdzenie 12 Dla \\(f \\in C_0(S)\\), \\[ T(t)f = \\lim_{\\epsilon \\to 0} T_\\epsilon(t) f \\tag{20} \\] jednostajnie na ograniczonych przedziałach \\(t\\). Definiuje to półgrupę Fellera, której generatorem jest \\(L\\). Proof. Najpierw sprawdzamy, że \\(L_\\epsilon\\) i \\(L_\\delta\\) komutują dla \\(\\epsilon, \\delta &gt; 0\\). Aby to sprawdzić zauważmy najpierw, że \\[(I - \\epsilon L)^{-1} (I - \\delta L)^{-1} = (I - \\delta L)^{-1} (I - \\epsilon L)^{-1},\\] co z kolei jest prawdziwe, ponieważ \\[(I - \\epsilon L)^{-1} (I - \\delta L)^{-1} f = g \\quad \\text{jest równoważne} \\quad f = g - (\\epsilon + \\delta)Lg + \\epsilon \\delta L^2 g,\\] co jest symetryczne w \\(\\epsilon\\) i \\(\\delta\\). Korzystając teraz z (19) napiszmy \\[\\begin{gathered} \\epsilon\\delta L_\\epsilon L_\\delta = \\left( (I-\\epsilon L)^{-1}-I \\right) \\left( (I-\\delta L)^{-1} -I \\right) = \\\\ \\left( (I-\\delta L)^{-1}-I \\right) \\left( (I-\\epsilon L)^{-1} -I \\right) = \\epsilon\\delta L_\\delta L_\\epsilon. \\end{gathered}\\] Zauważmy, że dzięki temu operatory \\(L_\\delta\\), \\(L_\\epsilon\\), \\(T_\\epsilon(t)\\) i \\(T_\\delta(t)\\) również komutują. Chcąc pokazać, że \\(T_\\epsilon(t)\\) przy \\(\\epsilon \\to 0\\) rzeczywiście zbiegają będziemy chcieli opierać się na zbieżności \\(L_\\epsilon\\) przy \\(\\epsilon \\to 0\\). Uzasadnimy teraz nierówność, która uzasadni, że takie wnioskowanie jest możliwe. Stosując zasadnicze twierdzenie rachunku różniczkowego i całkowego otrzymujemy, że dla każdej \\(f \\in C_0(S)\\), \\[T_\\epsilon(t)f - T_\\delta(t)f = \\int_0^t \\frac{\\mathrm{d}}{\\mathrm{d}s} [T_\\epsilon(s)T_\\delta(t - s)] f \\, \\mathrm{d}s = \\int_0^t T_\\epsilon(s)T_\\delta(t - s) (L_\\epsilon - L_\\delta) f \\, \\mathrm{d}s.\\] Skoro \\(\\| T_\\epsilon(s) \\| , \\|T_\\delta(t - s) \\| \\leq 1\\) udowodniona właśnie nierówność implikuje, że \\[ \\|T_\\epsilon(t)f - T_\\delta(t)f\\| \\leq t \\|L_\\epsilon f - L_\\delta f\\| (\\#eq:3-21).\\] Wystarczy pokazać zatem, że \\(L_\\epsilon\\) zbiegają. Spodziewamy się, że \\((I-\\epsilon L)^{-1} \\to I\\), co z kolei powinno implikować, że \\(L_\\epsilon \\to L\\). Sprawdzimy teraz formalnie to rozumowanie. Dla \\(f \\in \\mathcal{D}(L)\\) mamy \\[(I - \\epsilon L)^{-1} f - f = \\epsilon (I - \\epsilon L)^{-1} L f\\] Skoro \\(\\|(I-\\epsilon L)^{-1}\\|\\leq 1\\), to \\[\\|(I - \\epsilon L)^{-1} f - f\\| \\leq \\epsilon \\|L f\\|.\\] W szczególności, dla \\(f \\in \\mathcal{D}(L)\\), \\[\\lim_{\\epsilon \\to 0} (I - \\epsilon L)^{-1} f = f.\\] Skoro \\((I-\\epsilon L)^{-1}\\) jest ciągłe a \\(\\mathcal{D}(L)\\) gęste w \\(C_0(S)\\), powyższa zbieżność ma miejsce dla wszystkich \\(f \\in C_0(S)\\). Ponieważ \\[L_\\epsilon f = (I - \\epsilon L)^{-1} L f\\] dla \\(f \\in \\mathcal{D}(L)\\), to \\[ \\lim_{\\epsilon \\to 0} L_\\epsilon f = L f (\\#eq:3-22)\\] dla \\(f \\in \\mathcal{D}(L)\\). Tak więc przez @ref\\tag{21}, granica w (20) istnieje jednostajnie na ograniczonych zbiorach \\(t\\) dla \\(f \\in \\mathcal{D}\\). Ponieważ zbiór \\(\\mathcal{D}(L)\\) jest gęsty w \\(C_0(S)\\) i \\(T_\\epsilon(t)\\) jest kontrakcją, ten sam wniosek jest prawdziwy dla wszystkich \\(f \\in C_0(S)\\). Z udowodnionej właśnie zbieżności wynika, że a \\(T(t)\\) spełnia własność półgrupy oraz wszystkie postulaty procesu Fellera z wyjątkiem ostatniego. Aby uzasadnić, że \\((T_t)_{t \\in \\mathbb{R}_+}\\) spełnia ostatnią własność pokażemy najpierw, że \\[(I - \\alpha L)^{-1} = \\alpha \\int_0^\\infty e^{-\\alpha t} T_t \\mathrm{d} t.\\] Ustalmy w tym celu \\(\\lambda &gt; 0\\) tak małe, że \\(\\mathcal{R}(I - \\lambda L) = C_0(S)\\), i wybierzmy \\(\\alpha\\) tak, że \\(\\alpha \\lambda = 1\\). Ustalmy \\(g \\in C_0(S)\\). Chcąc uzasadnić \\[ (I - \\alpha L)^{-1}g = \\alpha \\int_0^\\infty e^{-\\alpha t} T_tg \\mathrm{d} t \\tag{22} \\] odwołamy się do aproksymacji \\(L\\) i \\(T(t)\\) przez odpowiednio \\(L_\\epsilon\\) oraz \\(T_\\epsilon(t)\\). Niech \\[f_\\epsilon = (I-\\alpha L_\\epsilon)^{-1}g = \\alpha U_\\epsilon(\\alpha)g = \\alpha \\int_0^\\infty e^{-\\alpha t} T_\\epsilon(t)g \\, \\mathrm{d}t.\\] Z (20), \\[\\lim_{\\epsilon \\to 0^+} f_\\epsilon = f, \\qquad f= \\alpha \\int_0^\\infty e^{-\\alpha t} T(t)g \\, \\mathrm{d}t.\\] Połóżmy \\(h_\\epsilon = (I - \\epsilon L)^{-1} f_\\epsilon\\). Wówczas z (19), \\(\\lim_{\\epsilon \\to 0} h_\\epsilon = f\\). Niech wreszcie \\(h = (I-\\lambda L)^{-1} g\\). Naszym aktualnym celem (22) we wprowadzonej właśnie notacji jest pokazać, że \\(f=h\\). Z definicji \\(L_\\epsilon\\), \\[\\begin{gathered} \\lambda L h_\\epsilon = L_\\epsilon f_\\epsilon = \\alpha\\int_0^\\infty e^{-\\alpha t} L_\\epsilon T_\\epsilon(t) f \\mathrm{d}t= \\alpha\\int_0^\\infty e^{-\\alpha t} \\frac{\\mathrm{d}}{\\mathrm{d}t} T_\\epsilon(t) f \\mathrm{d}t \\\\ -\\alpha g+\\alpha^2\\int_0^\\infty e^{-\\alpha t}T_\\epsilon(t) f \\mathrm{d}t = \\alpha(f_\\epsilon-g) \\rightarrow \\alpha (f - g) \\end{gathered}\\] gdy \\(\\epsilon \\to 0\\). Dlatego, \\[\\lim_{\\epsilon \\to 0} [(h_\\epsilon - h) - \\lambda L(h_\\epsilon - h)] = 0,\\] z czego z kolei wynika, że \\(h_\\epsilon - h \\to 0\\). Stąd \\(f = h\\). Zastosowanie (22) do \\(g_n\\) i \\(f_n =(I-\\lambda L)^{-1}g_n\\) z Definicji 8 widzimy, że \\[\\lim_{n \\to \\infty} \\alpha \\int_{0}^{\\infty} e^{-\\alpha t} (T(t)g_n)(x) \\mathrm{d}t =\\lim_{n \\to \\infty }f_n(x)= 1\\] dla każdego \\(x \\in S\\). Niech \\(\\mu_{t,x}\\) będzie miarą zdefiniowaną przez \\[T(t)f(x) = \\int f(y) \\mu_{t,x}(\\mathrm{d} y).\\] Skoro \\(T(t)\\) jest ograniczonym operatorem, to miara \\(\\mu_{t,x}\\) jest skończona. Jako, że \\(g_n \\to 1\\) punktowo, to twierdzenia o zbieżności ograniczonej mamy więc \\[T(t)g_n(x) \\to \\mu_{t,x}(S).\\] Stąd \\[1 = \\alpha \\int_{0}^{\\infty} e^{-\\alpha t} \\mu_{t,x}(S) \\mathrm{d} t.\\] Innymi słowy transformatą Laplace’a funkcji \\(t \\mapsto \\mu_{t,x}(S)\\) jest \\(\\alpha \\to 1/\\alpha\\). Widzimy więc, że \\(\\mu_{t,x}(S) \\equiv 1\\). Czyli \\(T(t) g_n \\to 1\\) punktowo. Oznacza to, że spełniona jest ostatnia własność w Definicji ??. Wreszcie, sprawdzamy, że ta półgrupa Fellera \\((T_t)_{t \\in \\mathbb{R}_+}\\) ma generator \\(L\\). Mamy \\[T_\\epsilon(t)f - f = \\int_{0}^{t} \\frac{\\mathrm{d}}{\\mathrm{d}s} T_\\epsilon(s) \\mathrm{d}s = \\int_{0}^{t} T_\\epsilon(s)L_\\epsilon f \\mathrm{d}s.\\] Jeśli \\(f \\in \\mathcal{D}(L)\\), to (20), @ref[eq:3-22) oraz własność kontrakcji \\(T_\\epsilon(t)\\) implikują, że można przejść do granicy w tym równaniu, aby uzyskać \\[T(t)f - f = \\int_{0}^{t} T(s)Lf \\mathrm{d}s.\\] Zatem \\[\\lim_{t \\to 0} \\frac{T(t)f - f}{t} = Lf \\text{ dla } f \\in \\mathcal{D}(L).\\] Pokazaliśmy właśnie, że \\[\\mathcal{D}(L) \\subseteq \\mathcal{D}&#39;(L) = \\left\\{ f \\in C_0(S) \\: :\\: \\lim_{t \\to 0} (T(t)f - f)/t \\text{ istnieje } \\right\\}.\\] Wiemy, że obie pary \\((L, \\mathcal{D}(L))\\) oraz \\((L, \\mathcal{D}&#39;(L))\\) są generatorami infinitezymalnymi. Niech \\(f \\in \\mathcal{D}&#39;(L)\\). Rozważmy \\(g = (I-\\lambda L)f\\). Istnieje \\(h \\in \\mathcal{D}(L)\\) takie, że \\(g = (I-\\lambda L)h\\). Stąd \\((I-\\lambda L)(f-h) =0\\) a co za tym idzie \\(f = h \\in \\mathcal{D}(L)\\). Pokazaliśmy właśnie, że \\(\\mathcal{D}(L) = \\mathcal{D}&#39;(L)\\). Twierdzenie 13 Jeśli \\((T_t)_{t \\geq 0}\\) jest półgrupą Fellera, to istnieje proces Fellera \\((\\mathbf{P}, \\mathbb{F})\\) spełniający \\[\\mathbf{E}_x f(X(t)) = T_tf(x)\\] dla \\(x \\in S, t \\geq 0\\), oraz \\(f \\in C_0(S)\\). Proof. Ustalmy \\(x \\in S\\). Pokażemy najpierw, że istnieje proces \\(Y(t), t \\in \\mathbb{Q}^+\\) na pewnej przestrzeni probabilistycznej, który ma pożądane rozkłady skończenie wymiarowe oraz \\(Y(0) = x\\). Dla dowolnego \\(n \\in \\mathbb{N}\\) i dowolnych \\(0 \\leq t_1&lt; t_2 &lt;\\ldots &lt; t_n\\) definiujemy rozkład \\(\\mu_{t_1, \\ldots, t_n}\\) na \\(S^n\\) poprzez następujący wymóg: dla dowolnych \\(f_1, \\ldots, f_n\\) z \\(C_0(S)\\) zachodzi \\[\\begin{equation} \\int f_1(y_1) f_2(y_2) \\cdots f_n(y_n) \\mu_{t_1, \\ldots, t_n}(\\mathrm{d}(y_1, \\ldots y_n)) = T_{t_1} \\left( f_1 T_{t_2-t_1} \\left( f_2 T_{t_3-t_2} ( \\ldots) \\right) \\right)(x).(\\#eq:3-semiii)\\end{equation}\\] Funkcje o rozdzielonych zmiennych \\(f_1(y_1) f_2(y_2) \\cdots f_n(y_n)\\) są liniowo gęste w \\(C\\left( S^n \\right)\\). Miara \\(\\mu_{t_1, \\ldots, t_n}\\) jest więc dobrze określona (istnieje dokładnie jedna spełniająca zadany warunek). Aby powołać się na twierdzenie Kołmogorowa o istnieniu procesu należy uzasadnić, że skonstruowane w ten sposób miary są zgodne. Dokładniej, że dla dowolnego \\(j\\leq n\\), i dowolnych borelowskich \\(A_1, \\ldots , A_n \\in S\\), \\[\\begin{gathered} \\mu_{t_1, \\ldots, t_n}(A_1 \\times \\cdots \\times A_{j-1} \\times S \\times A_{j+1} \\times \\cdots A_n) = \\\\ \\mu_{t_1, \\ldots, t_{j-1}, t_{j+1} \\ldots t_n}(A_1 \\times \\cdots \\times A_{j-1} \\times A_{j+1} \\times \\cdots A_n). \\end{gathered}\\] Wynika to z (??) dla \\(f_j\\equiv 1\\) i zastosowaniu własności półgrupy \\((T_t)_{t \\in \\mathbb{R}_+}\\). Zastosowanie twierdzenia Kołmogorowa pozwala wywnioskować istnienie procesu o zadanych rozkładach skończenie wymiarowych. Jednakże nie daje ono żadnej kontroli nad regularnością trajektorii. Dlatego jesteśmy zmuszeni przeprowadzić dodatkową konstrukcję. Stosując konstrukcję w poprzedniego paragrafu dla wymiernych \\(t\\) otrzymujemy proces \\((Y_q^{(x)} \\: : \\: q \\in \\mathbb{Q}_+)\\) o rozkładach zadanych rozkładach skończenie wymiarowych: \\[\\mathbb{P}[Y_{t_1}^{(x)}\\in A_1, \\ldots , Y_{t_n}^{(x)} \\in A_n] = \\mu_{t_1, \\ldots, t_n}(A_1 \\times \\cdots \\times A_n)\\] oraz taki, że \\(\\mathbb{P}[Y^{(x)}_0=x]=1\\). Proces ten ma własność Markowa. Z definicji miary \\(\\mu_{t_1, \\ldots, t_n}\\), \\[\\mathbb{E} [f_1(Y_{t_1}^{(x)}) \\cdots f_n(Y_{t_n}^{(x)}) ] = \\mathbb{E} [f_1(Y_{t_1}^{(x)}) \\cdots f_{n-1}(Y_{t_{n-1}}^{(x)}) T_{t_{n} - t_{n-1}} f_n (Y_{n-1}^{(x)}) ]\\] Powyższe implikuje \\[\\mathbb{E} [f_n(Y_{t_n}^{(x)}) | Y_q^{(x)}, q \\leq t_{n-1}] = T_{t_n-t_{n-1}} f_n(Y_{t_{n-1}}^{(x)})\\] W analogiczny sposób uzasadniamy własność Markowa. Jeśli \\(f \\in C_0(S)\\) jest nieujemna, to \\[e^{-\\alpha t} T(t) U(\\alpha) f = \\int_{t}^{\\infty} e^{-\\alpha s} T(s) f \\mathrm{d}s \\leq U(\\alpha) f.\\] Stąd wynika, że \\(e^{-\\alpha t} U(\\alpha) f(Y(t))\\) jest (ograniczonym) supermartingałem względem filtracji \\(\\{\\mathcal{F}_t, t \\in \\mathbb{Q}^+\\}\\), gdzie \\(\\mathcal{F}_t\\) jest generowana przez \\[(Y(s), s \\in \\mathbb{Q} \\cap [0,t]).\\] Rzeczywiście mamy \\[\\mathbb{E}[ e^{-\\alpha(t+s)} U(\\alpha) f(Y_{t+s}) | \\mathcal{F}_s] = e^{-\\alpha (t+s)} T_t U(\\alpha) f(Y_s) \\leq e^{-\\alpha s} U(\\alpha) f(Y_s).\\] Wówczas prawdopodobieństwem jeden, prawostronne i lewostronne granice tego supermartingału wzdłuż liczb wymiernych istnieją w każdym \\(t \\in [0, \\infty)\\). Dowód twierdzenia o zbieżności nadmartynggałów sprowadza się pokazania, że w prawdopodobieństwem jeden liczba przejść w dół przez każdy przedział o końcach wymiernych jest skończona. Każda funkcja zmiennej rzeczywistej o tej własności musi mieć lewostronną i prawostronną granicę w każdym punkcie. Dla każdej \\(\\alpha\\) i każdej nieujemnej \\(f\\) proces \\(e^{-\\alpha t} U(\\alpha) f(Y_t)\\) ma lewostronne i prawostronne granice. Oczywiście, zbiór o prawdopodobieństwie zero, na którym to się nie udaje, może zależeć od \\(f\\) i \\(\\alpha\\). Musimy więc ograniczyć naszą uwagę do gęstego, przeliczalnego zbioru \\(f\\) i \\(\\alpha\\), aby stwierdzić istnienie prawostronnych i lewostronnych granic procesu \\(Y(s)\\) samego w sobie. Ponieważ \\(\\alpha U(\\alpha) f \\to f\\) gdy \\(\\alpha \\to \\infty\\), oraz \\(C_0(S)\\) jest ośrodkową przestrzenią metryczną, możemy po prostu użyć funkcji \\(U(\\alpha) f\\), gdzie \\(\\alpha\\) jest liczbą całkowitą, oraz \\(f\\) jest wzięte z pewnego przeliczalnego gęstego zbioru w \\(C_0(S)\\). Istnieje nadal jeden problem, że te prawostronne i lewostronne granice mogą być jednopunktową kompaktyfikacją \\(S\\), i wykluczenie tego przypadku wymaga dodatkowego argumentu. Chodzi o to, żeby uzasadnić, że \\(Y\\) nie ucieka do nieskończoności. Niech \\(f \\in C_0(S)\\) będzie ściśle dodatnie. Rozważmy supermartingał \\(M(t) = e^{-tU(1)f(Y(t))}\\). To jest ściśle dodatnie, więc \\(\\inf_{s \\in \\mathbb{Q} \\cap [0,t]} M(s) &gt; 0\\) prawie na pewno dla każdego \\(t &gt; 0\\) (zadanie). Oznacza to, że z prawdopodobieństwem jeden \\(Y_s\\) dla \\(s \\leq t\\) są zawarte w zbiorze zwartym. Wiemy teraz, że z prawdopodobieństwem jeden, \\(Y(s)\\) ma prawostronne i lewostronne granice w każdym \\(t \\in [0, \\infty)\\). Więc możemy zdefiniować \\[X(t) = \\lim_{\\substack{s \\downarrow t \\\\ s \\in \\mathbb{Q}}} Y(s).\\] To jest automatycznie prawostronnie ciągłe i ma lewostronne granice. Ponieważ skończenie wymiarowe rozkłady odpowiadające \\(T(t)\\) są słabo ciągłe, \\(X(t)\\) ma poprawne skończenie wymiarowe rozkłady. "],["wykład-7-rozkłady-stacjonarne-zaburzenia-ruchu-browna.html", "Wykład 7: Rozkłady stacjonarne, zaburzenia ruchu Browna Rozkłady stacjonarne Zaburzenia ruchu Browna", " Wykład 7: Rozkłady stacjonarne, zaburzenia ruchu Browna Rozkłady stacjonarne Interesować nas będzie asymptotyczne zachowanie procesów Fellera. Podobnie jak w przypadku łańcuchów Markowa w czasie dyskretnym rozkłady graniczne są niezmiennicze ze względu na funkcje przejścia. Przez \\(\\Sigma\\) oznaczać będziemy \\(\\sigma\\)-ciało zbiorów borelowskich \\(S\\), czyli najmniejsze \\(\\sigma\\)-ciało zawierające wszystkie otwarte podzbiory \\(S\\). Skoro \\(S\\) jest ośrodkowa, to \\(\\Sigma\\) jest generowane przez wszystkie kule otwarte. Dla Procesu Fellera \\((\\mathbf{P}, \\mathbb{F})\\) oraz rozkładu prawdopodobieństwa \\(\\mu\\) na \\(S\\) definiujemy miarę probabilistyczną \\(\\mathbf{P}_\\mu\\) na \\((S, \\Sigma)\\) wzorem \\[\\mathbf{P}_\\mu[A] = \\int_S \\mathbf{P}_x[A] \\: \\mu(\\mathrm{d}x), \\qquad A \\in \\mathcal{F}.\\] W tym miejscu zachęcamy czytelnika do wprawdzenia, że odwzorowanie \\(x \\mapsto \\mathbf{P}_x[A]\\) jest mierzalne dla \\(A \\in \\mathcal{F}\\). Miara \\(\\mathbf{P}_\\mu\\) to rozkład procesu Markowa przy rozkładzie początkowym \\(\\mu\\). Definicja 9 Niech \\((\\mathbf{P}, \\mathbb{F})\\) będzie procesem Fellera Rozkład prawdopodobieństwa \\(\\pi\\) na \\((S, \\Sigma)\\) nazywamy rozkładem stacjonarnym jeżeli \\[\\mathbf{P}_\\pi \\left[ X(t) \\in A \\right] = \\pi(A)\\] dla każdego \\(A \\in \\Sigma\\). Chcielibyśmy wiedzieć, jak określić na podstawie generatora, czy miara prawdopodobieństwa na \\(S\\) jest stacjonarna dla procesu Fellera. Z tego powodu przepiszemy powyższą definicję w terminach półgrupy. Jeśli \\(\\mu\\) jest miarą prawdopodobieństwa na \\(S\\), rozkład procesu w czasie \\(t\\), gdy rozkład początkowy jest \\(\\mu\\), oznaczamy przez \\(\\mu T(t)\\). Spełnia on zależność \\[\\int f \\, \\mathrm{d}(\\mu T(t)) = \\int T(t)f \\, \\mathrm{d} \\mu = \\mathbf{E}_\\mu \\left[ f(X(t)) \\right]\\] dla \\(f \\in C_0(S)\\). Tutaj \\(\\mathbf{E}_\\mu\\) to wartość oczekiwana odpowiadająca \\(\\mathbf{P}_\\mu\\). Równoważnie \\[\\mathbf{E}_\\mu[Y] = \\int_X \\mathbf{E}_x[Y] \\: \\mu(\\mathrm{d}x)\\] dla każdej ograniczonej zmiennej losowej \\(Y \\colon \\Omega \\to \\mathbb{R}\\). Definicja 10 Miara prawdopodobieństwa \\(\\mu\\) na \\(S\\) jest stacjonarna dla procesu Fellera z półgrupą \\(T(t)\\), jeśli \\(\\mu T(t) = \\mu\\) dla wszystkich \\(t \\geq 0\\), tzn. jeśli \\[\\label{eq:3:28} \\int T(t)f \\, \\mathrm{d}\\mu = \\int f \\, \\mathrm{d}\\mu \\quad \\text{dla wszystkich} \\ f \\in C_0(S) \\ \\text{i} \\ t \\geq 0.\\] Zadanie 14 Pokaż, że jeśli \\(\\mu\\) jest miarą prawdopodobieństwa na \\(S\\) i \\(\\mu T(t) \\Rightarrow \\nu\\), to \\(\\nu\\) jest stacjonarna. Twierdzenie 14 Miara prawdopodobieństwa \\(\\mu\\) na \\(S\\) jest stacjonarna dla odpowiadającego procesu wtedy i tylko wtedy, gdy \\[\\int Lf \\, \\mathrm{d}\\mu = 0 \\quad \\text{dla wszystkich} \\ f \\in D.\\] Proof. Przypuśćmy, że \\(\\mu\\) jest stacjonarna, i weźmy \\(f \\in \\mathcal{D}(L)\\). Wtedy \\[\\int Lf \\, \\mathrm{d}\\mu = \\int \\lim_{t \\to 0} \\frac{T(t)f - f}{t} \\, \\mathrm{d}\\mu = \\lim_{t \\to 0} \\frac{\\int T(t)f \\mathrm{d}\\mu - \\int f \\, \\mathrm{d}\\mu}{t} = 0.\\] Przeciwnie, przypuśćmy, że \\(\\int Lf \\, \\mathrm{d}\\mu = 0\\) dla wszystkich \\(f \\in \\mathcal{D}(L)\\) i jeśli \\(f \\in \\mathcal{D}(L)\\) oraz \\(f - \\lambda Lf = g\\), to \\(\\int f \\, d\\mu = \\int g \\, d\\mu\\). Iterując to, otrzymujemy \\[\\int (I - \\lambda L)^{-n}g \\, \\mathrm{d}\\mu = \\int g \\, \\mathrm{d}\\mu.\\] Biorąc \\(\\lambda = t/n\\) i przechodząc z \\(n \\to \\infty\\) wnioskujemy, że \\[\\int T(t)g \\, \\mathrm{d}\\mu = \\int g \\, \\mathrm{d}\\mu.\\] Oto wystarczający warunek na istnienie rozkładu stacjonarnego. Twierdzenie 15 Jeśli \\(S\\) jest przestrzenią zwartą, to istnieje miara stacjonarna. Proof. Proof. Rozważmy proces Fellera z dowolnym rozkładem początkowym \\(\\mu\\). Niech \\(\\nu_n\\) będzie rozkładem zmiennej Zdefiniujmy miarę \\(\\nu_n\\) na \\(S\\) poprzez warunek \\[\\int_S f(y) \\nu_n( \\mathrm{d}y) = \\mathbb{E} \\left[ \\mathbf{E}_\\mu \\left[ f(X_{nU}) \\right] \\right]= \\mathbb{E} \\left[ \\int_S T_{nU}f(y) \\: \\mu(\\mathrm{d}y) \\right].\\] Dla \\(f \\in C_0(S)\\), własność półgrupy daje \\[\\int T(t)f(y) \\, \\nu_n (\\mathrm{d}y)= \\mathbb{E} \\left[ \\int_S T_{nU+t}f(y) \\: \\mu(\\mathrm{d}y) \\right].\\] tak że \\[\\begin{gathered} \\int f \\, \\mathrm{d}\\nu_n - \\int T(t)f \\, \\mathrm{d}\\nu_n = \\int f \\, \\mathrm{d}\\nu_n - \\int f \\, \\mathrm{d}(\\nu_n T(t))\\\\ = \\frac{1}{n} \\left[ \\int_0^t \\int_S T(r)f \\, \\mathrm{d}\\mu \\, \\mathrm{d}r - \\int_n^{n+t} \\int_S T(r)f \\, \\mathrm{d}\\mu \\, \\mathrm{d}r \\right]. \\end{gathered}\\] Prawa strona @(#eq:3:29) dąży do zera gdy \\(n \\to \\infty\\). Teraz, ponieważ \\(S\\) jest zwarty, twierdzenie Prochorowa, implikuje, że istnieje podciąg \\(\\nu_{n_k}\\) taki, że \\[\\nu_{n_k} \\Rightarrow \\nu\\] dla pewnej miary prawdopodobieństwa \\(\\nu\\) na \\(S\\). Zatem, ponieważ \\(T(t)f \\in C(S)\\), możemy przejść do granicy w @ref\\tag{23} wzdłuż ciągu \\(\\nu_{n_k}\\), aby otrzymać \\[\\int f \\, \\mathrm{d}\\nu = \\int T(t)f \\, \\mathrm{d}\\nu.\\] Ponieważ to zachodzi dla wszystkich \\(f \\in C_0(S)\\), wynika stąd, że \\(\\nu T(t) = \\nu\\). Zaburzenia ruchu Browna Przykład 5 Rozważmy ruch Browna na \\([0, \\infty)\\) z absorpcją w 0. Niech \\(\\tau\\) będzie czasem pierwszego uderzenia w 0. Zdefiniujmy \\[X_a(t) = \\begin{cases} X(t) &amp; \\text{jeśli } t &lt; \\tau, \\\\ 0 &amp; \\text{jeśli } t \\ge \\tau, \\end{cases}\\] oraz oznaczmy przez \\(L_a\\) i \\(T_a(t)\\) odpowiednio generator i półgrupę. Dla \\(f \\in C_0[0, \\infty)\\), niech \\(f_o\\) będzie „nieparzystym” przedłużeniem \\(f\\) na \\(\\mathbb{R}\\): \\[f_o(x) = \\begin{cases} f(x) &amp; \\text{jeśli } x \\ge 0, \\\\ 2f(0) - f(-x) &amp; \\text{jeśli } x &lt; 0. \\end{cases}\\] Z zasady odbicia dla każdej \\(g \\in C_0[0,\\infty)\\), \\[\\mathbf{E}_x \\left[g(X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right] = \\mathbf{E}_x \\left[g(-X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right].\\] Biorąc \\(g=f_o\\), \\[\\mathbf{E}_x \\left[f_o(X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right] = \\mathbf{E}_x \\left[f_o(-X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right].\\] Obie te wielkości są równe \\[\\frac{1}{2} \\mathbf{E}_x \\left[(f_o(X(t)) + f_o(-X(t)) ) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right].\\] Ostatnie wyrażenie, z definicji \\(f_o\\) jest równe \\[f(0)\\mathbf{P}_x(t \\ge \\tau).\\] Podsumowując dla \\(x \\ge 0\\), \\[T_a(t)f(x) = \\mathbf{E}_x \\left[f(X(t)) \\mathbf{1}_{\\{ t &lt; \\tau \\}} \\right] + f(0)\\mathbf{P}_x(t \\ge \\tau) = \\mathbf{E}_x f_o(X(t)).\\] Oczywiście \\(f_o \\notin C(\\mathbb{R})\\) o ile \\(f(0) = 0\\). Niemniej jednak, skoro \\[f_o&#39;&#39;(x) = \\begin{cases} f&#39;&#39;(x) &amp; \\text{jeśli } x &gt; 0, \\\\ -f&#39;&#39;(-x) &amp; \\text{jeśli } x &lt; 0, \\end{cases}\\] wtedy \\(f&#39;&#39;(0) = 0\\) jest potrzebne, aby \\(f_o&#39;&#39;\\) było ciągłe. Wynika z tego, że \\[\\mathcal{D}(L_a) = \\{f \\in C_0[0, \\infty) : f&#39;&#39; \\in C[0, \\infty), f&#39;&#39;(0) = 0\\},\\] a dla \\(f \\in \\mathcal{D}(L_a)\\), \\(L_a f = \\frac{1}{2} f&#39;&#39;\\). Przykład 6 Rozważmy ruch Browna na \\([0, \\infty)\\) z odbiciem w \\(0\\). Proces ten jest zdefiniowany jako \\[X_r(t) = |X(t)|,\\] a jego generator i półgrupa będą oznaczane odpowiednio przez \\(L_r\\) i \\(T_r(t)\\). Jeśli \\(f \\in C_0[0, \\infty)\\), niech \\(f_e\\) będzie parzystym przedłużeniem \\(f\\) na \\(\\mathbb{R}\\): \\[f_e(x) = \\begin{cases} f(x) &amp; \\text{jeśli } x \\ge 0, \\\\ f(-x) &amp; \\text{jeśli } x &lt; 0. \\end{cases}\\] Wtedy \\[T_r(t)f(x) = \\mathbf{E}_x \\left[f(|X(t)|)\\right] = \\mathbf{E}_x f_e(X(t)) \\quad \\text{dla } x \\ge 0.\\] Zatem, \\[f \\in \\mathcal{D}( L_r) \\iff f_e \\in \\mathcal{D}(L).\\] Wynika z tego, że \\[\\mathcal{D}(L_r) = \\{f \\in C[0, \\infty) : f&#39;, f&#39;&#39; \\in C[0, \\infty), f&#39;(0) = 0\\},\\] a dla \\(f \\in \\mathcal{D}(L_r)\\), \\(L_r f = \\frac{1}{2} f&#39;&#39;\\). Przykład 7 Zaprezentujemy teraz ruch Browna na \\([0, \\infty)\\) z lepkim \\(0\\). Dla \\(c &gt; 0\\), rozważmy operator \\(L_c\\) zdefiniowany jako \\(L_c f = \\frac{1}{2} f&#39;&#39;\\) na \\[\\mathcal{D}(L_c) = \\{f \\in C_0[0, \\infty) : f&#39;&#39; \\in C[0, \\infty), f&#39;(0) = c f&#39;&#39;(0)\\}.\\] Zauważmy, że graniczne przypadki \\(c \\downarrow 0\\) i \\(c \\uparrow \\infty\\) odpowiadają odpowiednio odbiciu i absorpcji w \\(0\\). Jest to generator prawdopodobieństwa — dowód jest pozostawiony jako ćwiczenie. Oto weryfikacja własności (d) w Definicji 2: Dla \\(g \\in C_0[0, \\infty)\\) i \\(\\lambda &gt; 0\\), musimy rozwiązać \\(f - \\lambda L_c f = g\\) dla \\(f \\in \\mathcal{D}(L_c)\\). Niech \\(f_a \\in \\mathcal{D}(L_a)\\) oraz \\(f_r \\in \\mathcal{D}(L_r)\\) będą rozwiązaniami \\[f_a - \\lambda L_a f_a = g \\quad \\text{oraz} \\quad f_r - \\lambda L_r f_r = g.\\] Ponieważ wszystkie trzy generatory są równe \\(\\frac{1}{2} f&#39;&#39;\\) na swoich dziedzinach, \\[f = \\gamma f_a + (1 - \\gamma) f_r\\] jest wymaganym rozwiązaniem, pod warunkiem że \\(f&#39;(0) = c f&#39;&#39;(0)\\). Ma to miejsce, gdy \\(\\gamma\\) spełnia \\[\\gamma f_a&#39;(0) = c (1 - \\gamma) f_r&#39;&#39;(0).\\] Aby znaleźć wartość \\(\\gamma\\), \\(f_a&#39;(0)\\) i \\(f_r&#39;&#39;(0)\\) muszą mieć ten sam znak. Aby to sprawdzić, rozważmy \\(h = f_a - f_r\\). Wtedy \\(h - \\frac{\\lambda}{2} h&#39;&#39; \\equiv 0\\), więc, ponieważ \\(h\\) jest ograniczone, \\[h(x) = h(0) e^{-x \\sqrt{2/\\lambda}}.\\] Wynika z tego, że \\[f_a&#39;(0) = -\\sqrt{2/\\lambda} h(0)\\] oraz \\[f_r&#39;&#39;(0) = -\\left(\\frac{2}{\\lambda}\\right) h(0),\\] więc mają ten sam znak, i \\[\\gamma = \\frac{2c}{2c + \\sqrt{2\\lambda}}.\\] Aby powiedzieć coś o zachowaniu tego procesu, gdy odwiedza 0, napiszmy \\[f = \\alpha U_c(\\alpha) g,\\] gdzie \\(\\alpha = \\lambda^{-1}\\), a \\(U_c\\) jest rozwiązaniem dla procesu \\(X_c(t)\\) z generatorem \\(L_c\\). Można to zapisać jako \\[f(x) = \\frac{2c f_a(x) + \\sqrt{2\\lambda} f_r(x)}{2c + \\sqrt{2\\lambda}} = \\alpha \\int_{0}^{\\infty} e^{-\\alpha t} \\mathbf{E}_x g(X_c(t)) \\,\\mathrm{d}t.\\] Zastosujmy tę tożsamość do ciągu funkcji \\(g\\), które są nieujemne i rosną do \\(1_{(0, \\infty)}\\). Odpowiadające im \\(f\\), \\(f_a\\), oraz \\(f_r\\) rosną odpowiednio do \\[\\alpha \\int_{0}^{\\infty} e^{-\\alpha t} \\mathbf{P}_x(X_c(t) &gt; 0) \\,\\mathrm{d}t, \\quad \\alpha \\int_{0}^{\\infty} e^{-\\alpha t} \\mathbf{P}_x(X_a(t) &gt; 0) \\,\\mathrm{d}t,\\] oraz \\(1\\). Biorąc \\(x = 0\\), otrzymujemy \\[\\mathbf{E}_0 \\int_{0}^{\\infty} \\alpha e^{-\\alpha t} 1_{\\{X_c(t) &gt; 0\\}} \\,\\mathrm{d}t = \\frac{1}{1 + c\\sqrt{2\\alpha}}.\\] Zatem miara Lebesgue’a zbioru \\(\\{t \\ge 0 : X_c(t) = 0\\}\\) jest dodatnia, w przeciwieństwie do przypadku procesu odbijanego, który odpowiada \\(c = 0\\). "],["wykład-8-układy-spinowe.html", "Wykład 8: Układy spinowe Motywacje Systemy spinowe Dygresja analityczna Konstrukcja systemów spinowych", " Wykład 8: Układy spinowe 2024-11-21 Piotr Dyszewski Motywacje Aby umotywować nasze przyszłe działania przedyskutujemy kilka przykładów. Od tej pory niech \\(G =(V, E)\\) będzie przeliczalnym grafem prostym o ograniczonym stopniu. Dokładniej zakładać będziemy, że zbiór jego wierzchołków \\(V\\) jest skończony bądź przeliczalny oraz, że \\[\\sup_{x \\in V} \\mathrm{deg}(x) &lt;\\infty.\\] Rozważmy następujące trzy procesy na \\(G\\). Przykład 8 (Voter model). Przypuśćmy, że na \\(G\\) są dwie wzajemnie zwalczające się frakcje (można myśleć o republikanach i demokratach). Na każdy wierzchołek jednej frakcji oddziałują (poprzez indoktrynację) sąsiedzi z frakcji przeciwnej. Na skutek czego niektóre wierzchołki zmieniają frakcję na przeciwną. Dokładniej każdy \\(x \\in V\\) zmienia frakcję w intensywnością równą \\[c(x) = \\# \\{ y \\in V \\: \\: x \\sim y, \\: \\mbox{$y$ jest z innej frakcji niż $x$} \\}.\\] Jest to równoważne z następującym opisem. Dla każdej pary sąsiednich wierzchołków \\(x\\sim y\\) pochodzących z różnych frakcji losujemy niezależnie dwie liczby \\(E_{x\\to y}\\) oraz \\(E_{y\\to x}\\) z rozkładu wykładniczego z parametrem jeden. \\(E_{x\\to y}\\) interpretujemy jako czas, jaki potrzebuje \\(x\\) aby przekabacić \\(y\\). W momencie, w którym którykolwiek z wierzchołków przeciągnie sąsiada (powiedzmy \\(y\\)) na swoją stronę, losujemy nowe wagi dla sąsiadów \\(y\\) (według nowego układu obu frakcji). Oczywiście powyższa, naiwna konstrukcja ma sens tylko, gdy graf \\(G\\) jest skończony. W przypadku grafów nieskończonych wymagana jest odpowiednia konstrukcja, którą opiszemy niebawem. Przykład 9 (Contact process). Załóżmy, że na grafie \\(G\\) panuje epidemia. Niech \\(\\lambda &gt;0\\) będzie ustalonym parametrem. Proces rozwija się według następujących zasad. Chore wierzchołki niezależnie zarażają swoich zdrowych sąsiadów z czasem wykładniczym z parametrem \\(\\lambda\\). Chore wierzchołki zdrowieją niezależnie z czasem wykładniczym z parametrem jeden. Ponownie powyższy opis ma sens jedynie w przypadku, gdy \\(G\\) jest skończony. Dla nieskończonych grafów \\(G\\) będziemy posługiwać się równoważną charakteryzacją Każdy chory wierzchołek zdrowieje z intensywnością \\(1\\) Każdy zdrowy wierzchołek \\(x\\) choruje z intensywnością \\[\\lambda \\# \\{ y\\sim x \\: : \\: y \\mbox{ chory}\\}.\\] Przykład 10 (Exclusion process). Rozważmy zjawisko migracji na grafie \\(G\\). Załóżmy, że pewne wierzchołki \\(G\\) są zajmowane przez osobników ustalonego gatunku (przykładowo krowy). Każdy osobnik czeka niezależnie wykładniczy czas z parametrem jeden, po czym podejmuje próbę przemieszczenia się. Jeżeli osobnik zajmuje wierzchołek \\(x \\in V\\), to losuje wierzchołek \\(y\\) z prawdopodobieństwem \\(p(x,y)\\). Jeżeli wierzchołek \\(y\\) nie jest zajęty, to osobnik przechodzi do \\(y\\). W każdym z powyższych przykładów stan układu w chwili \\(t\\) można zakodować przy pomocy \\(\\eta_t \\in \\{ 0,1\\}^V\\). Okazuje się, że wówczas \\(\\eta = (\\eta_t)_{t \\in \\mathbb{R}_+}\\) staje się procesem Fellera. Aby się o tym dokładnie przekonać musimy przeanalizować generatory wynikające z przykładów. Systemy spinowe Właściwością, która odróżnia systemy spinowe od innych procesów Fellera na \\(\\{0,1\\}^S\\), jest to, że poszczególne przejścia obejmują tylko jedną lokalizację. Niech \\(c \\colon V \\times \\{0,1\\}^V \\to \\mathbb{R}_+\\) będzie ograniczoną funkcją taką, że dla każdego \\(x \\in V\\), \\(c(x, \\cdot) \\colon \\{0,1\\}^V \\to \\mathbb{R}_+\\) jest ciągła. \\(c(x,\\eta)\\) interpretować będziemy jako intensywność, z jaką stan \\(\\eta\\) zmienia się poprzez zmianę wartości w \\(x\\). Dla \\(x \\in V\\) oraz \\(\\eta \\in \\{0,1\\}^V\\) definiujemy \\(\\eta^{(x)} \\in \\{0,1\\}^V\\) wzorem \\[\\eta^{(x)}(y) = \\left\\{ \\begin{array}{cc} \\eta(y), &amp; y \\neq x \\\\ 1-\\eta(x), &amp; y=x \\end{array}\\right..\\] Dla \\(f\\) pochodzącego z odpowiedniego podzbioru \\(C_0(\\{0,1\\}^V)\\) chcemy położyć \\[\\begin{equation} Lf(\\eta) = \\sum_{x\\in V} c(x, \\eta)\\left[f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right]. \\tag{24} \\end{equation}\\] Okazuje się, że dokładne napisanie dziedziny jest problematyczne. Aby obejść tę trudność rozważmy \\[\\begin{equation} D = \\left\\{ f \\in C(\\{0,1\\}^V) : \\|f\\|_o := \\sup_{\\eta} \\sum_x \\left|f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right| &lt; \\infty \\right\\}. \\tag{25} \\end{equation}\\] Naszym celem jest pokazanie, że zdefiniowanie \\(L\\) na \\(D\\) wystarcza do zdefiniowania procesu. Dygresja analityczna Generator infinitezymalny nie należy do najprostszych obiektów w teorii procesów Fellera. Jednym z powodów jest konieczność uwzględnienia dzieciny która, jak się już przekonaliśmy, ma istotny wpływ na kształt generowanego procesu. Rzadko się jednak zdarza, że dziedzinę można opisać jawnie. Dlatego często definiuje się proponowany generator na wygodnej podprzestrzeni dziedziny, a następnie bierze się domknięcie. Definicja 11 Operator liniowy \\((L, \\mathcal{D}(L))\\) na \\(C_0(S)\\) nazywany jest domkniętym, jeśli jego wykres \\[\\Gamma(L)=\\{(f, Lf) : f \\in D(L)\\}\\] jest domkniętym podzbiorem \\(C_0(S) \\times C_0(S)\\). Operator \\(L\\) jest domknięty wtedy i tylko wtedy, gdy \\(f_n \\in \\mathcal{D}(L)\\) są takie, że \\(f_n \\to f\\) oraz \\(Lf_n \\to h\\), to \\(f \\in \\mathcal{D}(L)\\) oraz \\(h=Lf\\). Definicja 12 Operator liniowy \\((L, \\mathcal{D}(L))\\) nazywany jest domykalnym, jeżeli domknięcie jego wykresu \\(\\Gamma(L)\\) jest wykresem operatora liniowego. W takiej sytuacji definiujemy domknięcie \\(\\overline{L}\\) operatora \\(L\\) poprzez \\[\\Gamma \\left( \\overline{L} \\right) = \\overline{\\Gamma(L)}.\\] Operator \\(L\\) jest domykalny wtedy i tylko wtedy, gdy \\(f_n \\to 0\\) oraz \\(Lf_n \\to h\\) implikują \\(h=0\\). Nie każdy operator liniowy ma domknięcie. Na przykład, przypuśćmy, że \\(S = [0, 1]\\) i \\[\\mathcal{D}(L) = \\{ f \\in C(S) : f&#39;(0) \\text{ istnieje} \\} \\quad \\text{i} \\quad Lf(x) = f&#39;(0)x \\text{ dla } f \\in \\mathcal{D}(L).\\] Wtedy domknięcie wykresu \\(L\\) nie jest wykresem operatora liniowego. Jednakże w kontekście Definicji 8 taka sytuacja nie występuje. Fakt 1 Niech \\((L, \\mathcal{D}(L))\\) będzie operatorem liniowym na \\(C_0(S)\\). Przypuśćmy, że \\(L\\) spełnia (GI1)-(GI3) z Definicji 8. Wtedy \\(L\\) jest domykalny, a jego domknięcie spełnia (GI1)-(GI3). Jeśli \\(L\\) spełnia (GI1)- (GI4) z Definicji 8, wtedy \\(L\\) jest domknięty. Jeśli \\(L\\) spełnia (GI3) i (GI4), to \\[\\mathcal{R}(I - \\lambda L) = C_0(S) \\quad \\text{dla każdego } \\lambda &gt; 0.\\] Jeśli \\(L\\) jest domknięty i spełnia (GI3), to \\(\\mathcal{R}(I - \\lambda L)\\) jest domkniętym podzbiorem \\(C_0(S)\\). Proof. Dla pierwszego stwierdzenia, musimy udowodnić, że \\(f_n \\in \\mathcal{D}(L), f_n \\to 0\\), oraz \\(Lf_n \\to h\\) implikuje \\(h = 0\\). Aby to zrobić, wybierzmy \\(g \\in \\mathcal{D}(L)\\). Korzystając z (13), \\[\\|(I - \\lambda L)(f_n + \\lambda g)\\| \\geq \\|f_n + \\lambda g\\|, \\quad \\lambda &gt; 0.\\] Biorąc \\(n \\to \\infty\\) i następnie dzieląc przez \\(\\lambda\\), dostajemy \\[\\|g - h - \\lambda Lg\\| \\geq \\|g\\|.\\] Jeżeli teraz wybierzemy \\(\\lambda \\to 0\\), to otrzymamy \\[\\| g-h \\| \\geq \\|g\\|.\\] Skoro \\(g\\in \\mathcal{D}(L)\\) jest wzięte z gęstego zbioru otrzymujemy \\(h = 0\\). Domknięcie \\(\\overline{L}\\) spełnia własności (GI1) i (GI2), ponieważ jest rozszerzeniem \\(L\\). Aby sprawdzić, czy spełnia własność (GI3), przypuśćmy, że \\(f \\in \\mathcal{D}(\\overline{L}), \\lambda \\geq 0\\) i \\(f - \\lambda \\overline{L}f = g\\). Przez definicję domknięcia, istnieją \\(f_n \\in \\mathcal{D}(L)\\), takie że \\(f_n \\to f\\) i \\(Lf_n \\to \\overline{L}f\\). Przez własność (GI3) dla \\(L\\), \\[\\inf_{x \\in S} f_n(x) \\geq \\inf_{x \\in S} g_n(x),\\] gdzie \\(g_n = f_n - \\lambda Lf_n\\). Teraz niech \\(n \\to \\infty\\). Dostajemy \\[\\inf_{x \\in S} f(x) \\geq \\inf_{x \\in S} g(x),\\] czyli własność \\((GI3)\\) dla \\(\\overline{L}\\). Dla dowodu drugiej części faktu, niech \\(\\overline{L}\\) będzie domknięciem \\(L\\). Jeśli \\(f \\in \\mathcal{D}(\\overline{L})\\) i \\(\\lambda &gt; 0\\) jest małe, przez własność (GI4) istnieje \\(h \\in \\mathcal{D}(L)\\) takie że \\[\\begin{equation} h - \\lambda Lh = f - \\lambda \\overline{L}f,\\tag{26}\\end{equation}\\] czyli \\((h - f) - \\lambda \\overline{L}(h - f) = 0\\). Z (13), \\(h = f\\), a wtedy \\(\\overline{L}f = Lh\\) z (26). Więc, \\(\\overline{L} = L\\) zgodnie z tezą. Przechodząc do trzeciego stwierdzenia, wystarczy sprawdzić, że dla \\(0 &lt; \\lambda &lt; \\gamma\\) warunek \\(\\mathcal{R}(I - \\lambda L) = C_0(S)\\) implikuje \\(\\mathcal{R}(I - \\gamma L) = C_0(S)\\). Załóżmy, że \\(g \\in C_0(S)\\), i zdefiniujmy \\(\\Gamma : C(S) \\to \\mathcal{D}(L)\\) przez \\[\\gamma \\Gamma h = \\lambda (I - \\lambda L)^{-1} g + (\\gamma - \\lambda)(I - \\lambda L)^{-1} h.\\] Definicja ta jest poprawna, ponieważ założyliśmy \\(\\mathcal{R}(I - \\lambda L) = C_0(S)\\). Mamy \\[\\gamma \\| \\Gamma h_1 - \\Gamma h_2 \\| = (\\gamma - \\lambda) \\| (I - \\mathcal{L})^{-1} (h_1 - h_2) \\| \\leq (\\gamma - \\lambda) \\| h_1 - h_2 \\|.\\] Stąd \\(\\Gamma\\) jest odwzorowaniem zwężającym, a więc z Twierdzenia Banacha o punkcie stałym posiada jedyny punkt stały \\(f\\). Wówczas \\(f \\in \\mathcal{D}(L)\\) oraz \\[\\gamma (I - \\lambda \\mathcal{L}) f = \\lambda g + (\\gamma - \\lambda) f.\\] Co można przekształcić do postaci \\(f - \\gamma L f = g\\). Czyli \\(g \\in \\mathcal{R}(I - \\gamma L)\\), co należało uzasadnić. Aby udowodnić ostatnie stwierdzenie, załóżmy, że \\(g_n \\in \\mathcal{R}(I - \\lambda L)\\) i \\(g_n \\to g\\). Wtedy możemy zdefiniować \\(f_n \\in \\mathcal{D}(L)\\) przez \\[\\label{eq:3.26} f_n - \\lambda L f_n = g_n.\\] Wówczas \\[(f_n - f_m) - \\lambda L (f_n - f_m) = g_n - g_m,\\] a zatem \\(\\| f_n - f_m \\| \\leq \\| g_n - g_m \\|\\). Ponieważ \\(\\{g_n\\}_{n \\in \\mathbb{N}}\\) jest ciągiem Cauchy’ego, to \\(\\{f_n\\}_{n \\in \\mathbb{N}}\\) również. Niech \\(f = \\lim_{n \\to \\infty} f_n\\). Ponieważ \\(f_n \\to f\\) i \\(g_n \\to g\\), to z (??) wynika, że \\(\\lim_{n \\to \\infty} L f_n\\) również istnieje. Ponieważ \\(L\\) jest domknięte, granicą jest \\(L f\\), a więc \\(f - \\lambda L f = g\\), co oznacza, że \\(g \\in \\mathcal{R}(I - \\lambda L)\\), czego należało dowieść. Konstrukcja systemów spinowych Naszym pierwszym celem jest znalezienie naturalnych warunków na \\(c(x, \\eta)\\), które gwarantują, że domknięcie \\((L,D)\\), gdzie \\(L\\) i \\(D\\) są dane odpowiednio przez (24) oraz (25) jest operatorem infinitezymalnym. Warunki (GI1), (GI2), (GI3) i (GI5) są łatwe do sprawdzenia i nie wymagają dodatkowych założeń. Prawdziwym wyzwaniem jest (GI4). Dla warunku (GI2), używamy twierdzenia Stone’a-Weierstrassa: \\(D\\) jest algebrą funkcji ciągłych na zbiorze zwartym, która rozdziela punkty. Istotnie, dla \\(\\eta \\neq \\zeta\\) istnieje \\(x \\in S\\) takie, że \\(\\eta(x) \\neq \\zeta(x)\\). Funkcja \\(f(\\xi) = \\xi(x)\\) rozdziela \\(\\eta\\) i \\(\\zeta\\). Algebra \\(D\\) zawiera funkcje stałe, więc \\(\\overline{D} = C(\\{0,1\\}^V)\\). Dla (GI3), załóżmy \\(f \\in D\\), \\(\\lambda \\geq 0\\), oraz \\(f - \\lambda Lf = g\\). Ponieważ \\(\\{0,1\\}^V\\) jest zbiorem zwartym i \\(f\\) jest ciągła, istnieje takie \\(\\eta\\), dla którego \\(f\\) osiąga swoje minimum. Wtedy \\(Lf(\\eta) \\geq 0\\), więc \\[\\min_\\zeta f(\\zeta) = f(\\eta) \\geq g(\\eta) \\geq \\min_\\zeta g(\\zeta).\\] Warunek (GI5) wynika z faktu, że \\(1 \\in D\\) oraz \\(L1 = 0\\). Aby sprawdzić (GI4) musimy wyprowadzić ograniczenie dla rozwiązań równania \\(f - \\lambda Lf = g\\). Niech \\[\\epsilon = \\inf_{u, \\eta} [c(u, \\eta) + c(u, \\eta_u)] \\quad \\text{oraz} \\quad \\gamma(x, u) = \\sup_\\eta |c(x, \\eta_u) - c(x, \\eta)|.\\] Zauważmy, że \\(\\gamma(x,u)\\) mierzy stopień, w jakim intensywność zmiany w miejscu \\(x\\) zależy od konfiguracji w miejscu \\(u\\). Niech \\(\\ell_1(V)\\) będzie przestrzenią Banacha funkcji \\(\\alpha : V \\to \\mathbb{R}\\), które spełniają \\[||\\alpha|| := \\sum_x |\\alpha(x)| &lt; \\infty.\\] Macierz \\(\\gamma\\) definiuje operator \\(\\Gamma\\) na \\(\\ell_1(S)\\) przez \\[\\Gamma \\alpha(u) = \\sum_{x: x \\neq u} \\alpha(x) \\gamma(x, u).\\] Operator ten jest dobrze zdefiniowany i ograniczony, pod warunkiem że \\[M := \\sup_x \\sum_{u: u \\neq x} \\gamma(x, u) &lt; \\infty,\\] a wtedy \\(||\\Gamma|| = M\\). Dla \\(f \\in C(\\{0,1\\}^V)\\) i \\(x \\in S\\), niech \\[\\Delta f(x) = \\sup_{\\eta} \\left|f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right|.\\] Wtedy \\(\\|f\\|_o = ||\\Delta f||_{l_1(V)}\\). Oto oszacowanie, którego potrzebujemy. (#prp:4.2) Załóżmy, że spełniony jest jeden z warunków \\(f \\in D\\), \\(f\\) jest ciągła i \\[\\begin{equation} c(x, \\cdot) \\equiv 0 \\text{ dla wszystkich oprócz skończonej liczby } x \\in V.(\\#eq:4.3) \\end{equation}\\] Wówczas jeśli \\(f - \\lambda Lf = g \\in D\\), \\(\\lambda &gt; 0\\), oraz \\(\\lambda M &lt; 1 + \\lambda \\epsilon\\), to \\[ \\Delta f \\leq \\left[ (1 + \\lambda \\epsilon)I - \\lambda \\Gamma \\right]^{-1} \\Delta g,(\\#eq:4.4)\\] gdzie nierówność zachodzi współrzędna po współrzędnej, a odwrotność jest zdefiniowana przez nieskończony szereg \\[\\begin{equation} \\left[ (1 + \\lambda \\epsilon)I - \\lambda \\Gamma \\right]^{-1} \\alpha = \\frac{1}{1 + \\lambda \\epsilon} \\sum_{k=0}^{\\infty} \\left( \\frac{\\lambda}{1 + \\lambda \\epsilon} \\right)^k \\Gamma^k \\alpha.(\\#eq:4.5)\\end{equation}\\] Proof. Zauważmy, że szereg w @ref(eq:4.5) jest zbieżny dla \\(\\alpha \\in \\ell_1(V)\\) na mocy założenia \\(\\lambda M &lt; 1 + \\lambda \\epsilon\\). Pisząc \\(f - \\lambda Lf = g\\) w punktach \\(\\eta\\) oraz \\(\\eta^{(u)}\\), odejmując i zauważając że \\((\\eta^{(u)})^{(u)} = \\eta\\), otrzymujemy \\[\\begin{multline} [f(\\eta^{(u)}) - f(\\eta)][1 + \\lambda c(u, \\eta) + \\lambda c(u, \\eta^{(u)})] = [g(\\eta^{(u)}) - g(\\eta)]\\\\ + \\lambda \\sum_{x:x \\neq u} \\left\\{ c(x, \\eta^{(u)}) [f((\\eta^{(u)})^{(x)}) - f(\\eta^{(u)})] - c(x, \\eta)[f(\\eta^{(x)}) - f(\\eta)] \\right\\}. (\\#eq:4.6) \\end{multline}\\] Ponieważ wartości \\(f(\\eta^{(u)}) - f(\\eta)\\), gdy \\(\\eta\\) zmienia się a \\(u\\) jest ustalone, tworzą zbiór symetryczny, a ta różnica jest funkcją ciągłą \\(\\eta\\), dla każdego \\(u\\) istnieje takie \\(\\eta\\), że \\[f(\\eta^{(u)}) - f(\\eta) = \\sup_{\\zeta} |f(\\zeta^{(u)}) - f(\\zeta)| = \\Delta f(u).\\] Stąd, \\[f(\\zeta^{(u)}) - f(\\zeta) \\leq f(\\eta^{(u)}) - f(\\eta)\\] dla każdej \\(\\zeta\\). Stosując to dla \\(\\zeta = \\eta^{(x)}\\) i przekształcając, otrzymujemy \\[f((\\eta^{(u)})^{(x)}) - f(\\eta^{(u)}) = f((\\eta^{(x)})^{(u)}) - f(\\eta^{(u)}) \\leq f(\\eta^{(x)}) - f(\\eta),\\] Używając tej nierówności w @ref(eq:4.6), \\[\\begin{multline} \\Delta f(u)(1 + \\lambda \\epsilon) \\leq \\Delta f(u)[1 + \\lambda c(u, \\eta) + \\lambda c(u, \\eta^{(u)})] \\\\ \\leq \\Delta g(u) + \\lambda \\sum_{x:x \\neq u} \\left[ c(x, \\eta^{(u)}) - c(x, \\eta) \\right] [f(\\eta^{(x)}) - f(\\eta)] \\\\ \\leq \\Delta g(u) + \\lambda \\sum_{x:x \\neq u} \\gamma(x,u) \\Delta f(x). (\\#eq:4.7) \\end{multline}\\] Jeśli @ref(eq:4.3) zachodzi, to tylko skończona liczba wyrazów po prawej stronie jest niezerowa, więc \\(\\|\\Delta_f\\|_1 = \\|f\\|_o &lt;\\infty\\). Zastem przy któregokolwiek z założeń faktu, \\(f \\in D\\). Dlatego @ref(eq:4.7) można zapisać jako \\[(1 + \\lambda \\epsilon) \\Delta f \\leq \\Delta g + \\lambda \\Gamma \\Delta f.\\] Iteracja tej nierówności prowadzi to do \\[\\Delta f \\leq \\frac{1}{1 + \\lambda \\epsilon} \\sum_{k=0}^{n} \\left( \\frac{\\lambda}{1 + \\lambda \\epsilon} \\right)^k \\Gamma^k \\Delta g + \\left( \\frac{\\lambda}{1 + \\lambda \\epsilon} \\right)^{n+1} \\Gamma^{n+1} \\Delta f.\\] Jeżeli rozważymy teraz \\(n \\to \\infty\\), dostaniemy @ref(eq:4.4). (#thm:4.3) Załóżmy, że \\(M &lt; \\infty\\). Wtedy \\(\\overline{L}\\) jest generatorem infinitezymalnym półgrupy Fellera \\(T=(T(t))_{t \\in \\mathbb{R}_+}\\). Ponadto, \\[\\begin{equation} \\Delta T(t)f \\leq e^{-t \\epsilon} e^{t \\Gamma} \\Delta f. (\\#eq:4.8) \\end{equation}\\] W szczególności, jeśli \\(f \\in D\\), to \\(T_tf \\in D\\) oraz \\[\\begin{equation} \\|T(t)f\\|_o \\leq e^{(M - \\epsilon)t} \\|f\\|. (\\#eq:4.9) \\end{equation}\\] Proof. Własności (GI1), (GI2), (GI3) i (GI5) z Definicji ?? zachodzą dla \\((L, D)\\) są i są dziedziczone przez \\(\\overline{L}\\) z Faktu 1. Aby sprawdzić warunek (GI4) weźmy wstępujący ciąg \\(V_n\\subseteq V\\) taki, że \\(\\bigcup_nV_n=V\\). Niech \\[\\begin{equation} L_n f(\\eta) = \\sum_{x \\in V_n} c(x, \\eta) [f(\\eta_x) - f(\\eta)], \\quad f \\in C(\\{0,1\\}^V). (\\#eq:4.10) \\end{equation}\\] To jest generator dla systemu spinowego, w którym współrzędne \\[(\\eta_t(x) : x \\notin V_n)\\] są stałe w czasie. Ponieważ \\(L_n\\) jest ograniczonym generatorem, spełnia \\[\\mathcal{R}(I - \\lambda L_n) = C(\\{0,1\\}^V)\\] dla wszystkich \\(\\lambda &gt; 0\\). Dla \\(g \\in D\\), możemy zdefiniować \\(f_n \\in C(\\{0,1\\}^V)\\) przez \\(f_n - \\lambda L_n f_n = g\\). Ponieważ \\(L_n\\) spełnia @ref(eq:4.3), jeśli \\(\\lambda\\) jest wystarczająco małe, tak że \\(\\lambda M &lt; 1 + \\lambda \\epsilon\\), wtedy \\(f_n \\in D\\) zgodnie z Faktem @ref(prp:4.2). W związku z tym możemy położyć \\[g_n = f_n - \\lambda L f_n \\in \\mathcal{R}(I - \\lambda L).\\] Niech \\(K = \\sup_{x, \\eta} c(x, \\eta) &lt;\\infty\\), wtedy z Faktu @ref(prp:4.2) \\[\\begin{multline} \\|g_n - g\\| = \\lambda ||(L - L_n) f_n|| \\leq \\lambda K \\sum_{x \\notin V_n} \\Delta f_n(x)\\\\ \\leq \\lambda K \\sum_{x \\notin V_n} \\left[ (1 + \\lambda \\epsilon)I - \\lambda \\Gamma \\right]^{-1} \\Delta g(x). (\\#eq:4.11) \\end{multline}\\] Ponieważ \\(\\Delta g \\in \\ell_1(V)\\), prawa strona @ref(eq:4.11) dąży do zera, gdy \\(n \\to \\infty\\), więc \\(g_n \\to g\\). Stąd \\(g \\in \\mathcal{R}(I - \\lambda L)\\), więc wnioskujemy, że \\(D \\subseteq \\mathcal{R}(I - \\lambda L)\\). Ponieważ \\(D\\) jest gęste w \\(C(\\{0,1\\}^V)\\), widzimy, że \\(\\mathcal{R}(I - \\lambda L)\\) jest również gęste. Zatem \\[\\mathcal{R}(I - \\lambda \\overline{L}) = C(\\{0,1\\}^V)\\] zgodnie z Faktem 1. To kończy weryfikację, że \\(\\overline{L}\\) jest generatorem infinitezymalnym. Przechodząc do drugiego stwierdzenia, zapiszmy @ref(eq:4.4) jako \\[\\Delta_{(I - \\lambda L)^{-1}} g \\leq \\left[ (1 + \\lambda \\epsilon)I - \\lambda \\Gamma \\right]^{-1} \\Delta g,\\] a następnie iterujmy, aby uzyskać \\[\\Delta_{(I - \\frac{t}{n} L)^{-1}} g \\leq \\left[ \\left( 1 + \\frac{t}{n} \\epsilon \\right) I - \\frac{t}{n} \\Gamma \\right]^{-n} \\Delta g.\\] Przechodząc do granicy otrzymujemy @ref(eq:4.8). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
