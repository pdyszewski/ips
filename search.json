[{"path":"index.html","id":"section-stochastyczne-modele-układów-oddziałujących","chapter":"Stochastyczne modele układów oddziałujących","heading":"Stochastyczne modele układów oddziałujących","text":"Notatki te powstały na potrzebę kursu o tym samym tytule prowadzonym w Instytucie Matematycznym Uniwersytetu Wrocławskiego zimą 2024.\nZainspirowany Complex analysis autorstwa Juan Carlos Ponce Campuzano oraz\nCollision Detection Jeffreya Thompsona chciałem zbadać\nmożliwości tworzenia interaktywnych podręczników. Wierzę, że wchodzenie w interkację z czytelnikiem pozwala mu lepiej zrozumieć prezentowany materiał.\nJednym z najlepszych sposobów na zrozumienie nawet najbardziej zawiłych procesów jest zobaczenie ich w akcji.\nNotatki te dostępne są w formi pdf.Notatki te w dalszym ciągu są wstępnej fazie niestety nie są dolne od błedów.\nGorąco zachęcam zgłaszania wszystkich problemów, błedów niedociągnięć na github.napisania książki wykorzystałem bibliotekę bookdown z pakietu R.\nSymulacje zostały napisane z wykorzystaniem p5.js.Wrocław, grudzień 2024\nPiotr Dyszewski","code":""},{"path":"section-sylabus.html","id":"section-sylabus","chapter":"Sylabus","heading":"Sylabus","text":"","code":""},{"path":"section-sylabus.html","id":"section-dane-dotyczące-przedmiotu","chapter":"Sylabus","heading":"Dane dotyczące przedmiotu","text":"Nazwa przedmiotu: Stochastyczne modele systemów oddziałującychJednostka oferująca przedmiot: Instytut MatematycznyZałożenia: Teoria prawdopodobieństwa 2 (28-MT-S-tTPrawd2)Strona www: https://sites.google.com/site/piotrdyszewski/teaching/SMUOForma zajęć: wykład + ćwiczeniaPunkty ECTS: 6","code":""},{"path":"section-sylabus.html","id":"section-skrócony-plan-wykładu","chapter":"Sylabus","heading":"Skrócony plan wykładu","text":"W trakcie wykładu poruszymy następujące zagadnienia:) Procesy Markowa: łańcuchy Markowa w czasie ciągłym, procesy Fellera, półgrupy operatorów, generatory infinitezymalne, martyngały, rozkłady stacjonarne, ruch Browna procesy pokrewneB) Układy cząstek: konstrukcja, ergodyczność, model głosowania, model epidemii, exclusion processPodstawowa literatura wykładu:Liggett, Thomas Milton. Continuous time Markov processes: introduction. Vol. 113. American Mathematical Soc., 2010.Liggett, Thomas Milton, Thomas M. Liggett. Interacting particle systems. Vol. 2. New York: Springer, 1985.Liggett, Thomas M. Stochastic interacting systems: contact, voter exclusion processes. Vol. 324. Springer Science & Business Media, 2013.","code":""},{"path":"section-sylabus.html","id":"section-szczegółowy-plan-wykładu","chapter":"Sylabus","heading":"Szczegółowy plan wykładu","text":"Wstępny plan tematów poruszanych na poszczególnych wykładach:Łańcuchy Markowa w czasie ciągłymProcesy Fellera, półgrupy generatoryOd procesu półgrupy generatoraOd generatora procesuKonstrukcje generatorów, zagadnienie martyngałowe, rozkłady stacjonarneProcesy dualneZaburzenia ruchu BrownaKonstrukcja systemów oddziałującychErgodyczność systemów oddziałującychKilka narzędziModel głosowania: przypadek rekurencyjnyModel głosowania: przypadek tranzytywnyModel epidemii: reprezentacja graficzna addytywnośćModel epidemii na drzewie jednorodnymExclusion process","code":""},{"path":"section-sylabus.html","id":"section-efekty-kształcenia","chapter":"Sylabus","heading":"Efekty kształcenia","text":"Po wykładzie student:Formułuje podstawowe obiekty z zakresu teorii procesów Markowa ();Podaje przykłady stochastycznych systemów oddziałujących wraz z ich podstawowymi własnościami (B);Formułuje związki między generatorami, półgrupami oraz procesami Markowa ();Analizuje dowody prostych twierdzeń z wykładu z uzasadnieniem poszczególnych ich założeń ();Formułuje główne twierdzenia teorii procesów Markowa ();Analizuje dowody najważniejszych twierdzeń z wykładu z uzasadnieniem poszczególnych ich założeń (, B);Formułuje główne twierdzenia z zakresu teorii systemów oddziałujących (B);Stosuje teorię procesów Markowa w przykładach ().","code":""},{"path":"section-sylabus.html","id":"section-sposób-weryfikacji-efektów-kształcenia","chapter":"Sylabus","heading":"Sposób weryfikacji efektów kształcenia","text":"Na zaliczenie składać się będą:Aktywność na ćwiczeniach;Zadania domowe.","code":""},{"path":"section-sylabus.html","id":"section-metody-i-kryteria-oceniania","chapter":"Sylabus","heading":"Metody i kryteria oceniania","text":"Zaliczenie ćwiczeń na podstawie zadań domowych aktywności w czasie zajęć. Ocena z egzaminu wystawiona jest na podstawie egzaminu ustnego.","code":""},{"path":"section-sylabus.html","id":"section-warunkiem-zaliczenia-przedmiotu-jest","chapter":"Sylabus","heading":"Warunkiem zaliczenia przedmiotu jest:","text":"Uzyskanie 30% punktów za zadania stanowiące bieżącą weryfikację efektów kształcenia;Uzyskanie pozytywnej oceny z egzaminu stanowiącego końcową weryfikację efektów kształcenia.","code":""},{"path":"section-sylabus.html","id":"section-kryteria-ocen","chapter":"Sylabus","heading":"Kryteria ocen:","text":"(dst) student realizuje punkty 1-2 efektów kształcenia(db) student realizuje punkty 1-5 efektów kształcenia(bdb) student realizuje punkty 1-8 efektów kształceniaWrocław, wrzesień 2024\nPiotr Dyszewski","code":""},{"path":"section-wykład-1-łańcuchy-markowa-w-czasie-ciągłym.html","id":"section-wykład-1-łańcuchy-markowa-w-czasie-ciągłym","chapter":"Wykład 1: Łańcuchy Markowa w czasie ciągłym","heading":"Wykład 1: Łańcuchy Markowa w czasie ciągłym","text":"2024-10-03Piotr DyszewskiCelem tego rozdziału jest skonstruowanie procesów Markowa w czasie ciągłym na przeliczalnym (lub skończonym)\nzbiorze \\(S\\) w oparciu o jego opis infinitezymalny. W następnym rozdziale zbadamy problem konstrukcji dla procesów\nna bardziej ogólnej przestrzeni stanów.\nNa razie ograniczamy naszą uwagę bardziej konkretnej sytuacji przeliczalnego \\(S\\).\nW tym przypadku często używa się słowa “łańcuch” zamiast “proces”. clclPrzypomnijmy, że łańcuchem Markowa w czasie dyskretnym nazywamy proces stochastyczny\n\\(\\{X_n\\}_{n \\\\mathbb{N}}\\) takie, że dla dowolnego \\(n \\\\mathbb{N}\\) \ndowolnych \\(s_0, s_1, \\ldots, s_n \\S\\) takich, że\n\\[\n\\mathbb{P}\\left[ X_{n-1}=s_{n-1}, \\: X_{n-2}=s_{n-2}, \\ldots , X_0=s_0 \\right]>0\n\\]\nzachodzi\n\\[\n\\mathbb{P}\\left[ X_n=s_n \\: | \\: X_{n-1}=s_{n-1}, \\:  \\ldots , X_0=s_0 \\right]=\n\\mathbb{P}\\left[ X_n=s_n \\: | \\: X_{n-1}=s_{n-1} \\right].\n\\]\nPowyższa własność jest bardzo często przytaczana jako wyjściowa definicja łańcucha Markowa. Mimo swojej prostoty, która ułatwia czytelnikom pierwsze zetknięcie z własnością Markowa, własność ta ma jedną wadę, która ujawnia się przy bardziej zaawansowanych rozważaniach teoretycznych.Jeżeli chcemy badać tylko procesy na przeliczalnej przestrzeni stanów, jedno naturalne uogólnienie ma następującą formę. Proces stochastyczny w czasie ciągłym \\(\\{X(t)\\}_{t \\\\mathbb{R}_+}\\) nazwiemy łańcuchem Markowa w czasie ciągłym na przeliczalnej przestrzeni stanów \\(S\\), jeżeli dla dowolnego \\(n\\) dowolnych \\(0 \\leq t_0 < t_1 < \\ldots < t_n\\) dowolnych \\(s_0, s_1, \\ldots, s_n \\S\\) takich, że\n\\[\n\\mathbb{P}\\left[ X(t_{n-1})=s_{n-1}, \\: X(t_{n-2})=s_{n-2}, \\ldots , X(t_0)=s_0 \\right]>0\n\\]\nzachodzi\n\\[\\begin{multline*}\n\\mathbb{P}\\left[ X(t_n)=s_n \\: | \\: X(t_{n-1})=s_{n-1}, \\: X(t_{n-2})=s_{n-2}, \\ldots , X(t_0)=s_0 \\right]\\\\=\n\\mathbb{P}\\left[ X(t_n)=s_n \\: | \\: X(t_{n-1})=s_{n-1} \\right].\n\\end{multline*}\\]\nPowyższa własność nie jest zbyt przydatna, jeżeli chcemy badać procesy na nieprzeliczalnej przestrzeni stanów. Dla bardzo wielu naturalnych obiektów zmienne losowe \\(X(t)\\) w badanym przez nas procesie mogą mieć rozkład ciągły. Oznacza , że warunek powyższy nie jest spełniony dla dowolnego wyboru parametrów.W celu znalezienia bardziej elastycznego warunku zauważmy,\nże własność Markowa dla jednorodnego łańcucha Markowa\n\\(\\{X_n\\}_{n \\\\mathbb{N}}\\) w czasie dyskretnym z macierzą przejścia\n\\[\np(,j) = \\mathbb{P}[X_1=j \\: | \\: X_0=]\n\\]\nzapisuje się jako\n\\[\n\\mathbb{P}[X_n = j \\: | \\: \\mathcal{F}_n] = p(X_n,j),\n\\]\ngdzie \\(\\mathcal{F}_n = \\sigma(X_0, X_1, \\ldots, X_n)\\).\nDokładne uzasadnienie powyższej własności pozostawiamy jako zadanie.\nPodobnie, dla dowolnego \\(m \\\\mathbb{N}\\),\n\\[\n\\mathbb{P}[X_{n+m} = j \\: | \\: \\mathcal{F}_n] = p^{(m)}(X_n,j),\n\\]\ngdzie \\((p^{(m)(,j) })_{, j \\S}\\) jest \\(m\\)-tą potęgą macierzy przejścia\n\\[\np^{(m)}(,j) = \\mathbb{P}[X_m=j \\: | \\: X_0=].\n\\]\nOznacza , że dla dowolnej funkcji mierzalnej \\(f \\colon S \\\\mathbb{R}\\),\n\\[\n\\mathbb{E}[ f(X_{n+m}) \\: | \\: \\mathcal{F}_n] = \\sum_{s \\S} f(s) p^{(m)}(X_n, s).\n\\]\nPowyższa definicja względnie łatwo zapisuje się w czasie ciągłym\n\\[\n\\mathbb{E}[ f(X_{s+t}) \\: | \\: \\mathcal{F}_s] = \\sum_{x\\S} f(x) p^{(t)}(X_n, x),\n\\]\ngdzie\n\\[\np^{(t)}(y,x) = \\mathbb{P}[X_t=x | X_0=y].\n\\]\nRelacja powyższa daje się zapisać w przypadku nieprzeliczalnej przestrzeni stanów jako\n\\[\\begin{equation*}\n\\mathbb{E}[ f(X_{s+t}) \\: | \\: \\mathcal{F}_s] = \\int_S f(x) p^{(t)}(X_n, \\mathrm{d} x),\n\\end{equation*}\\]\ngdzie\n\\[\\begin{equation*}\np^{(t)}(y, \\mathrm{d} x) = \\mathbb{P}[X_t \\\\mathrm{d} x | X_0=y]\n\\end{equation*}\\]\njest rozkładem \\(X_t\\) pod warunkiem \\(\\{X_0=y\\}\\). niesie ze sobą kolejne problemy, ponieważ jak wcześniej zauważyliśmy \\(\\{X_0=y\\}\\) może być zdarzeniem o prawdopodobieństwie zero. Przedstawione podejście jest uratowania pod kątem formalnym przez odniesienie się regularnych rozkładów warunkowych.Zamiast tego podejdziemy problemu od innej strony. Naszym punktem wyjścia będzie odpowiednia rodzina miar. Jak zobaczymy wkrótce, podejście będzie również opierało się o odpowiednik powyższej relacji. Oznacza , że w rezultacie będziemy opisywali tę samą klasę procesów stochastycznych bez konieczności obchodzenia się z warunkowaniem po zdarzeniach niemożliwych.","code":""},{"path":"section-wykład-1-łańcuchy-markowa-w-czasie-ciągłym.html","id":"section-podstawowe-definicje","chapter":"Wykład 1: Łańcuchy Markowa w czasie ciągłym","heading":"Podstawowe definicje","text":"Zaczynamy od prezentacji definicji trzech obiektów, na których się skupimy w tym rozdziale. Przypomnijmy, że będziemy definiować łańcuchy Markowa w czasie ciągłym na dyskretnej przestrzeni stanów \\(S\\). Topologia na \\(S\\) oczywiście topologia dyskretna, względem której wszystkie funkcje są ciągłe.Głównym obiektem naszych badań będą procesy stochastyczne. Ze względów technicznych pracować będziemy na bardzo konkretnej przestrzeni zdarzeń elementarnych. Niech \\(\\Omega\\) będzie zbiorem prawostronnie ciągłych funkcji \\(\\omega \\colon [0, \\infty) \\S\\) ze skończoną liczbą skoków w dowolnym skończonym przedziale czasowym.Dla każdego \\(t \\\\mathbb{R}_+ = [0, +\\infty)\\) rozważmy funkcję \\(X_t \\colon \\Omega \\S\\) zadaną przez\n\\[\nX_t(\\omega) = \\omega(t).\n\\]\nNiech \\(\\sigma\\)-ciało \\(\\mathcal{F}\\) na \\(\\Omega\\) będzie najmniejszym takim, że odwzorowanie \\(\\omega \\mapsto \\omega(t)\\) jest mierzalne dla każdego \\(t \\\\mathbb{R}_+ = [0, +\\infty)\\). Niech wreszcie dla \\(s \\\\mathbb{R}_+\\) oznaczmy przez \\(\\theta_s\\) odwzorowanie \\(\\Omega \\\\Omega\\) zadane przez\n\\[\n\\theta_s(\\omega)(t) = \\omega(t+s).\n\\]\nW szczególności \\(X_t \\circ \\theta_s = X_{t+s}\\). O odwzorowaniu \\(\\theta_s\\) można myśleć jak o przesunięciu czasu o \\(s\\). Zamiast utożsamiać własność Markowa z procesem stochastycznym \\(X=\\{X(t)\\}_{t \\\\mathbb{R}_+}\\), utożsamimy ją z rodziną miar probabilistycznych na \\(\\Omega\\), względem której \\(X\\) będzie procesem Markowa.Definicja 1  Łańcuchem Markowa w czasie ciągłym na przestrzeni stanów \\(S\\) nazywamy parę uporządkowaną\n\\(( \\mathbf{P}, \\mathbb{F})\\) taką, że(ŁM1) \\(\\mathbb{F}=(\\mathcal{F}_t)_{t \\\\mathbb{R}_+}\\) jest filtracją względem której\n\\(X = (X(t))_{t \\\\mathbb{R}_+}\\) jest adaptowalny \n\\(\\mathcal{F}_t \\subseteq \\mathcal{F}\\) dla każdego \\(t \\\\mathbb{R}_+\\).(ŁM1) \\(\\mathbb{F}=(\\mathcal{F}_t)_{t \\\\mathbb{R}_+}\\) jest filtracją względem której\n\\(X = (X(t))_{t \\\\mathbb{R}_+}\\) jest adaptowalny \n\\(\\mathcal{F}_t \\subseteq \\mathcal{F}\\) dla każdego \\(t \\\\mathbb{R}_+\\).(ŁM2) \\(\\mathbf{P}= \\{ \\mathbf{P}_x\\}_{x \\S}\\). Dla każdego \\(x \\S\\), \\(\\mathbf{P}_x\\) jest miarą probabilistyczną na \\(\\Omega\\) taką, że\n\\[\n\\mathbf{P}_x[X_0 = x] = \\mathbf{P}_x[\\omega \\\\Omega : \\omega(0)=x]= 1.\n\\](ŁM2) \\(\\mathbf{P}= \\{ \\mathbf{P}_x\\}_{x \\S}\\). Dla każdego \\(x \\S\\), \\(\\mathbf{P}_x\\) jest miarą probabilistyczną na \\(\\Omega\\) taką, że\n\\[\n\\mathbf{P}_x[X_0 = x] = \\mathbf{P}_x[\\omega \\\\Omega : \\omega(0)=x]= 1.\n\\](ŁM3) Spełniona jest własność Markowa\n\\[\n\\mathbf{E}_x[Y \\circ \\theta_s | \\mathcal{F}_s] = \\mathbf{E}_{X(s)}[Y] \\text{ p.n. } \\mathbf{P}_x\n\\]\ndla wszystkich \\(x \\S\\) wszystkich ograniczonych mierzalnych \\(Y\\) na \\(\\Omega\\).(ŁM3) Spełniona jest własność Markowa\n\\[\n\\mathbf{E}_x[Y \\circ \\theta_s | \\mathcal{F}_s] = \\mathbf{E}_{X(s)}[Y] \\text{ p.n. } \\mathbf{P}_x\n\\]\ndla wszystkich \\(x \\S\\) wszystkich ograniczonych mierzalnych \\(Y\\) na \\(\\Omega\\).W powyższej definicji \\(\\mathbf{E}_x\\) jest wartością oczekiwaną odpowiadającą\nmierze probabilistycznej \\(\\mathbf{P}_x\\), czyli\n\\[\n\\mathbf{E}_x[Y] = \\int_\\Omega Y(\\omega) \\: \\mathbf{P}_x(\\mathrm{d} \\omega).\n\\]\nO zmiennej losowej \\(Y \\colon \\Omega \\\\mathbb{R}\\)\nmożna myśleć jak o statystyce całej trajektorii procesu\n\\(\\{X(t)\\}_{t \\\\mathbb{R}_+}\\). Załóżmy, że \\(Y\\) jest postaci\n\\[\nY(\\omega) = f(\\omega(t_1), \\omega(t_2), \\ldots , \\omega(t_n)) = f(X(t_1), X(t_2), \\ldots, X(t_n))\n\\]\ndla pewnej mierzalnej ograniczonej funkcji \\(f \\colon \\mathbb{R}^n \\\\mathbb{R}\\)\n(dla ćwiczenia warto sprawdzić, że \\(Y\\) powyższej postaci jest istotnie zmienną losową, tj. jest mierzalna względem \\(\\mathcal{F}\\)).\nWówczas\n\\[\\begin{multline*}\nY\\circ \\theta_s(\\omega) = f(\\omega(t_1+s), \\omega(t_2+s), \\ldots , \\omega(t_n+s)) \\\\= f(X(t_1+s), X(t_2+s), \\ldots, X(t_n+s)).\n\\end{multline*}\\]\nWłasność Markowa w Definicji mówi zatem , co powinna. Rozkład wektora losowego \\((X(t_1+s), X(t_2+s), \\ldots, X(t_n+s))\\) pod warunkiem \\(\\mathcal{F}_s\\) jest taki sam, jak rozkład wektora \\((X'(t_1), X'(t_2), \\ldots, X'(t_n))\\) dla \\(X'=\\{X'(t)\\}_{t\\\\mathbb{R}_+}\\) będącym niezależną kopią \\(X\\), zapoczątkowaną w \\(X_0'=X(s)\\).Naszym nadrzędnym celem będzie sprowadzenie powyższej definicji bardziej przystępnych terminów. Zanim jednak tego przejdziemy, rozważmy następujący przykład.Przykład 1  Niech \\(N=(N_t)_{t \\\\mathbb{R}_+}\\) będzie jednorodnym\nprocesem Poissona z intensywnością \\(\\lambda >0\\) określonym na przestrzeni\nprobabilistycznej \\((\\Sigma, \\mathcal{G}, \\mathbb{P})\\).\nPrzypomnijmy, że oznacza , że \\(t \\mapsto N_t\\) jest prawostronnie ciągła oraz\ndla każdego \\(t>0\\) zmienna losowa \\(N_t\\) ma rozkład Poissona z parametrem \\(\\lambda>0\\),\n\\[\n\\mathbb{P}[N_t=k] = e^{-\\lambda} \\frac{\\lambda^k}{k!},\n\\]\nwreszcie dla \\(t>s\\geq 0\\), zmienna \\(N_t - N_s\\) jest niezależna\nod \\(\\sigma\\)-ciała \\(\\mathcal{F}_s^N = \\sigma(N_r : r \\leq s)\\).Uzasadnimy, że \\(N\\) jest procesem Markowa w sensie przyjętej przez nas definicji.\nPrzestrzenią stanów jest \\(S = \\mathbb{N}\\).\nNiech \\(\\mathbf{P}_x[] = \\mathbb{P}[N + x \\]\\) dla \\(\\\\mathcal{F}\\).\nTutaj \\(x+N\\) oznacza funkcję \\(t \\mapsto N_t + x\\).\nPokażemy teraz, że spełniona jest własność Markowa.Dla \\(\\\\mathcal{F}_s\\) mamy\n\\[\n\\mathbf{E}_x[Y \\circ \\theta_s \\cdot \\mathbf{1}_{}]\n= \\mathbb{E}[Y(N \\circ \\theta_s + x) \\mathbf{1}_{\\{N \\\\}}]\n= \\mathbb{E}[Y(N \\circ \\theta_s - N_s + N_s + x) \\mathbf{1}_{\\{N \\\\}}].\n\\]\nSkoro \\(\\\\mathcal{F}_s\\), \\(\\{N \\\\} \\\\mathcal{F}_s^N\\) (zadanie).\nSkoro \\(N \\circ \\theta_s - N_s = (N_{t+s} - N_s)_{t \\\\mathbb{R}_+}\\)\njest niezależny od \\(\\mathcal{F}_s^N\\), \n\\[\n\\mathbb{E}[Y(N \\circ \\theta_s - N_s + N_s + x) \\mathbf{1}_{\\{N \\\\}} | \\mathcal{F}_s] = \\mathbf{1}_{\\{N \\\\}} \\cdot y(N_s + x),\n\\]\ngdzie\n\\[\ny(k) = \\mathbb{E}[Y(N \\circ \\theta_s - N_s + k)] = \\mathbf{E}_k[Y].\n\\]\nPodsumowując,\n\\[\n\\mathbf{E}_x[Y \\circ \\theta_s \\cdot \\mathbf{1}_{}]\n= \\mathbb{E}[y(N_s + x) \\mathbf{1}_{\\{N \\\\}}]\n= \\mathbf{E}_x[y(X(s)) \\mathbf{1}_A]\n\\]\nodwołując się teraz definicji warunkowej wartości oczekiwanej\n\\[\n\\mathbf{E}_x[Y \\circ \\theta_s | \\mathcal{F}_s]\n= y(X(s))\n= \\mathbf{E}_{X(s)}[Y].\n\\]Powyższy przykład pokazuje, że uzasadnienie własności Markowa wprost z definicji nie jest najprostszym zadaniem.\nPrzekonamy się w przyszłości, że taka forma jest przydatna teoretycznych rozważań.\nMimo przyda się nam bardziej przystępny sposób mówienia o łańcuchach Markowa w czasie ciągłym.\nOdnosząc się czasu dyskretnego, korzystać będziemy z funkcji przejścia.Definicja 2  Funkcją przejścia nazywamy rodzinę odwzorowań \\(p = (p_t)_{t \\\\mathbb{R}_+}\\),\ngdzie \\(p_t \\colon S \\times S \\[0,1]\\) zdefiniowanych dla \\(t \\ge 0\\) takich, że\n\\[\np_t(x, y) \\ge 0, \\quad \\sum_{y \\S} p_t(x, y) = 1, \\quad \\lim_{t \\0} p_t(x, x) = p_0(x, x) = 1,\n\\]\nspełniających równania Chapmana-Kolmogorowa\n\\[\np_{s+t}(x, y) = \\sum_{z \\S} p_s(x, z)p_t(z, y).\n\\]Interpretacją wartości \\(p_t(x,y)\\) jest prawdopodobieństwo,\nże w czasie \\(t\\) proces przejdzie ze stanu \\(x\\) stanu \\(y\\).\nInnymi słowy,\n\\[\np_t(x,y) = \\mathbf{P}_x[X_t = y].\n\\]\nJak się niebawem przekonamy,\ndzięki własności Markowa pozwala ona jednoznacznie wyznaczyć rozkład procesu,\ntj. jednoznacznie wyznaczyć miarę \\(\\mathbf{P}_x\\).Przykład 2  Rozważmy \\(S = \\mathbb{N}\\), \\(\\lambda > 0\\) oraz\n\\(p_t \\colon S \\times S \\[0,1]\\) zadane przez\n\\[\np_t(x, y) = e^{-\\lambda t} \\frac{(\\lambda t)^{y - x}}{(y - x)!} \\mathbf{1}_{\\{y \\geq x\\}}.\n\\]\nWówczas \\(p\\) jest funkcją przejścia.\nWystarczy zauważyć, że \\(p_t(x)\\) prawdopodobieństwo, że zmienna losowa o\nrozkładzie Poissona z parametrem \\(\\lambda t\\) jest równa \\(y - x\\).\nRównania Chapmana-Kołmogorowa wynikają z następującej własności rozkładu Poissona:\njeżeli niezależne zmienne losowe \\(X\\) \\(Y\\) mają rozkłady Poissona odpowiednio z\nparametrami \\(\\lambda t\\) \\(\\lambda s\\),\n\\(X + Y\\) ma rozkład Poissona z parametrem \\(\\lambda (t + s)\\).Twierdzenie 1  Dla łańcucha Markowa \\((\\mathbf{P}, \\mathbb{F})\\) połóżmy\n\\[\np_t(x, y) = \\mathbf{P}_x[X_t = y]\n\\]\ndla \\(t \\ge 0\\) oraz \\(x, y \\S\\). Wówczas:\\(p_t(x, y)\\) jest funkcją przejścia,\\(p_t(x, y)\\) jest funkcją przejścia,\\(p_t(x, y)\\) określa miary \\(\\mathbf{P}_x\\) jednoznacznie.\\(p_t(x, y)\\) określa miary \\(\\mathbf{P}_x\\) jednoznacznie.Proof. Najpierw pokażemy, że \\(\\lim_{t \\0} p_t(x, x) = 1\\).\nPrzez prawostronną ciągłość ścieżek,\n\\(\\tau = \\inf \\{ t > 0 : X_t \\neq X_0 \\} > 0\\) \\(\\mathbf{P}_x\\)-p.w.\ndla dowolnego \\(x\\) z \\(S\\).\nPonieważ\n\\[p_t(x, x) \\ge \\mathbf{P}_x[\\tau > t] \\1\\]\nprzy \\(t \\0\\), istotnie \\(\\lim_{t \\0} p_t(x, x) = 1\\).Równania Chapmana-Kołmogorowa wynikają z własności Markowa.\nAby zobaczyć, rozważmy \\(Y = 1_{\\{ X(t)=y \\}}\\).\nWłasność Markowa zapisuje się jako\n\\[\n    \\mathbf{P}_x[X_{s+t} = y | \\mathcal{F}_s]\n    = \\mathbf{P}_{X(s)}[X_t = y]\n    = p_t(X(s), y) \\text{ p.n. } \\mathbf{P}_x.\n\\]\nBiorąc wartości oczekiwane względem \\(\\mathbf{P}_x\\) w tej tożsamości,\notrzymujemy równania Chapmana-Kołmogorowa:\n\\[\n    \\mathbf{P}_x[X_{t+s}=y]\n    = \\mathbf{E}_x \\left[\\mathbf{P}_x[X_{s+t} = y | \\mathcal{F}_s] \\right]\n    = \\mathbf{E}_x\\left[ p_t(X(s), y) \\right]\n    = \\sum_{z \\S } p_t(z, y) p_s(x, z).\n\\]\ndowodzi .\nb. Użyjemy własności Markowa wielokrotnie, aby otrzymać\n\\[\n    \\mathbf{P}_x[X(t_1) = x_1, \\ldots, X(t_n) = x_n]\n    = p_{t_1}(x, x_1) p_{t_2 - t_1}(x_1, x_2) \\cdots p_{t_n - t_{n-1}}(x_{n-1}, x_n)\n\\]\ndla \\(0 < t_1 < \\ldots < t_n\\) oraz \\(x_1, \\ldots, x_n \\S\\).\nAby zobaczyć oznaczmy\n\\[\n    \\mathcal{H}_{n-1} = \\{ X(t_1) = x_1, \\ldots, X(t_{n-1}) = x_{n-1} \\} \\\\mathcal{F}_{t_{n-1}}.\n\\]\nZ własności Markowa\n\\[\n    \\mathbf{P}_x[X(t_1) = x_1, \\ldots, X(t_n) = x_n | \\mathcal{F}_{t_{n-1}}]\n    = \\mathbf{P}_x[\\mathcal{H}_{n-1}, X(t_n) = x_n | \\mathcal{F}_{t_{n-1}}]\n\\]\n\\[\n    = \\mathbf{1}_{\\mathcal{H}_{n-1}} \\mathbf{P}_x[X(t_n) = x_n | \\mathcal{F}_{t_{n-1}}]\n    = \\mathbf{1}_{\\mathcal{H}_{n-1}} \\mathbf{P}_x[X \\circ \\theta_{t_{n-1}} (t_n - t_{n-1}) = x_n | \\mathcal{F}_{t_{n-1}}]\n\\]\n\\[\n    = \\mathbf{1}_{\\mathcal{H}_{n-1}} \\mathbf{P}_{X(t_{n-1})}[X (t_n - t_{n-1}) = x_n]\n    = \\mathbf{1}_{\\mathcal{H}_{n-1}} p_{t_n - t_{n-1}}(x_{n-1}, x_n).\n\\]\nBiorąc wartości oczekiwane,\n\\[\n    \\mathbf{P}_x[X(t_1) = x_1, \\ldots, X(t_n) = x_n]\n    = \\mathbf{P}_x[\\mathcal{H}_{n-1}] p_{t_n - t_{n-1}}(x_{n-1}, x_n).\n\\]\nPostulowaną równość otrzymujemy przez iterację powyższej procedury.\nUdowodniona właśnie równość uzasadnia,\nże funkcja przejścia określa rozkłady skończenie wymiarowe \\(\\mathbf{P}_x\\).\nOkreśla również pełną miarę \\(\\mathbf{P}_x\\),\nponieważ miary prawdopodobieństwa na \\((\\Omega, \\mathcal{F})\\) są określane\nprzez ich skończenie wymiarowe rozkłady w świetle twierdzenia \\(\\pi - \\lambda\\).\nAby zobaczyć, załóżmy, że \\(\\mu\\) oraz \\(\\nu\\) dwie takie miary,\nktóre mają te skończenie wymiarowe rozkłady,\nniech \\(\\mathcal{P}\\) będą skończenie wymiarowymi zbiorami w \\((\\Omega, \\mathcal{F})\\) oraz\n\\[\n    \\mathcal{L} = \\{ \\\\mathcal{F} : \\mu() = \\nu() \\}.\n\\]\nWówczas \\(\\mathcal{L}\\) jest \\(\\lambda\\)-układem zawierającym\n\\(\\pi\\)-układ \\(\\mathcal{P}\\).\nPrzez twierdzenie \\(\\pi - \\lambda\\),\n\\(\\sigma(\\mathcal{P}) \\subseteq \\mathcal{L}\\).\nSkoro \\(\\sigma(\\mathcal{P}) = \\mathcal{F}\\),\n\\(\\mu() = \\nu()\\) dla wszystkich \\(\\\\mathcal{F}\\).Skoro \\(X\\) jest elementem \\(\\Omega\\),\nzbioru funkcji prawostronnie ciągłych o wartościach w przeliczalnym \\(S\\),\n\\(X\\) jest funkcją kawałkami stałą.\nW rezultacie opisu \\(X\\) wystarczy sprecyzować,\nw jaki sposób \\(X\\) zmienia wartość.\nOpis ten jest dokonywany w kategoriach \\(Q\\)-macierzy.Definicja 3  \\(Q\\)-macierzą nazywamy macierz \\((q(x, y))_{x,y \\S}\\)\nliczb rzeczywistych indeksowanych przez \\(x, y \\S\\),\nktóre spełniają\n\\[\nq(x, y) \\ge 0 \\text{ dla } x \\neq y \\quad \\text{oraz} \\quad \\sum_y q(x, y) = 0.\n\\]Ponieważ wyrazy diagonalne są niedodatnie \nodgrywają specjalną rolę,\nnaturalne jest użycie specjalnego oznaczenia dla nich:\n\\[\nc(x) = -q(x, x).\n\\]\nPrzejście od funkcji przejścia \\(Q\\)-macierzy\njest trudniejsze niż przejście od procesu funkcji przejścia\nwymaga dokonania dodatkowych założeń.\nZaczynamy od kilku własności, które obowiązują dla wszystkich funkcji przejścia.Twierdzenie 2  Załóżmy, że \\(p\\) jest funkcją przejścia.Wówczas \\(p_t(x, x) > 0\\) dla wszystkich \\(t \\ge 0\\) oraz \\(x \\S\\).Wówczas \\(p_t(x, x) > 0\\) dla wszystkich \\(t \\ge 0\\) oraz \\(x \\S\\).Jeśli \\(p_t(x, x) = 1\\) dla pewnego \\(t > 0\\) oraz \\(x \\S\\), wtedy \\(p_t(x, x) = 1\\) dla wszystkich \\(t > 0\\) oraz tego \\(x\\).Jeśli \\(p_t(x, x) = 1\\) dla pewnego \\(t > 0\\) oraz \\(x \\S\\), wtedy \\(p_t(x, x) = 1\\) dla wszystkich \\(t > 0\\) oraz tego \\(x\\).Dla każdego \\(x, y \\S\\), \\(p_t(x, y)\\) jest jednostajnie ciągła w \\(t\\). Dokładniej,\n\\[\n   |p_t(x, y) - p_s(x, y)| \\leq 1 - p_{|t-s|}(x, x).\n\\]Dla każdego \\(x, y \\S\\), \\(p_t(x, y)\\) jest jednostajnie ciągła w \\(t\\). Dokładniej,\n\\[\n   |p_t(x, y) - p_s(x, y)| \\leq 1 - p_{|t-s|}(x, x).\n\\]Proof. Najpierw zauważmy, że \\(p_t(x, x) > 0\\) dla małych \\(t\\) zgodnie z\nprzyjętą Definicją 2\nZ równań Chapmana-Kołmogorowa,\n\\[\np_{s+t}(x, x) \\geq p_s(x, x)p_t(x, x),\n\\]\nwięc ścisła dodatniość rozciąga się na wszystkie \\(t\\).Najpierw zauważmy, że \\(p_t(x, x) > 0\\) dla małych \\(t\\) zgodnie z\nprzyjętą Definicją 2\nZ równań Chapmana-Kołmogorowa,\n\\[\np_{s+t}(x, x) \\geq p_s(x, x)p_t(x, x),\n\\]\nwięc ścisła dodatniość rozciąga się na wszystkie \\(t\\).Użyjmy równania Chapmana-Kołmogorowa raz jeszcze, aby napisać\n\\[\np_{s+t}(x, x) \\leq p_s(x, x)p_t(x, x) + [1 - p_s(x, x)] = 1 - p_s(x, x)[1 - p_t(x, x)].\n\\]\nZatem, jeśli \\(p_{s+t}(x, x) = 1\\), \\(p_t(x, x) = 1\\),\nponieważ \\(p_s(x, x) > 0\\) z części .\nStąd \\(\\{ t \\ge 0 : p_t(x, x) = 1 \\}\\) jest przedziałem zaczynającym się od 0.\nZ dowodu części . wynika, że musi być cała dodatnia oś.Użyjmy równania Chapmana-Kołmogorowa raz jeszcze, aby napisać\n\\[\np_{s+t}(x, x) \\leq p_s(x, x)p_t(x, x) + [1 - p_s(x, x)] = 1 - p_s(x, x)[1 - p_t(x, x)].\n\\]\nZatem, jeśli \\(p_{s+t}(x, x) = 1\\), \\(p_t(x, x) = 1\\),\nponieważ \\(p_s(x, x) > 0\\) z części .\nStąd \\(\\{ t \\ge 0 : p_t(x, x) = 1 \\}\\) jest przedziałem zaczynającym się od 0.\nZ dowodu części . wynika, że musi być cała dodatnia oś.Ponownie użyjmy równania Chapmana-Kołmogorowa,\naby napisać\n\\[\np_{t+s}(x, y) - p_t(x, y) = p_t(x, y)[p_s(x, x) - 1] + \\sum_{z \\neq x} p_s(x, z)p_t(z, y).\n\\]\nPierwszy składnik po prawej stronie jest niedodatni,\ndrugi jest nieujemny. Wartość bezwzględna każdego z nich nie jest większa niż \\(1 - p_s(x, x)\\).\npociąga postulowaną nierówność, która z kolei pociąga jednostajną ciągłość.Ponownie użyjmy równania Chapmana-Kołmogorowa,\naby napisać\n\\[\np_{t+s}(x, y) - p_t(x, y) = p_t(x, y)[p_s(x, x) - 1] + \\sum_{z \\neq x} p_s(x, z)p_t(z, y).\n\\]\nPierwszy składnik po prawej stronie jest niedodatni,\ndrugi jest nieujemny. Wartość bezwzględna każdego z nich nie jest większa niż \\(1 - p_s(x, x)\\).\npociąga postulowaną nierówność, która z kolei pociąga jednostajną ciągłość.Twierdzenie 3  Załóżmy, że \\(p\\) jest funkcją przejścia.Dla każdego \\(x\\), prawostronna pochodna\n\\[\n     c(x) = -q(x, x) = - \\left. \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, x) \\right|_{t=0} \\[0, \\infty]\n\\]\nistnieje spełnia\n\\[\n     p_t(x, x) \\ge e^{-c(x)t}.\n\\]Dla każdego \\(x\\), prawostronna pochodna\n\\[\n     c(x) = -q(x, x) = - \\left. \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, x) \\right|_{t=0} \\[0, \\infty]\n\\]\nistnieje spełnia\n\\[\n     p_t(x, x) \\ge e^{-c(x)t}.\n\\]Jeśli \\(c(x) < \\infty\\), wtedy dla tego \\(x\\) dla wszystkich \\(y \\neq x\\), prawostronna pochodna\n\\[\n     q(x, y) = \\left. \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, y) \\right|_{t=0} \\[0, \\infty)\n\\]\nistnieje oraz\n\\[\n     \\sum_{y \\S} q(x, y) \\leq 0.\n\\]Jeśli \\(c(x) < \\infty\\), wtedy dla tego \\(x\\) dla wszystkich \\(y \\neq x\\), prawostronna pochodna\n\\[\n     q(x, y) = \\left. \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, y) \\right|_{t=0} \\[0, \\infty)\n\\]\nistnieje oraz\n\\[\n     \\sum_{y \\S} q(x, y) \\leq 0.\n\\]Jeśli dla pewnego \\(x \\S\\),\n\\(c(x) < \\infty\\) \\(\\sum_y q(x, y) = 0\\), \\(p_t(x, y)\\)\njest różniczkowalna w sposób ciągły względem \\(t\\) dla tego \\(x\\) każdego \\(y\\),\noraz spełnia równania retrospektywne Kołmogorowa:\n\\[\n     \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, y) = \\sum_z q(x, z)p_t(z, y).\n\\]Jeśli dla pewnego \\(x \\S\\),\n\\(c(x) < \\infty\\) \\(\\sum_y q(x, y) = 0\\), \\(p_t(x, y)\\)\njest różniczkowalna w sposób ciągły względem \\(t\\) dla tego \\(x\\) każdego \\(y\\),\noraz spełnia równania retrospektywne Kołmogorowa:\n\\[\n     \\frac{\\mathrm{d}}{\\mathrm{d} t} p_t(x, y) = \\sum_z q(x, z)p_t(z, y).\n\\]Proof. Niech \\(f(t) = - \\log p_t(x, x)\\).\nWówczas \\(f\\) jest dobrze określona jednostajnie ciągła.\nJest ona też subaddytywna.\nZatem, zgodnie z Lematem Fekete,\n\\[\nc(x) = \\lim_{t \\0} \\frac{f(t)}{t} \\[0, \\infty]\n\\]\nspełnia \\(f(t) \\leq c(x)t\\).Niech \\(f(t) = - \\log p_t(x, x)\\).\nWówczas \\(f\\) jest dobrze określona jednostajnie ciągła.\nJest ona też subaddytywna.\nZatem, zgodnie z Lematem Fekete,\n\\[\nc(x) = \\lim_{t \\0} \\frac{f(t)}{t} \\[0, \\infty]\n\\]\nspełnia \\(f(t) \\leq c(x)t\\).Przypuśćmy, że \\(c(x) < \\infty\\). Z dowiedzionej właśnie nierówności,\n\\[\n1 - p_t(x, x) \\leq 1 - e^{-c(x)t} \\leq c(x)t,\n\\]\nstąd\n\\[\n\\sum_{y : y \\neq x} \\frac{p_t(x, y)}{t} \\leq c(x).\n\\]\nZatem,\n\\[\n\\limsup_{t \\0} \\frac{p_t(x, y)}{t} < \\infty\n\\tag{1}\n\\]\ndla \\(y \\neq x\\).\nNiech \\(q(x, y)\\) będzie wartością powyższej granicy górnej.\nAby pokazać, że granica rzeczywiście istnieje,\nweźmy \\(\\delta > 0\\) dodatnią liczbę całkowitą \\(n\\).\nMacierz \\(p_{\\delta}(x, y)\\) można traktować jako prawdopodobieństwa\nprzejścia dla dyskretnego łańcucha Markowa na \\(S\\),\nwtedy zgodnie z równaniami Chapmana-Kołmogorowa,\nodpowiadające \\(n\\)-krokowe prawdopodobieństwa przejścia są dane przez \\(p_{n\\delta}(x, y)\\).\nRozkładając zdarzenie,\nże ten łańcuch znajduje się w \\(y\\) w czasie \\(n\\)\nzgodnie z czasem pierwszej wizyty w \\(y\\), mamy dla \\(y \\neq x\\),\n\\[\np_{n\\delta}(x, y) \\geq \\sum_{k=0}^{n-1} p_{k\\delta}^k(x, x) p_{\\delta}(x, y)p_{(n-k-1)\\delta}(y, y).\n\\]\nStąd\n\\[\n\\frac{p_{n\\delta}(x, y)}{n\\delta}\n\\geq \\frac{p_{\\delta}(x, y)}{\\delta} e^{-c(x)n\\delta} \\inf_{0 \\leq s \\leq n\\delta} p_s(y, y).\n\\]\nTeraz niech \\(\\delta \\downarrow 0\\) wzdłuż\nciągu realizującego granicę górną w (1), tak że\n\\[\n\\frac{p_{\\delta}(x, y)}{\\delta} \\q(x, y).\n\\]\nWybierzmy teraz \\(n \\\\infty\\) tak, że \\(n\\delta \\t\\). Wówczas\n\\[\n\\frac{p_t(x, y)}{t} \\geq q(x, y)e^{-c(x)t} \\inf_{0 \\leq s \\leq t} p_s(y, y)\n\\]\ndla \\(t > 0\\). Zatem,\n\\[\n\\liminf_{t \\0} \\frac{p_t(x, y)}{t} \\geq q(x, y).\n\\]\nPostulowana nierówność wynika teraz z lematu Fatou.Przypuśćmy, że \\(c(x) < \\infty\\). Z dowiedzionej właśnie nierówności,\n\\[\n1 - p_t(x, x) \\leq 1 - e^{-c(x)t} \\leq c(x)t,\n\\]\nstąd\n\\[\n\\sum_{y : y \\neq x} \\frac{p_t(x, y)}{t} \\leq c(x).\n\\]\nZatem,\n\\[\n\\limsup_{t \\0} \\frac{p_t(x, y)}{t} < \\infty\n\\tag{1}\n\\]\ndla \\(y \\neq x\\).\nNiech \\(q(x, y)\\) będzie wartością powyższej granicy górnej.\nAby pokazać, że granica rzeczywiście istnieje,\nweźmy \\(\\delta > 0\\) dodatnią liczbę całkowitą \\(n\\).\nMacierz \\(p_{\\delta}(x, y)\\) można traktować jako prawdopodobieństwa\nprzejścia dla dyskretnego łańcucha Markowa na \\(S\\),\nwtedy zgodnie z równaniami Chapmana-Kołmogorowa,\nodpowiadające \\(n\\)-krokowe prawdopodobieństwa przejścia są dane przez \\(p_{n\\delta}(x, y)\\).\nRozkładając zdarzenie,\nże ten łańcuch znajduje się w \\(y\\) w czasie \\(n\\)\nzgodnie z czasem pierwszej wizyty w \\(y\\), mamy dla \\(y \\neq x\\),\n\\[\np_{n\\delta}(x, y) \\geq \\sum_{k=0}^{n-1} p_{k\\delta}^k(x, x) p_{\\delta}(x, y)p_{(n-k-1)\\delta}(y, y).\n\\]\nStąd\n\\[\n\\frac{p_{n\\delta}(x, y)}{n\\delta}\n\\geq \\frac{p_{\\delta}(x, y)}{\\delta} e^{-c(x)n\\delta} \\inf_{0 \\leq s \\leq n\\delta} p_s(y, y).\n\\]\nTeraz niech \\(\\delta \\downarrow 0\\) wzdłuż\nciągu realizującego granicę górną w (1), tak że\n\\[\n\\frac{p_{\\delta}(x, y)}{\\delta} \\q(x, y).\n\\]\nWybierzmy teraz \\(n \\\\infty\\) tak, że \\(n\\delta \\t\\). Wówczas\n\\[\n\\frac{p_t(x, y)}{t} \\geq q(x, y)e^{-c(x)t} \\inf_{0 \\leq s \\leq t} p_s(y, y)\n\\]\ndla \\(t > 0\\). Zatem,\n\\[\n\\liminf_{t \\0} \\frac{p_t(x, y)}{t} \\geq q(x, y).\n\\]\nPostulowana nierówność wynika teraz z lematu Fatou.Napiszmy\n\\[\n\\frac{p_{t+s}(x, y) - p_t(x, y)}{s} = \\sum_z \\left[ \\frac{p_s(x, z) - p_0(x, z)}{s} - q(x, z) \\right] p_t(z, y).\n\\]\nKażdy wyraz w sumie dąży \\(0\\), gdy \\(s \\downarrow 0\\),\nzgodnie z pierwszymi dwoma częściami twierdzenia.\nZatem musimy kontrolować ogony sumy.\nWeźmy skończony zbiór \\(T \\subset S\\) zawierający \\(x\\) zauważmy, że\n\\[\n\\sum_{z \\notin T} \\left| \\frac{p_s(x, z)}{s} - q(x, z) \\right| p_t(z, y) \\leq \\sum_{z \\notin T} \\frac{p_s(x, z)}{s} + \\sum_{z \\notin T} q(x, z)\n\\]\n\\[\n= s^{-1} \\left[ 1 - \\sum_{z \\T} p_s(x, z) \\right] - \\sum_{z \\T} q(x, z) \\-2 \\sum_{z \\T} q(x, z)\n\\]\ngdy \\(s \\downarrow 0\\).\nGranicę po prawej stronie można uczynić dowolnie małą,\nwybierając \\(T\\) duże, ponieważ \\(\\sum_z q(x, z) = 0\\).\nZatem prawa strona dąży zera,\ngdy \\(s \\downarrow 0\\). dowodzi, że prawostronne pochodne \\(p_t(x, y)\\)\nistnieją spełniają zadaną równość.\nAby zobaczyć, że obustronne pochodne rzeczywiście istnieją,\nwystarczy zauważyć, że prawa strona jest ciągła w \\(t\\) użyć faktu,\nże funkcja ciągła z ciągłą prawostronną pochodną jest różniczkowalna.Napiszmy\n\\[\n\\frac{p_{t+s}(x, y) - p_t(x, y)}{s} = \\sum_z \\left[ \\frac{p_s(x, z) - p_0(x, z)}{s} - q(x, z) \\right] p_t(z, y).\n\\]\nKażdy wyraz w sumie dąży \\(0\\), gdy \\(s \\downarrow 0\\),\nzgodnie z pierwszymi dwoma częściami twierdzenia.\nZatem musimy kontrolować ogony sumy.\nWeźmy skończony zbiór \\(T \\subset S\\) zawierający \\(x\\) zauważmy, że\n\\[\n\\sum_{z \\notin T} \\left| \\frac{p_s(x, z)}{s} - q(x, z) \\right| p_t(z, y) \\leq \\sum_{z \\notin T} \\frac{p_s(x, z)}{s} + \\sum_{z \\notin T} q(x, z)\n\\]\n\\[\n= s^{-1} \\left[ 1 - \\sum_{z \\T} p_s(x, z) \\right] - \\sum_{z \\T} q(x, z) \\-2 \\sum_{z \\T} q(x, z)\n\\]\ngdy \\(s \\downarrow 0\\).\nGranicę po prawej stronie można uczynić dowolnie małą,\nwybierając \\(T\\) duże, ponieważ \\(\\sum_z q(x, z) = 0\\).\nZatem prawa strona dąży zera,\ngdy \\(s \\downarrow 0\\). dowodzi, że prawostronne pochodne \\(p_t(x, y)\\)\nistnieją spełniają zadaną równość.\nAby zobaczyć, że obustronne pochodne rzeczywiście istnieją,\nwystarczy zauważyć, że prawa strona jest ciągła w \\(t\\) użyć faktu,\nże funkcja ciągła z ciągłą prawostronną pochodną jest różniczkowalna.","code":"## List of 1\n##  $ plot.margin: 'margin' num [1:4] 0mm 25mm 0mm 0mm\n##   ..- attr(*, \"unit\")= int 7\n##  - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n##  - attr(*, \"complete\")= logi FALSE\n##  - attr(*, \"validate\")= logi TRUE"},{"path":"section-wykład-2-mocna-własność-markowa.html","id":"section-wykład-2-mocna-własność-markowa","chapter":"Wykład 2: Mocna własność Markowa","heading":"Wykład 2: Mocna własność Markowa","text":"2024-10-10Piotr DyszewskiPojęcie czasu zatrzymania odgrywa kluczową rolę w teorii procesów stochastycznych.\nSą losowe momenty adaptowalne z góry zadanej filtracji.\nJest kluczowa koncepcj w silnej własności Markowa.\nBędziemy korzystać z ciągłej filtracji \\(\\mathbb{F}=( \\mathcal{F}_t )_{t \\\\mathbb{R}_+}\\).","code":""},{"path":"section-wykład-2-mocna-własność-markowa.html","id":"section-czasy-zatrzymania","chapter":"Wykład 2: Mocna własność Markowa","heading":"Czasy zatrzymania","text":"Przypomnijmy, że w czasie dyskretnym definicja czasu zatrzymania \\(\\tau\\) jest taka,\nże \\(\\{ \\tau = n \\} \\\\mathcal{F}_n\\) dla każdego naturalnego \\(n\\).\nJest równoważne z warunkiem, że \\(\\{ \\tau \\leq n \\} \\\\mathcal{F}_n\\)\ndla każdego naturalnego \\(n\\). W czasie ciągłym ta równoważność nie zachodzi,\nponieważ \\([0, \\infty)\\) nie jest przeliczalne.\nWarunek analogiczny tego drugiego jest naturalny użycia w czasie ciągłym,\nponieważ zazwyczaj zdarzenie \\(\\{ \\tau = t \\}\\) ma zerowe prawdopodobieństwo dla każdego \\(t\\).Definicja 4  Zmienna losowa \\(\\tau : \\Omega \\[0, \\infty]\\) nazywana jest \\(\\mathbb{F}\\)-czasem zatrzymania,\njeśli \\(\\{ \\tau \\leq t \\} \\\\mathcal{F}_t\\) dla każdego \\(t \\geq 0\\).W niektórych kontekstach filtracja \\(\\mathbb{F}\\), z którą pracujemy,\njest na tyle regularna, że ułatwia weryfikację, czy zmienna jest czasem zatrzymania.Definicja 5  Powiemy, że filtracja \\(\\mathbb{F} = (\\mathcal{F}_t)_{t \\\\mathbb{R}_+}\\) jest prawostronnie ciągła, jeżeli\n\\[\n\\mathcal{F}_t = \\mathcal{F}_{t_+}, \\qquad \\text{gdzie} \\qquad \\mathcal{F}_{t+} := \\bigcap_{s > t} \\mathcal{F}_s\n\\]\ndla każdego \\(t \\\\mathbb{R}_+\\).Zadanie 1  Załóżmy, że filtracja \\(\\mathbb{F}\\) jest prawostronnie ciągła.\nWówczas \\(\\tau\\) jest czasem zatrzymania wtedy tylko wtedy,\ngdy \\(\\{ \\tau < t \\} \\\\mathcal{F}_t\\) dla każdego \\(t \\\\mathbb{R}_+\\).Zadanie 2  Pokaż, że jeśli \\(\\tau_1\\) \\(\\tau_2\\) są czasami zatrzymania,\nrównież \\(\\tau_1 \\wedge \\tau_2\\), \\(\\tau_1 \\vee \\tau_2\\) \n\\(\\tau_1 + \\tau_2\\) są czasami zatrzymania.Zadanie 3  Udowodnij, że jeśli \\(\\{\\tau_n\\}_{n \\\\mathbb{N}}\\) jest ciągiem czasów zatrzymania,\nktóre maleją \\(\\tau\\), \\(\\tau\\) jest czasem zatrzymania.Własność Markowa dotyczy warunkowej wartości oczekiwanej względem \\(\\mathcal{F}_s\\) dla ustalonego \\(s\\).\nMocna własność Markowa jest analogiczna, ale warunkowanie odbywa się względem \\(\\sigma\\)-algebry \\(\\mathcal{F}_\\tau\\),\ngdzie \\(\\tau\\) jest czasem stopu.\nSkłada się ona ze zdarzeń, które są określone przez przeszłość aż czasu \\(\\tau\\).Definicja 6  Dla czasu zatrzymania \\(\\tau\\) kładziemy\n\\[\n\\mathcal{F}_\\tau = \\left\\{ \\\\mathcal{F} : \\cap \\{ \\tau \\leq t \\} \\\\mathcal{F}_t \\text{ dla każdego } t \\\\mathbb{R}_+ \\right\\}.\n\\]Zadanie 4  Pokaż, że:\\(\\mathcal{F}_\\tau\\) jest \\(\\sigma\\)-algebrą,Załóżmy, że \\(\\mathbb{F}\\) jest prawostronnie ciągła. Pokaż, że\n\\[\n\\mathcal{F}_\\tau = \\{ : \\cap \\{ \\tau < t \\} \\\\mathcal{F}_t \\text{ dla każdego } t \\geq 0 \\}.\n\\]Oto niektóre podstawowe własności \\(\\mathcal{F}_\\tau\\).Twierdzenie 4  Jeśli \\(\\tau\\), \\(\\tau_n\\), \\(n \\\\mathbb{N}\\) są czasasmi zatrzymania, :\\(\\tau\\) jest mierzalny względem \\(\\mathcal{F}_\\tau\\).Jeśli \\(\\tau_n \\downarrow \\tau\\), \\(\\mathcal{F}_\\tau = \\bigcap_n \\mathcal{F}_{\\tau_n}\\).\\(\\tau_1 \\leq \\tau_2\\) implikuje \\(\\mathcal{F}_{\\tau_1} \\subseteq \\mathcal{F}_{\\tau_2}\\).Proof. Zadanie.","code":""},{"path":"section-wykład-2-mocna-własność-markowa.html","id":"section-mocna-własność-markowa","chapter":"Wykład 2: Mocna własność Markowa","heading":"Mocna własność Markowa","text":"Twierdzenie 5  Niech \\((\\mathbb{P}, \\mathbb{F})\\) będzie łańcuchem Markowa na przeliczalnej przestrzeni stanów \\(S\\).\nZałóżmy, że \\(Y\\) jest ograniczoną zmienną losową oraz że \\(\\tau\\) jest czasem zatrzymania.\nWówczas dla każdego \\(x \\S\\),\n\\[\\begin{equation}\n\\mathbf{E}_x [ Y \\circ \\theta_\\tau | \\mathcal{F}_\\tau]\n= \\mathbf{E}_{X(\\tau)} [Y] \\quad \\mathbf{P}_x-\\text{prawie na pewno} \\quad \\text{na} \\quad \\{\\tau < \\infty\\}.\n\\tag{2}\n\\end{equation}\\]Silna własność Markowa jest zazwyczaj używana w następujący sposób:\nPrzemnóż równość (2) przez \\(\\mathbf{1}_{\\{\\tau < \\infty\\}}\\),\nnastępnie zastosuj \\(\\mathbf{E}_x\\).\nWynik, z uwzględnieniem, że \\(\\{\\tau < \\infty\\} \\\\mathcal{F}_\\tau\\), :\n\\[\n\\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_{\\{\\tau < \\infty\\}} \\right]\n= \\mathbf{E}_x \\left[ \\mathbf{E}_{X(\\tau)} \\left[ Y \\right] \\mathbf{1}_{\\{ \\tau < \\infty\\}} \\right].\n\\]Proof (Twierdzenia 5). Najpierw załóżmy, że \\(\\tau\\) przyjmuje wartości z przeliczalnego zbioru \\(0 \\leq t_1 \\leq t_2 \\leq \\dots\\) oraz \\(\\infty\\).\nW tym przypadku silna własność Markowa sprowadza się własności Markowa, jak teraz pokażemy.Zauważmy, że prawa strona (2) jest mierzalna względem \\(\\mathcal{F}_\\tau\\).\nMusimy więc sprawdzić, że jeśli \\(\\\\mathcal{F}_\\tau\\) oraz \\(\\subseteq \\{\\tau < \\infty\\}\\), \n\\[\n\\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_A \\right]\n= \\mathbf{E}_x \\left[ \\mathbf{E}_{X(\\tau)} \\left[ Y \\right] \\mathbf{1}_A \\right].\n\\]\nAby wykazać, napiszmy:\n\\[\\begin{multline*}\n\\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_A \\right]\n= \\sum_{n \\\\mathbb{N}} \\mathbf{E}_x \\left[ Y \\circ \\theta_{t_n} \\mathbf{1}_{\\cap \\{\\tau = t_n\\}} \\right] \\\\\n= \\sum_{n \\\\mathbb{N}} \\mathbf{E}_x \\left[ \\mathbf{E}_{X(t_n)} \\left[ Y \\right] \\mathbf{1}_{ \\cap \\{\\tau = t_n\\}} \\right]\n=\\mathbf{E}_x\\left[ \\mathbf{E}_{X(\\tau)}[Y] \\mathbf{1}_A\\right].\n\\end{multline*}\\]\nW drugim kroku skorzystaliśmy z własności Markowa, ponieważ\n\\[\n\\cap \\{\\tau = t_n\\} \\\\mathcal{F}_{t_n}\n\\]\nzgodnie z definicją \\(\\mathcal{F}_\\tau\\).W drugim kroku uzasadnimy tezę dla dowolnych \\(\\tau\\) \\(Y\\) postaci\n\\[\\begin{equation}\nY(\\omega) = \\prod_{j=1}^m f_j(\\omega(t_j)),\n\\tag{3}\n\\end{equation}\\]\ndla pewnego \\(m \\\\mathbb{N}\\),\n\\(t_1, \\ldots, t_m \\\\mathbb{R}_+\\) oraz ograniczonych funkcji\n\\(f_1, \\ldots, f_m \\colon S \\\\mathbb{R}\\).Czas \\(\\tau\\) przybliżamy go od góry czasami stopu \\(\\{\\tau_n\\}_{n \\\\mathbb{N}}\\) zdefiniowanymi przez\n\\[\n\\tau_n = \\frac{k + 1}{2^n} \\quad \\text{jeśli} \\quad \\frac{k}{2^n} \\leq \\tau < \\frac{k + 1}{2^n}.\n\\]\ndla dostatecznie dużych \\(k\\).\nWeźmy teraz \\(\\\\mathcal{F}_\\tau \\subseteq \\mathcal{F}_{\\tau_k}\\) takie, że \\(\\subseteq \\{\\tau < \\infty\\}\\).\nZ pierwszej części dowodu,\n\\[\n    \\mathbf{E}_x \\left[ Y \\circ \\theta_{\\tau_k} \\mathbf{1}_A \\right]\n    = \\mathbf{E}_x\\left[ \\mathbf{E}_{X(\\tau_k)}[Y] \\mathbf{1}_A \\right].\n\\]\nMusimy przejść granicy, gdy \\(k \\\\infty\\). Po prawej stronie,\n\\(\\tau_k \\downarrow \\tau\\) z prawostronnej ciągłości \\(X(\\tau_k) \\X(\\tau)\\) w \\(S\\), czyli \\(X(\\tau_k) = X(\\tau)\\) dla dostatecznie dużych \\(k\\).\nPo lewej stronie, napiszmy\n\\[\\begin{equation*}\n\\left( Y \\circ \\theta_{\\tau_k} \\right)(\\omega) = \\prod_{m=1}^n f_m(\\omega(t_m + \\tau_k))\n\\\\prod_{m=1}^n f_m(\\omega(t_m + \\tau)) = \\left( Y \\circ \\theta_\\tau \\right)(\\omega).\n\\end{equation*}\\]\nkiedy \\(k \\\\infty\\), dzięki prawostronnej ciągłości ścieżek. pokazuje tezę:\n\\[\n\\mathbf{E}_x \\left[ Y \\circ \\theta_\\tau \\mathbf{1}_A \\right] = \\mathbf{E}_x \\left[ \\mathbf{E}_{X(\\tau)}[Y] \\mathbf{1}_A \\right].\n\\]\nW ostatnim kroku dowodu pokażemy tezę dla dowolnego \\(Y\\).\nDla dowolnego \\(m \\\\mathbb{N}\\) oraz \\(t_1, \\ldots, t_m \\\\mathbb{R}_+\\) rozważmy \\(\\pi_{t_1, \\ldots, t_m} \\colon \\Omega \\\\mathbb{R}^m\\)\ndane wzorem\n\\[\\begin{equation*}\n  \\pi_{t_1, \\ldots, t_m}(\\omega) = (\\omega(t_1), \\omega(t_2), \\ldots, \\omega(t_m)).\n\\end{equation*}\\]\nWówczas dla dowolnych \\(A_1, A_2, \\ldots, A_m \\subseteq S\\),\n\\[\\begin{equation*}\n\\pi_{t_1, \\ldots, t_m}^{-1}[A_1\\times A_2 \\times \\ldots \\times A_m ]\n= \\{ \\omega \\\\Omega \\: : \\:\n\\omega(t_1) \\A_1, \\ldots, \\omega(t_m) \\A_m \\}.\n\\end{equation*}\\]\nRozważmy teraz \\(\\pi\\)-układ\n\\[\\begin{equation*}\n    \\mathcal{B} = \\left\\{ \\pi_{t_1, \\ldots, t_m}^{-1}[A_1 \\times A_2 \\times \\ldots \\times A_m ] \\: : \\:  \n    m \\\\mathbb{N}, t_1, \\ldots , t_m \\\\mathbb{R}_+, \\: A_1, \\ldots , A_m \\subseteq S \\right\\}.\n\\end{equation*}\\]\noraz \\(\\lambda\\)-układ\n\\[\\begin{equation*}\n   \\mathcal{L} = \\left\\{ G \\\\mathcal{F} \\: : \\: \\mathbf{P}_x \\left[ \\theta_\\tau \\G, \\right] =\n   \\mathbf{E}_x \\left[ \\mathbf{P}_{X(\\tau)} \\left[ G \\right] \\mathbf{1}_{ } \\right] \\text{ dla } \\\\mathcal{F}_\\tau,\n   \\: \\subseteq \\{\\tau < \\infty\\} \\right\\}.\n\\end{equation*}\\]\nAproksymując \\(\\mathbf{1}_{\\pi_{t_1, \\ldots, t_m}^{-1}[A_1 \\times \\ldots \\times A_m]}\\)\nzmiennymi \\(Y\\) postaci (3),\ndostajemy \\(\\mathcal{B} \\subseteq \\mathcal{L}\\).\nZ lematu o \\(\\pi\\)-\\(\\lambda\\) układach mamy \\(\\mathcal{F} = \\sigma(\\mathcal{B}) \\subseteq \\mathcal{L}\\).\nCzyli dla każdego \\(G \\\\mathcal{F}\\),\n\\[\\begin{equation*}\n  \\mathbf{P}_x \\left[ \\theta_\\tau \\G, \\right] =  \\mathbf{E}_x \\left[ \\mathbf{P}_{X(\\tau)} \\left[ G \\right] \\mathbf{1}_{} \\right]\n\\end{equation*}\\]\ndla każdego \\(\\\\mathcal{F}_\\tau\\) takiego, że \\(\\subseteq \\{\\tau < \\infty\\}\\).\nJest równoważne naszej tezie dla \\(Y = \\mathbf{1}_G\\).\nZ liniowości teza jest zatem prawdziwa dla każdego \\(Y\\) przyjmującego skończenie wiele wartości.\nZastosowanie standardowego twierdzenia granicznego dowodzi tezy dla dowolnego ograniczonego \\(Y\\).","code":""},{"path":"section-wykład-2-mocna-własność-markowa.html","id":"section-charakteryzacja","chapter":"Wykład 2: Mocna własność Markowa","heading":"Charakteryzacja","text":"Zauważmy, że każda funkcja \\(\\omega \\\\Omega\\) musi być następującego typu:\nIstnieje \\(t_1 \\(0, \\infty]\\), taki że \\(\\omega(t) = \\omega(0)\\)\ndla każdego \\(t \\[0, t_1)\\),\nnastępnie, jeśli \\(t_1 < \\infty\\), istnieje \\(t_2 \\(t_1, \\infty]\\) taki,\nże \\(\\omega(t) = \\omega(t_1) \\neq \\omega(0)\\)\ndla każdego \\(t \\[t_1, t_2)\\), tak dalej.\nPowyższe czasy \\(t_1, t_2, \\ldots\\) zależą oczywiście od wyboru \\(\\omega\\).\nDla każdego \\(\\omega \\\\Omega\\), istnieje zatem ciąg\n\\[\nT_0(\\omega) = 0 < T_1(\\omega) \\leq T_2(\\omega) \\leq T_3(\\omega) \\leq \\dots \\leq \\infty,\n\\]\ntaki, że \\(X_t(\\omega) = X_0(\\omega)\\) dla każdego \\(t \\[0, T_1(\\omega))\\)\noraz dla każdej liczby całkowitej \\(\\geq 1\\),\nwarunek \\(T_i(\\omega) < \\infty\\) implikuje \\(T_i(\\omega) < T_{+1}(\\omega)\\),\n\\(X_{T_i(\\omega)}(\\omega) \\neq X_{T_{-1}(\\omega)}(\\omega)\\) \n\\(X_t(\\omega) = X_{T_i(\\omega)}(\\omega)\\) dla każdego \\(t \\[T_i(\\omega), T_{+1}(\\omega))\\).\nCo więcej, \\(T_n(\\omega) \\uparrow \\infty\\), gdy \\(n \\\\infty\\).\nNietrudno jest sprawdzić, że \\(T_0, T_1, T_2, \\dots\\) są czasami stopu. Na przykład,\n\\[\n    \\{ T_1 \\leq t \\} = \\{X(t) \\neq X(0)\\} \\cup \\bigcup_{q \\(0, 1) \\cap \\mathbb{Q}} \\{ X_q \\neq X_0 \\} \\\\mathcal{F}_t.\n\\]\nPrzypomnijmy, że dla \\(\\lambda > 0\\),\ndodatnia zmienna losowa \\(U\\) ma rozkład wykładniczy z parametrem \\(\\lambda\\),\njeśli \\(\\mathbb{P}[U > r] = e^{-\\lambda r}\\)\ndla każdego \\(r \\geq 0\\).\nW poniższym lemacie przyjmujemy konwencję, że zmienna losowa wykładnicza o parametrze \\(0\\) jest równa \\(\\infty\\) prawie na pewno.Lemma 1  Niech \\(x \\S\\).\nIstnieje rzeczywista liczba \\(c(x) \\geq 0\\),\ntaka że zmienna losowa \\(T_1\\) ma rozkład wykładniczy z parametrem \\(c(x)\\)\npod \\(\\mathbf{P}_x\\). Co więcej, jeśli \\(c(x) > 0\\),\n\\(T_1\\) \\(X_{T_1}\\) są niezależne pod \\(\\mathbf{P}_x\\).Proof. Niech \\(s, t \\geq 0\\). Mamy\n\\[\n\\mathbf{P}_x[T_1 > s + t] = \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 > s\\}} \\Phi \\circ \\theta_s],\n\\]\ngdzie \\(\\Phi(\\omega) = \\mathbf{1}_{\\{\\omega(r)=\\omega(0), \\, \\forall r \\[0,t]\\}}\\).\nUżywając własności Markowa, dostajemy\n\\[\\begin{multline*}\n\\mathbf{P}_x[T_1 > s + t] = \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 > s\\}} \\mathbf{E}_x[\\Phi]] \\\\\n= \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 > s\\}} \\mathbf{P}_x[T_1 > t]] = \\mathbf{P}_x[T_1 > s] \\mathbf{P}_x[T_1 > t],\n\\end{multline*}\\]\nco implikuje, że \\(T_1\\) ma rozkład wykładniczy pod \\(\\mathbf{P}_x\\).Załóżmy teraz, że \\(c(x) > 0\\). Wówczas \\(T_1 < \\infty\\), \\(\\mathbf{P}_x\\) prawie na pewno.\nDla każdego \\(t \\geq 0\\) \\(y \\S\\),\n\\[\\begin{equation*}\n\\mathbf{P}_x[T_1 > t, X_{T_1} = y] = \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 > t\\}} \\Psi \\circ \\theta_t ],\n\\end{equation*}\\]\ngdzie dla \\(\\omega \\\\Omega\\), \\(\\Psi(\\omega) = 0\\) jeśli \\(\\omega\\) jest stałe,\nw przeciwnym razie \\(\\Psi(\\omega) = \\mathbf{1}_{\\{ \\gamma_1(\\omega) = y\\}}\\),\ngdzie \\(\\gamma_1(\\omega)\\) jest wartością \\(\\omega\\)\npo jego pierwszym skoku. Zatem mamy\n\\[\\begin{multline*}\n\\mathbf{P}_x[T_1 > t, X_{T_1} = y] =\n\\mathbf{E}_x [\\mathbf{1}_{\\{T_1 > t\\}} \\mathbf{E}_x[\\Psi ]] = \\\\ \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 > t\\}} \\mathbf{P}_x[X_{T_1} = y]]\n= \\mathbf{P}_x[T_1 > t] \\mathbf{P}_x[X_{T_1} = y],\n\\end{multline*}\\]\nco daje pożądaną niezależność.Punkty, dla których \\(c(x) = 0\\),\nsą stanami pochłaniającymi dla procesu Markowa,\nw tym sensie, że\n\\(\\mathbf{P}_x[X_t = x, \\, \\forall t \\geq 0] = 1\\).\nDla każdych \\(x, y \\S\\) definiujemy\n\\[\\begin{equation*}\n\\Pi(x, y) = \\left\\{ \\begin{array}{cc} \\mathbf{P}_x[X_{T_1} = y] & c(x)>0 \\\\ \\delta_x(y) & c(x)=0 \\end{array}\\right.\n\\end{equation*}\\]\nZauważmy, że \\(\\Pi(x, \\cdot)\\) jest miarą prawdopodobieństwa na \\(S\\).Twierdzenie 6  Niech \\((\\mathbf{P}, \\mathbb{F})\\) będzie łańcuchem Markowa\nw czasie ciągłym takim, że \\(\\sup_{x \\S} c(x) < \\infty\\).\nWówczas\n\\[\\begin{equation*}\n\\frac{\\mathrm{d}}{\\mathrm{d} t} \\left. \\mathbb{P}_x[X_t=y]\\right|_{t=0} = c(x) \\Pi(x,y).\n\\end{equation*}\\]Proof. Jeśli \\(c(x) = 0\\), \\(\\mathbf{P}_x[X_t=x] = \\mathbf{P}_x[X_0 =x]=1\\), stąd\n\\[\\begin{equation*}\n\\lim_{t \\0} \\frac{\\mathbf{P}_x[X_t=x] - 1}{t} = 0.\n\\end{equation*}\\]\nZałóżmy teraz, że \\(c(x) > 0\\). Najpierw zauważmy, że\n\\[\\begin{equation}\n\\mathbf{P}_x[T_2 \\leq t] = O\\left(t^2\\right)\n\\tag{4}\n\\end{equation}\\]\ngdy \\(t \\0\\). Rzeczywiście, używając silnej własności Markowa w \\(T_1\\),\n\\[\\begin{equation*}\n\\mathbf{P}_x[T_2 \\leq t] \\leq \\mathbf{P}x[T_1 \\leq t, T_2 \\leq T_1 + t]\n= \\mathbf{E}_x[\\mathbf{1}_{\\{T_1 \\leq t\\}} \\mathbf{E}_{X_{T_1}}[T_1 \\leq t]],\n\\end{equation*}\\]\nmożemy oszacować\n\\[\\begin{equation*}\n\\mathbf{P}_{X_{T_1}}[T_1 \\leq t] \\leq \\sup_{y \\S} \\mathbf{P}_y[T_1 \\leq t] \\leq \\sup_{y \\S} c(y),\n\\end{equation*}\\]\nco daje oczekiwany wynik, ponieważ mamy również \\(\\mathbf{P}_x[T_1 \\leq t] \\leq c(x)t\\).\nZ (4) wynika, że\n\\[\\begin{multline*}\n\\mathbf{P}_x[X_t=y] = \\mathbf{P}_x[X_t=y, \\: T_1 > t] + \\mathbf{P}_x[X_{T_1}=y, \\: T_1 \\leq t] + O(t^2)\n\\\\ = \\delta_{x}(y) e^{-c(x)t} + \\left(1 - e^{-c(x)t}\\right)\\Pi(x,y) + O(t^2),\n\\end{multline*}\\]\nużywając niezależności \\(T_1\\) \\(X_{T_1}\\) oraz definicji\n\\(\\Pi(x, y)\\). Dochodzimy wniosku, że\nskoro \\(\\mathbf{P}_x[X_0=y] = \\delta_x(y)\\), \n\\[\\begin{equation*}\n\\frac{\\mathbf{P}_x[X_t=y]  -  \\mathbf{P}_x[X_0=y]}{t} \\-c(x)\\delta_{x}(y) + c(x)\\Pi(x,y).\n\\end{equation*}\\]\nco kończy dowód.Kolejne twierdzenie dostarcza pełnego opisu ścieżek procesu \\(X\\) pod \\(\\mathbf{P}_x\\).\nDla uproszczenia zakładamy, że nie ma stanów pochłaniających,\nale czytelnik łatwo rozszerzy stwierdzenie na przypadek ogólny.Twierdzenie 7  Zakładamy, że \\(c(y) > 0\\) dla każdego \\(y \\S\\) że \\(\\sup_{y \\S} c(y) < \\infty\\).\nNiech \\(x \\S\\).\nWówczas, \\(\\mathbf{P}_x\\) p.n.,\nczasy skoku \\(T_1 < T_2 < T_3 < \\dots\\) są skończone, \nciąg \\(X_0, X_{T_1}, X_{T_2}, \\dots\\) pod \\(\\mathbf{P}_x\\) jest dyskretnym łańcuchem Markowa\nz macierzą przejścia \\(\\Pi\\) rozpoczętym w \\(x\\).\nPonadto, pod warunkiem \\((X_0, X_{T_1}, X_{T_2}, \\dots)\\),\nzmienne losowe \\(T_1 - T_0, T_2 - T_1, \\dots\\) są niezależne,\ndla każdej liczby całkowitej \\(\\geq 0\\),\nrozkład warunkowy \\(T_{+1} - T_i\\) jest wykładniczy z parametrem \\(c(X_{T_i})\\).Proof. Zastosowanie silnej własności Markowa pokazuje,\nże wszystkie czasy stopu \\(T_1, T_2, \\dots\\) są skończone \\(\\mathbf{P}_x\\)-p.n.\nNastępnie, niech \\(y, z \\S\\), \\(f_1, f_2 \\colon S \\\\mathbb{R}\\).\nUżywając silnej własności Markowa w \\(T_1\\):\n\\[\\begin{multline*}\n\\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_1} = y\\}} f_1(T_1) 1_{\\{X_{T_2} = z\\}} f_2(T_2 - T_1)]\\\\\n= \\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_1} = y\\}} f_1(T_1) \\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_2} = z\\}} f_2(T_2 - T_1)]] \\\\\n= \\Pi(x, y) \\Pi(y, z) \\int_0^\\infty e^{-c(x)s_1} f_1(s_1)\\mathrm{d} s_1 \\int_0^\\infty  e^{-c(y)s_2} f_2(s_2) \\mathrm{d} s_2.\n\\end{multline*}\\]\nPostępując indukcyjnie, otrzymujemy dla każdych \\(y_1, \\dots, y_p \\S\\) oraz \\(f_1, \\dots, f_p \\colon S \\\\mathbb{R}\\):\n\\[\\begin{multline*}\n\\mathbf{E}_x[\\mathbf{1}_{\\{X_{T_1} = y_1\\}} \\mathbf{1}_{\\{X_{T_2} = y_2\\}} \\dots \\mathbf{1}_{\\{X_{T_p} = y_p\\}} f_1(T_1) f_2(T_2 - T_1) \\dots f_p(T_p - T_{p-1})] \\\\\n= \\Pi(x, y_1) \\Pi(y_1, y_2) \\dots \\Pi(y_{p-1}, y_p) \\prod_{=1}^p \\left( \\int_0^\\infty e^{-c(y_{-1})s} f_i(s) \\mathrm{d} s\\right),\n\\end{multline*}\\]\ngdzie \\(y_0 = x\\).Z powyższego twierdzenia wynika charakteryzacja łańcucha Markowa w terminach \\(Q\\)-macierzy.\nPrzez \\(\\mathbb{F}^X = (\\mathcal{F}_t^X)_t\\) oznaczać będziemy najmniejszą możliwą filtrację, tj.\n\\[\\begin{equation*}\n\\mathcal{F}_t = \\sigma(X_s \\: : \\: s \\leq t).\n\\end{equation*}\\]Wniosek 1  Niech \\(q = (q(x,y))_{x,y\\S}\\) będzie \\(Q\\)-macierzą taką, że \\(\\sup_{x \\S}|q(x,x)| < \\infty\\).\nWówczas istnieje jedyna rodzina miar\n\\(\\mathbf{P}\\) taka, że \\((\\mathbf{P}, \\mathbb{F}^X)\\) jest łańcuchem Markowa stowarzyszonym z \\(Q\\)-macierzą \\(q\\).","code":""},{"path":"section-wykład-2-mocna-własność-markowa.html","id":"section-symulacja-łańcucha-markowa-w-czasie-ciągłym","chapter":"Wykład 2: Mocna własność Markowa","heading":"Symulacja łańcucha Markowa w czasie ciągłym","text":"W tej sekcji przedstawimy, jak zaimplementować zwizualizować łańcuch Markowa w czasie ciągłym z 5 stanami.","code":"\n# Funkcja do symulacji łańcucha Markowa w czasie ciągłym\nsimulate_markov_chain <- function(Q, initial_state, time_horizon) {\n  states <- nrow(Q)\n  state <- initial_state\n  time <- 0\n  times <- c(0)\n  trajectory <- c(state)\n\n  while (time < time_horizon) {\n    rate <- -Q[state, state]  # Intensywność opuszczenia stanu\n    if (rate == 0) {\n      break\n    }\n    waiting_time <- rexp(1, rate)  # Czas do następnego zdarzenia\n    time <- time + waiting_time\n    if (time > time_horizon) {\n      break\n    }\n    times <- c(times, time)\n\n    # Prawdopodobieństwa przejść do innych stanów\n    probs <- Q[state, ]\n    probs[state] <- 0  # Usuwamy intensywność własną\n    probs <- probs / sum(probs)\n\n    state <- sample(1:states, 1, prob = probs)  # Nowy stan\n    trajectory <- c(trajectory, state)\n  }\n\n  return(data.frame(time = times, state = trajectory))\n}\n\n# Macierz przejść (generator Q)\nQ <- matrix(c(-2,  1,  1,  0,  0,\n               1, -3,  1,  1,  0,\n               1,  1, -3,  0,  1,\n               0,  1,  0, -2,  1,\n               0,  0,  1,  1, -2), byrow = TRUE, nrow = 5)\n\n# Symulacja łańcucha Markowa\nset.seed(42)  # Ustalenie ziarna dla powtarzalności wyników\ninitial_state <- 1\nczas_horyzont <- 10\ndata <- simulate_markov_chain(Q, initial_state, czas_horyzont)\n\n# Przygotowanie danych do wykresu kawałkami stałego\nprepare_data_for_step <- function(data) {\n  result <- data.frame(\n    x_start = head(data$time, -1),\n    x_end = tail(data$time, -1),\n    state = head(data$state, -1)\n  )\n  return(result)\n}\n\n# Przygotowanie danych\nstep_data <- prepare_data_for_step(data)\n\n# Tworzenie wykresu kawałkami stałego\nggplot(step_data, aes(x = x_start, y = state)) +\n  geom_segment(aes(xend = x_end, yend = state), size = 1) +\n  scale_y_continuous(breaks = 1:5) +\n  labs(\n    title = \"Symulacja łańcucha Markowa w czasie ciągłym\",\n    x = \"Czas\",\n    y = \"Stan\"\n  ) +\n  theme_minimal()"},{"path":"section-wykład-3-procesy-i-półgrupy-fellera.html","id":"section-wykład-3-procesy-i-półgrupy-fellera","chapter":"Wykład 3: procesy i półgrupy Fellera","heading":"Wykład 3: procesy i półgrupy Fellera","text":"2024-10-17Piotr DyszewskiW tym rozdziale \\(S\\) jest ośrodkową, lokalnie zwartą przestrzenią\nmetryczną, \\(C(S)\\) jest przestrzenią ciągłych funkcji rzeczywistych na \\(S\\).\nPrzez \\(C_0(S)\\) oznaczać będziemy klasę funkcji z \\(C(S)\\) znikających w nieskończoności. Dokładniej\n\\(C_0(S)\\) zbiór funkcji \\(f\\) z \\(C(S)\\) takich, że dla każdego dodatniego \\(\\epsilon\\) istnieje zwarty\n\\(K \\subseteq S\\) taki, że \\(|f(x)| \\leq \\epsilon\\) dla \\(x \\S \\setminus K\\).\nZauważmy, że jeżeli \\(S\\) jest zwarta, \\(C_0(S) = C(S)\\). Dodatkowo każda\n\\(f\\) z \\(C_0(S)\\) jest jednostajnie ciągła, tj. dla każdego dodatniego \\(\\epsilon\\) istnieje dodatnia \\(\\delta\\),\ntaka, że dla każdych \\(x , y\\S\\),\n\\[\\begin{equation*}\n    \\mathrm{d}(x,y) < \\delta \\quad \\Rightarrow \\quad |f(x)-f(y)| <\\epsilon.\n\\end{equation*}\\]\nTutaj \\(\\mathrm{d}\\) jest metryką na \\(S\\).\nW obu przestrzeniach \\(C(S)\\) \\(C_0(S)\\) używamy normy jednostajnej\n\\[\n    \\|f\\| = \\sup_{x \\S} |f(x)|,\n\\]\nco czyni \\(C_0(S)\\) przestrzenią Banacha.\nGłównym powodem stosowania ciągłych funkcji zanikających w\nnieskończoności zamiast ograniczonych ciągłych funkcji w przypadku lokalnie zwartym jest ,\nże jednostajna ciągłość jest wymagana w wielu argumentach.\nOgraniczone funkcje ciągłe nie są zwykle jednostajnie ciągłe,\npodczas gdy ciągłe funkcje zanikające na nieskończoności są.\nInnym powodem jest , że \\(C_0(S)\\) jest ośrodkowa,\nco nie jest ogólnie prawdziwe dla przestrzeni wszystkich ograniczonych ciągłych funkcji na \\(S\\).","code":""},{"path":"section-wykład-3-procesy-i-półgrupy-fellera.html","id":"section-proces","chapter":"Wykład 3: procesy i półgrupy Fellera","heading":"Proces","text":"Zaczynamy od opisu składników potrzebnych definicji głównego obiektu zainteresowania w tym rozdziale.\nKonstrukcja będzie analogiczna łańcuchów Markowa w czasie ciągłym.\nNiech \\(\\Omega = D[0, \\infty)\\) będzie zbiorem\nfunkcji prawostronnie ciągłych$ \\(\\omega : [0, \\infty) \\S\\) z lewymi granicami w każdym punkcie.\nTak jak poprzednio dla \\(s, t \\\\mathbb{R}_+\\) połóżmy też\n\\[\n    X_t(\\omega) = \\omega(t) \\text{ oraz } (\\theta_s \\omega)(t) = \\omega(t + s).\n\\]\nNiech \\(\\mathcal{F}\\) będzie najmniejszym \\(\\sigma\\)-ciałem podzbiorów \\(\\Omega\\) względem którego wszystkie \\(X_t\\) dla \\(t \\\\mathbb{R}_+\\) są mierzalne.Definicja 7  Procesem Fellera na \\(S\\) nazywamy parę uporządkowaną \\((\\mathbf{P}, \\mathbb{F})\\) taką, że(PF1) \\(\\mathbf{P}=\\{\\mathbf{P}_x\\}_{x \\S}\\), gdzie dla każdego \\(x \\S\\), \\(\\mathbf{P}_x\\) jest miarą probabilistyczną\nna \\((\\Omega, \\mathcal{F})\\) taką, że\n\\[\\begin{equation}\n\\mathbf{P}_x[X_0 = x] = \\mathbf{P}_x [\\omega \\: : \\: \\omega(0)=x] = 1.\n\\tag{5}\n\\end{equation}\\](PF1) \\(\\mathbf{P}=\\{\\mathbf{P}_x\\}_{x \\S}\\), gdzie dla każdego \\(x \\S\\), \\(\\mathbf{P}_x\\) jest miarą probabilistyczną\nna \\((\\Omega, \\mathcal{F})\\) taką, że\n\\[\\begin{equation}\n\\mathbf{P}_x[X_0 = x] = \\mathbf{P}_x [\\omega \\: : \\: \\omega(0)=x] = 1.\n\\tag{5}\n\\end{equation}\\](PF2) \\(\\mathbb{F}=\\{\\mathcal{F}_t\\}_{t \\\\mathbb{R}_+}\\) jest filtracją na \\(\\Omega\\),\nwzględem której zmienne losowe \\(X(t)\\) są adaptowane.(PF2) \\(\\mathbb{F}=\\{\\mathcal{F}_t\\}_{t \\\\mathbb{R}_+}\\) jest filtracją na \\(\\Omega\\),\nwzględem której zmienne losowe \\(X(t)\\) są adaptowane.(PF3) Odwzorowanie\n\\[\\begin{equation}\nx \\mapsto \\mathbf{E}_x \\left[f(X_t) \\right] \\text{ jest w } C_0(S)\n\\text{ dla wszystkich } f \\C_0(S) \\text{ } t \\geq 0.\n\\tag{6}\n\\end{equation}\\](PF3) Odwzorowanie\n\\[\\begin{equation}\nx \\mapsto \\mathbf{E}_x \\left[f(X_t) \\right] \\text{ jest w } C_0(S)\n\\text{ dla wszystkich } f \\C_0(S) \\text{ } t \\geq 0.\n\\tag{6}\n\\end{equation}\\](PF4) Spełniona jest własność Markowa\n\\[\\begin{equation}\n\\mathbf{E}_x\\left[Y \\circ \\theta_s \\mid \\mathcal{F}_s\\right] = \\mathbf{E}_{X(s)}\\left[Y\\right] \\quad\n\\mathbf{P}_x\\text{-prawie wszędzie }\n\\tag{7}\n\\end{equation}\\]\ndla wszystkich \\(x \\S\\) oraz wszystkich ograniczonych mierzalnych \\(Y\\) na \\(\\Omega\\).(PF4) Spełniona jest własność Markowa\n\\[\\begin{equation}\n\\mathbf{E}_x\\left[Y \\circ \\theta_s \\mid \\mathcal{F}_s\\right] = \\mathbf{E}_{X(s)}\\left[Y\\right] \\quad\n\\mathbf{P}_x\\text{-prawie wszędzie }\n\\tag{7}\n\\end{equation}\\]\ndla wszystkich \\(x \\S\\) oraz wszystkich ograniczonych mierzalnych \\(Y\\) na \\(\\Omega\\).Własność (6) znana jest jako własność Fellera.\nInnym sposobem przedstawienia części ciągłości, który wydaje się całkiem naturalny,\njest , że \\(x_n \\x\\) w \\(S\\) implikuje, że rozkład \\(X\\) dla procesu rozpoczynającego się w \\(x_n\\)\nzbiega się słabo tego dla procesu rozpoczynającego się w \\(x\\).\nWłasność Fellera (razem z prawostronną ciągłością trajektorii) implikuje silną własność Markowa.Twierdzenie 8  Każdy proces Fellera ma silną własność Markowa. Jeżeli \\((\\mathbf{P}, \\mathbb{F})\\) jest procesem Fellera,\ndla każdej ograniczonej zmiennej \\(Y \\colon \\Omega \\\\mathbb{R}\\) oraz \\(\\mathbb{F}\\)-czasu zatrzymania \\(\\tau\\)\nkażdego \\(x\\),\n\\[\n    \\mathbf{E}_x [Y \\circ \\theta_\\tau \\mid \\mathcal{F}_\\tau] =\n    \\mathbf{E}_{X(\\tau)} \\left[ Y \\right]\\quad \\text{prawie na pewno } \\mathbf{P}_x\n\\]\nna zdarzeniu \\(\\{\\tau < \\infty\\}\\).Zadanie 5  Niech \\((\\mathbf{P}, \\mathbb{F})\\) będzie procesem Fellera.\nPokaż, że odwzorowanie\n\\[\\begin{equation}\n    x \\mapsto \\mathbf{E}_x \\left[ \\prod_{j=1}^n f_j(X_{t_j}) \\right]\n    \\tag{8}  \n\\end{equation}\\]\njest ciągłe dla dowolnego \\(n\\), dowolnych \\(t_1, \\ldots, t_n \\\\mathbb{R}\\)\noraz dowolnych \\(f_1, \\ldots, f_n \\C_0(S)\\).Proof (Twierdzenia 8). Rozumowanie przebiega identycznie jak w przypadku łańcuchów Markowa w czasie ciągłym. W miejscu, w którym\nwymagana jest ciągłość odwzorowań \\(x \\mapsto \\mathbf{E}_x[Y]\\) należy powołać się na tezę Zadania 5.Przykład 3  Niech \\(B = (B_t)_{t \\\\mathbb{R}_+}\\) będzie standardowym ruchem Browna określonym na przestrzeni probabilistycznej\n\\((\\Sigma, \\mathcal{G}, \\mathbb{P})\\). Przypomnijmy, że oznacza , że\\(B_0=0\\) \\(\\mathbb{P}\\)-p.w.Dla dowolnych \\(t,s \\\\mathbb{R}_+\\), \\(t\\geq s\\) zmienna \\(B_t-B_s\\) ma rozkład normalny \\(\\mathcal{N}(0,t-s)\\) o średniej zero wariancji \\(t-s\\).Dla dowolnych \\(t,s \\\\mathbb{R}_+\\), \\(t\\geq s\\) zmienna \\(B_t-B_s\\) jest niezależna od sigma ciała\n\\(\\mathcal{G}_s^B = \\sigma(B_r \\: : \\: r \\leq s)\\).Odwzorowanie \\(t \\mapsto B_t\\) jest ciągłe.Pokażemy, że ruch Browna jest procesem Fellera w myśl przyjętej przez nas definicji.\nPołóżmy \\(\\mathcal{F}_t = \\sigma(X_s \\: : \\: s \\leq t)\\).\nNiech \\(S = \\mathbb{R}\\). Dla \\(x \\S\\) zdefiniujmy \\(\\mathbf{P}_x\\) jako rozkład ruchu\nBrowna (rozumianego jako funkcji \\(\\mathbb{R}_+ \\\\mathbb{R}\\))\nzapoczątkowanego w punkcie \\(x\\), dokładniej\ndla \\(\\\\mathcal{F}\\) niech \\(\\mathbf{P}_x[] = \\mathbb{P}[B+x \\]\\).\nTutaj przez \\(B+x\\) rozumiemy funkcję \\(t \\mapsto B_t+x\\).Spełniona jest własność (PF1), ponieważ\n\\[\\begin{equation*}\n    \\mathbf{P}_x[X_0=x] = \\mathbb{P}[B_0+x=x]=1.\n\\end{equation*}\\]\nWłasność (PF2) jest spełniona wprost z definicji filtracji \\(\\mathbb{F}\\). Aby uzasadnić własność Fellera (PF3) ustalmy \\(f \\C_0(S)\\).\nCiągłość\n\\[\\begin{equation*}\n    x \\mapsto \\mathbf{E}_x\\left[ f(X_t) \\right] = \\mathbb{E}[f(B_t+x)]\n\\end{equation*}\\]\nwynika z ciągłości \\(f\\) oraz twierdzenia o zbieżności ograniczonej. Aby uzasadnić, że powyższe odwzorowanie jest\nklasy \\(C_0(S)\\) należy pokazać, że\n\\[\\begin{equation*}\n    \\lim_{|x| \\\\infty} \\mathbf{E}_x[f(X_t)] =0.\n\\end{equation*}\\]\nWystarczy w tym celu rozważyć oszacowanie\n\\[\\begin{equation*}\n    \\left|\\mathbb{E}[f(B_t+x)]\\right| \\leq \\|f \\| \\mathbb{P}[|B_t| >|x|/2] + \\sup_{|y| >|x|/2}|f(y)|.\n\\end{equation*}\\]\nOba składniki po prawej stronie zbiegają zera, przy czy zbieżność tego drugiego wynika z \\(f \\C_0(\\mathbb{R})\\).\nWłasność Markowa uzasadniamy dokładnie w taki sam sposób, w jaki zrobiliśmy dla procesu\nPoissona w Przykładzie 1.Zadanie 6  Niech \\(S=\\mathbb{Z}\\). Pokaż, że łańcuch Markowa w czasie ciągłym\n\\((\\mathbf{P}, \\mathbb{F})\\) jest procesem Fellera wtedy tylko wtedy, gdy dla każdego\n\\(y \\S\\) każdego \\(t \\\\mathbb{R}_+\\),\n\\[\\begin{equation*}\n    \\lim_{|x| \\\\infty} \\mathbf{P}_x[X_t=y]=0.\n\\end{equation*}\\]","code":""},{"path":"section-wykład-3-procesy-i-półgrupy-fellera.html","id":"section-półgrupa","chapter":"Wykład 3: procesy i półgrupy Fellera","heading":"Półgrupa","text":"Chcemy teraz przedstawić odpowiednik funkcji przejścia na\nnieprzeliczalnej przestrzeni stanów. W naturalny sposób nasuwa się\nrozważenie rozkładów \\(\\mathbf{P}_x[X_t \\\\mathrm{d} y]\\).\nJednak na dłuższą metę język rozkładów jest nieporęczny.\nO wiele bardziej praktyczny jest język półgrup.\nAby umotywować następną definicję, rozważmy przeliczalną przestrzeń stanów \\(S_0\\)\noraz funkcję przejścia \\(p\\)\nna \\(S_0\\).\nFunkcję przejścia można zakodować w\nkategoriach rodziny operatorów\n\\[\\begin{equation}\nT_tf(x) = \\sum_{y\\S_0} p_t(x, y)f(y),\n\\tag{9}\n\\end{equation}\\]\ndla \\(f \\C_0(S_0)\\).\nJasne jest, że znając \\(T_t\\), więc znając\nwartości \\(T_tf\\) dla wszystkich \\(f \\C_0(S_0)\\),\nznamy też funkcję przejścia \\(p_t(x,y)\\).\nWykorzystując równiania Chapmana-Kołmogorowa dostajemy dla \\(s,t\\geq 0\\),\n\\[\\begin{multline*}\nT_{s+t} f(x) = \\sum_{y \\S_0} p_{t+s}(x,y) f(y)\n= \\sum_{y \\S_0} \\sum_{z \\S_0} p_t(x,z)p_s(z,y)f(y) \\\\\n\\sum_{z \\S_0} p_t(x,z)\\sum_{y\\S_0}p_s(z,y)f(y)\n=\\sum_{z \\S_0} p_t(x,z) (T_sf)(z) = T_t(T_s(f))(x).\n\\end{multline*}\\]\nWszystkie powyższe manipulacje są dozwolone\nponieważ \\(f \\C_0(S_0)\\) jest ograniczona.\nPowyższa tożsamość zapisuje się jako\n\\(T_t T_s = T_t\\circ T_s = T_{t+s}\\).\nOznacza , że \\((T_{t})_{t \\geq 0}\\) tworzą półgrupę.Definicja 7  Półgrupa Fellera rodzina ciągłych operatorów liniowych\n\\(T=\\{T_t\\}_{t \\\\mathbb{R}_+}\\) na \\(C_0(S)\\) spełniających następujące własności:\\(T_0f = f\\) dla wszystkich \\(f \\C_0(S)\\).Dla każdego \\(f \\C_0(S)\\), \\(\\lim_{t \\0} T_tf = f\\) w \\(C_0(S)\\).\\(T_{t+s}f = T_sT_tf\\) dla każdego \\(f \\C_0(S)\\).\\(T_tf \\geq 0\\) dla każdego nieujemnego \\(f \\C_0(S)\\).Istnieje rodzina \\(f_n \\C_0(S)\\), \\(n\\\\mathbb{N}\\) taka, że \\(\\sup_n \\|f_n\\| < \\infty\\),\noraz \\(T_tf_n\\) zbiega punktowo \\(1\\) dla każdego \\(t \\geq 0\\).Część c. analogia równań Chapmana-Kolmogorowa \nnazywana jest własnością półgrupy.\nJedną z jej konsekwencji jest , że \\(T(t)\\) \\(T(s)\\) komutują,\ntj. \\(T_tT_s=T_{t+s}=T_{s+t}=T_sT_t\\).\nZ części d. e. wynika, że \\(\\|T(t)f\\| \\leq \\|f\\|\\)\ndla wszystkich \\(f \\C_0(S)\\),\ntak więc każdy \\(\\|T\\|\\leq 1\\).\nWłasność b. jest znana jako mocna ciągłość.\nWraz z c. własnością kontrakcji, implikuje ,\nże funkcja\n\\(t \\mapsto T(t)f\\) z \\([0, \\infty)\\) \\(C_0(S)\\)\njest ciągła.Oto ważny przykład - półgrupa Gaussa–Weierstrassa.\nCzęść b. tego ćwiczenia ilustruje powody przyjmowania\nfunkcji w \\(C_0(S)\\) zanikających na nieskończoności.Zadanie 7  Niech \\(S = \\mathbb{R}\\) \\(B=(B_t)_{t \\\\mathbb{R}_+}\\) będzie ruchem Browna.Pokaż, że \\(T_t\\) zdefiniowane przez\n\\[\nT_tf(x) = \\mathbb{E} [f(B_t+x)]\n\\]\njest półgrupą Fellera.Wyjaśnij, dlaczego \\(T\\) nie jest mocno ciągła jako półgrupa operatorów na klasie \\(C_b(S)\\) ograniczonych\nfunkcji z \\(C(S)\\).Działanie półgrupy Gaussa-Weierstrassa na funkcji \\(f(x)\\), zdefiniowanej jako\n\\(\\sin(x)\\) na przedziale \\([-\\pi, \\pi]\\) \n\\(0\\) poza nim. Funkcja wynikowa\n\\(T_tf(x)\\) jest wyznaczana jako splot \\(f\\) z jądrem Gaussa dla zadanego parametru \\(t\\).W tym rozdziale konieczne będzie całkowanie funkcji ciągłych\nprzyjmujących wartości w \\(C(S)\\) względem \\(t\\).\nRachunek takich funkcji jest analogiczny rachunku funkcji rzeczywistych.\nW tym duchu wiążemy z półgrupą\njej transformata Laplace’\n\\[\\begin{equation}\nU(\\alpha)f = \\int_0^\\infty e^{-\\alpha t} T_tf \\, \\mathrm{d} t, \\quad \\alpha > 0,\n\\tag{10}\n\\end{equation}\\]\nktóra nazywana jest rezolwentą półgrupy.\nFunkcję \\(U(\\alpha)f\\) można interpretować jako całkę Bochnera pojawiającą się po prawej stronie\n(10).\nMożna też równoważnie myśleć, że jest funkcja \\(S \\\\mathbb{R}\\)\nzadana przez\n\\[\\begin{equation*}\nU(\\alpha)f(x) = \\int_0^\\infty e^{-\\alpha t} T_tf(x) \\, \\mathrm{d} t, \\quad x \\S.\n\\end{equation*}\\]\nW każdym razie całka w (10) jest dobrze określona,\nponieważ funkcja \\(t \\mapsto e^{-\\alpha t} T_tf\\) jest ciągła oraz\n\\[\n\\|e^{-\\alpha t} T_tf\\| \\leq e^{-\\alpha t} \\|f\\|.\n\\]\nZauważmy też, że \\(U(\\alpha)\\) jest operatorem liniowym na \\(C_0(S)\\) spełnia\n\\[\n\\|U(\\alpha)f\\| \\leq  \\|f\\|/\\alpha.\n\\]Zadanie 8  Pokaż, że dla każdego \\(f \\C_0(S)\\),\n\\[\n\\lim_{\\alpha \\\\infty} \\alpha U(\\alpha) f = f.\n\\]Własność półgrupy przekłada się na następującą użyteczną relację, znaną jako równanie rezolwenty:\n\\[\\begin{equation}\nU(\\alpha) - U(\\beta) = (\\beta - \\alpha) U(\\alpha) U(\\beta).\n\\tag{11}\n\\end{equation}\\]\nAby sprawdzić, weźmy \\(\\alpha \\neq \\beta\\) zapiszmy\n\\[\\begin{multline}\nU(\\alpha) U(\\beta) f\n= \\int_0^\\infty e^{-\\alpha t} T_t U(\\beta) f \\, \\mathrm{d} t\n= \\int_0^\\infty e^{-\\alpha t} \\left( \\int_0^\\infty e^{-\\beta s} T_t T_s f \\, \\mathrm{d} s \\right) \\mathrm{d} t\\\\\n= \\int_0^\\infty   \\int_0^r e^{-\\alpha t} e^{-\\beta (r-t)} \\, \\mathrm{d} t  T_r f\\mathrm{d} r\n= \\int_0^\\infty  \\frac{e^{-\\alpha r} - e^{-\\beta r}}{\\beta - \\alpha} T_r f \\mathrm{d} r.\n\\tag{12}\n\\end{multline}\\]Jedną z konsekwencji (11) jest , że \\(U(\\alpha)\\) \\(U(\\beta)\\) komutują.","code":""},{"path":"section-wykład-4-generatory.html","id":"section-wykład-4-generatory","chapter":"Wykład 4: Generatory","heading":"Wykład 4: Generatory","text":"2024-10-24Piotr DyszewskiDo tej pory te definicje powinny być dość intuicyjne.\nKolejna definicja może wydawać się mniej oczywista,\njednak okazuje się być odpowiednim odpowiednikiem definicji macierzy \\(Q\\).Aby nakreślić analogię, biorąc macierz \\(Q\\) na przeliczalnym zbiorze \\(S_0\\), niech\n\\(p\\) będzie funkcją przejścia zadaną jako\n\\[\\begin{equation*}\n    p_t = e^{tq} = \\sum_{k=0}^\\infty \\frac{t^k}{k!} q^k.\n\\end{equation*}\\]\nZ tą funkcją wiążemy półgrupę\n\\[\\begin{equation*}\n    T_t f(x) = \\sum_{y \\S_0} p_t(x,y) f(y),\n\\end{equation*}\\]\ntakże rezolwentę\n\\[\\begin{equation*}\n    U(\\alpha) f(x) = \\int_0^\\infty e^{-\\alpha t} T_t f(x) \\, \\mathrm{d}t\n    = \\int_0^\\infty e^{-\\alpha t} e^{tq} f(x) \\, \\mathrm{d}t\n    = \\int_0^\\infty e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t \\, f(x).\n\\end{equation*}\\]\nOstatnie przejście wynika z faktu, że mamy tutaj czynienia z mnożeniem\nwektora przez macierz. Zauważmy, że\n\\[\\begin{equation*}\n    (q - \\alpha ) \\int_0^\\infty e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t\n    = \\int_0^\\infty (q - \\alpha ) e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t\n    = \\int_0^\\infty \\frac{\\mathrm{d}}{\\mathrm{d}t} e^{-\\alpha t} e^{tq} \\, \\mathrm{d}t = -.\n\\end{equation*}\\]\nOznacza , że\n\\[\\begin{equation*}\n    U(\\alpha) f(x) = (\\alpha - q)^{-1} f(x).\n\\end{equation*}\\]\nZ własności rezolwenty wiemy, że\n\\[\\begin{equation*}\n    \\left\\| (- \\frac{1}{\\alpha} q)^{-1} \\right\\| = \\|\\alpha U(\\alpha)\\| \\leq 1.\n\\end{equation*}\\]\nOstatnia własność rezolwenty, z której tutaj skorzystaliśmy, wynika z kontraktywności\noperatorów w półgrupie \\(T\\) (\\(\\|T_t\\| \\leq 1\\)).Definicja 8  Generator infinitezymalny na \\(C_0(S)\\) para\nuporządkowana \\((L, \\mathcal{D}(L))\\) taka, że:(GI1) \\(\\mathcal{D}(L)\\) jest gęstą podprzestrzenią liniową \\(C_0(S)\\).(GI2) \\(L \\colon \\mathcal{D}(L) \\C_0(S)\\) jest operatorem liniowym.(GI3) Jeśli \\(f \\\\mathcal{D}(L)\\), \\(\\lambda \\geq 0\\), \\(f - \\lambda L f = g\\), \n\\[\n\\inf_{x \\S} f(x) \\geq \\inf_{x \\S} g(x).\n\\](GI4) \\(\\mathcal{R}(- \\lambda L) = C_0(S)\\) dla wszystkich dostatecznie małych \\(\\lambda > 0\\).(GI5) Dla dostatecznie małych dodatnich \\(\\lambda\\) istnieje\nciąg \\(f_n \\\\mathcal{D}(L)\\) (który może zależeć od \\(\\lambda\\)) taki, że \\(g_n = f_n - \\lambda L f_n\\) spełnia warunek\n\\(\\sup_n \\|g_n\\| < \\infty\\), zarówno \\(f_n\\), jak \\(g_n\\) zbiega punktowo \\(1\\).Zauważmy, że własność (GI3) ma następującą konsekwencję:\n\\[\\begin{equation}\n    f \\\\mathcal{D}(L), \\lambda \\geq 0, f - \\lambda L f = g \\implies \\|f\\| \\leq \\|g\\|.\n    \\tag{13}\n\\end{equation}\\]\nAby zobaczyć, napiszmy:\n\\[\n    \\inf_{x \\S} g(x) \\leq \\inf_{x \\S} f(x) \\leq\n    \\sup_{x \\S} f(x) \\leq \\sup_{x \\S} g(x),\n\\]\ngdzie ostatnia nierówność wynika z (GI3), gdy zastąpimy \\(f\\) \\(g\\) odpowiednio\nprzez \\(-f\\) \\(-g\\). Oznacza , że operator \\(- \\lambda L\\) jest różnowartościowy.\nRzeczywiście, dla \\(f - \\lambda L f = g = h - \\lambda L h\\), mamy \\(\\|f - h\\| \\leq \\|g - g\\| = 0\\).\nTak więc, dla dostatecznie małych dodatnich \\(\\lambda\\), \\((- \\lambda L)^{-1}\\)\njest dobrze określoną kontrakcją, która odwzorowuje funkcje nieujemne na funkcje nieujemne.Ponieważ Definicja 8 jest dość abstrakcyjna, pomocne może być rozważenie\nnastępującego przykładu, który okazuje się być generatorem procesu na prostej,\nporuszającego się w prawo z jednostkową prędkością.\nZauważmy, że najtrudniejszą własnością sprawdzenia jest (GI4). Zazwyczaj tak bywa.Zadanie 9  Przypuśćmy, że \\(S = \\mathbb{R}\\),\n\\[\n    \\mathcal{D}(L) = \\{f \\C_0(\\mathbb{R}) : f' \\C_0(\\mathbb{R})\\},\n\\]\noraz \\(L f = f'\\).\nPokaż, że para \\((L, \\mathcal{D}(L))\\) jest generatorem infinitezymalnym.","code":""},{"path":"section-wykład-4-generatory.html","id":"section-od-procesu-do-półgrupy-i-generatora","chapter":"Wykład 4: Generatory","heading":"Od procesu do półgrupy i generatora","text":"Oto pierwszy krok w przejściu od procesu Fellera jego generatora.Twierdzenie 9  Niech dany będzie proces Fellera \\((\\mathbf{P}, \\mathbb{F})\\).\nDla \\(t \\geq 0\\) zdefiniujmy\n\\[\\begin{equation}\nT_t f(x) = \\mathbf{E}_x [f(X(t))]\n\\tag{14}\n\\end{equation}\\]\ndla \\(f \\C_0(S)\\).\nWtedy \\(T = (T_t)_{t \\geq 0}\\) jest półgrupą Fellera.Proof. Własności ., d. e. z Definicji 7 są natychmiastowe.\nWłasność półgrupy c. wynika z własności Markowa:\n\\[\n    T_{s + t} f(x) =\n    \\mathbf{E}_x f(X(s + t)) =\n    \\mathbf{E}_x [ \\mathbf{E}_{X(s)} f(X(t)) | \\mathcal{F}_s]\n    = \\mathbf{E}_x [T_t f(X(s))]\n    = T_s T_t f(x).\n\\]\nZbieżność punktowa w b. wynika z ciągłości ścieżek ciągłości \\(f\\).\nAby sprawdzić wymaganą jednostajność w tej zbieżności, użyjemy rezolwenty\n\\[\\begin{equation*}\n    U(\\alpha) f(x) = \\int_0^\\infty e^{-\\alpha t} T_t f(x) \\, \\mathrm{d}t\n    = \\mathbf{E}_x \\left[ \\int_0^\\infty e^{-\\alpha t} f(X_t) \\, \\mathrm{d}t \\right].\n\\end{equation*}\\]\nW dowodzie równania rezolwenty (11), użyliśmy wspomnianej jednostajności,\nponieważ całki były interpretowane jako całki funkcji o wartościach w \\(C_0(S)\\).\nJednak te obliczenia stosują się równania rezolwenty bez tej jednostajności,\njeśli całki są interpretowane jako zwykłe całki dla ustalonego \\(x\\).\nAby uzasadnić zamianę kolejności całkowania, zauważmy,\nże \\(T_t f(x)\\) jest jednostajnie ograniczone, prawostronnie ciągłe w \\(t\\) dla każdego \\(x\\),\ntakże ciągłe w \\(x\\) dla każdego \\(t\\), zatem jest wspólnie mierzalne względem \\(x\\) \\(t\\).Zbiór\n\\(\\mathcal{L} = \\mathcal{R}(U(\\alpha))\\) jest niezależny od \\(\\alpha\\). Z równania\nrezolwenty mamy bowiem\n\\[\\begin{equation*}\n    U(\\alpha)f = U(\\beta) \\left(f + (\\beta - \\alpha) U(\\beta) f \\right).\n\\end{equation*}\\]\nJeśli \\(f = U(\\alpha)g \\\\mathcal{L}\\), \n\\[\n    T_t f = \\int_0^\\infty e^{-\\alpha s} T_{s + t} g \\, \\mathrm{d}s\n    = \\int_t^\\infty e^{-\\alpha(r - t)} T_r g \\, \\mathrm{d}r,\n\\]\nco zbiega jednostajnie \\(f\\) gdy \\(t \\downarrow 0\\).\nPonieważ każde \\(T_t\\) jest kontrakcją, mamy \\(\\|T_t f - f\\| \\0\\)\ndla wszystkich \\(f\\) w mocnym domknięciu \\({\\rm cl}(\\mathcal{L})\\).\nPokażemy teraz, że wspomniane mocne domknięcie jest równe słabemu domknięciu:\n\\[\\begin{equation*}\n    {\\rm wcl}(\\mathcal{L}) = \\left\\{ f \\C_0(S) \\: : \\:\n    \\exists \\{f_n\\}_n \\subseteq \\mathcal{L}, \\:\n    x(f_n) \\x(f), \\: \\forall x \\C_0(S)^* \\right\\}.\n\\end{equation*}\\]\nSkoro zbieżność w normie implikuje zbieżność słabą, \n\\({\\rm cl}(\\mathcal{L}) \\subseteq {\\rm wcl}(\\mathcal{L})\\). Weźmy\n\\(f \\notin {\\rm cl}(\\mathcal{L})\\). Ponieważ \\(\\mathcal{L}\\) jest wypukły, z twierdzenia\nHahna-Banacha istnieje funkcjonał liniowy \\(\\mu\\)\noddzielający \\(f\\) od \\({\\rm cl}(\\mathcal{L})\\), taki że\n\\[\\begin{equation*}\n    \\mu(f) < \\inf_{g \\{\\rm cl}(\\mathcal{L})} \\mu(g).\n\\end{equation*}\\]\nFunkcjonał ten dowodzi, że \\(f \\notin {\\rm wcl}(\\mathcal{L})\\).Pozostaje pokazać, że słabe domknięcie \\(\\mathcal{L}\\) jest równe całemu \\(C_0(S)\\).\nJest tak, ponieważ \\(\\alpha U(\\alpha) f\\) zbiega punktowo \\(f\\) gdy \\(\\alpha \\\\infty\\)\ndla każdego \\(f \\C_0(S)\\).Następnie zobaczymy, jak przejść od półgrupy generatora.Twierdzenie 10  Przypuśćmy, że \\((T_t)_{t \\geq 0}\\) jest półgrupą Fellera. Zdefiniujmy\n\\[\\begin{equation}\n    Lf = \\lim_{t \\downarrow 0} \\frac{T(t)f - f}{t}\n    \\tag{15}\n\\end{equation}\\]\ndla \\(f\\) z\n\\[\n\\mathcal{D}(L) = \\{f \\C(S) : \\text{przy $t \\0$ granica }  (T_tf-f)/t \\text{ istnieje}\\}.\n\\]\nWtedy para \\((L, \\mathcal(L))\\) jest generatorem infinitezymalnym.\nPonadto:Dla dowolnego \\(g \\C_0(S)\\) oraz \\(\\alpha > 0\\),\n\\[\\begin{equation}\nf = \\alpha U(\\alpha)g \\text{ wtedy tylko wtedy, gdy } f \\\\mathcal{D}(L) \\text{ spełnia } f - \\alpha^{-1} Lf = g.\n\\tag{16}\n\\end{equation}\\]Dla dowolnego \\(g \\C_0(S)\\) oraz \\(\\alpha > 0\\),\n\\[\\begin{equation}\nf = \\alpha U(\\alpha)g \\text{ wtedy tylko wtedy, gdy } f \\\\mathcal{D}(L) \\text{ spełnia } f - \\alpha^{-1} Lf = g.\n\\tag{16}\n\\end{equation}\\]Jeśli \\(f \\\\mathcal{D}(L)\\), \\(T_t f \\\\mathcal{D}(L)\\)\ndla wszystkich \\(t \\geq 0\\), jest funkcją ciągłą, różniczkowalną względem \\(t\\), spełnia\n\\[\\begin{equation}\n\\frac{\\mathrm{d}}{\\mathrm{d}t} T_t f = T_t Lf = L T_t f.\n\\tag{17}\n\\end{equation}\\]Jeśli \\(f \\\\mathcal{D}(L)\\), \\(T_t f \\\\mathcal{D}(L)\\)\ndla wszystkich \\(t \\geq 0\\), jest funkcją ciągłą, różniczkowalną względem \\(t\\), spełnia\n\\[\\begin{equation}\n\\frac{\\mathrm{d}}{\\mathrm{d}t} T_t f = T_t Lf = L T_t f.\n\\tag{17}\n\\end{equation}\\]Pochodna półgrupy Gaussa-Weierstrassa na funkcji \\(f(x)\\), zdefiniowanej jako\n\\(\\sin(x)\\) na przedziale \\([-\\pi, \\pi]\\) \n\\(0\\) poza nim. Funkcja wynikowa \n\\((T_tf(x)- f(x))/t\\) dla zadanego parametru \\(t\\).Proof. Przypuśćmy, że \\(f = \\alpha U(\\alpha)g\\) dla pewnego \\(\\alpha > 0\\) oraz \\(g \\C_0(S)\\).\nKorzystając z własności półgrupy zmieniając zmienne\njak w dowodzie Twierdzenia 9, mamy:\n\\[\\begin{align*}\n    \\frac{T_t f - f}{t} &= \\alpha \\frac{e^{\\alpha t} - 1}{t}\n    \\int_t^{\\infty} e^{-\\alpha s} T_s g \\, \\mathrm{d}s\n    - \\alpha \\frac{1}{t} \\int_0^t e^{-\\alpha s} T_s g \\, \\mathrm{d}s \\\\\n    &\\\\alpha^2 U(\\alpha) g - \\alpha g\n    = \\alpha f - \\alpha g,\n\\end{align*}\\]\ngdy \\(t \\downarrow 0\\).\nPrzy przejściu granicy skorzystaliśmy z własności b. z Definicji 7.\ndowodzi jednej implikacji w (16),\njak również (GI4) w Definicji 8.Ponieważ \\(\\alpha U(\\alpha)g \\\\mathcal{D}(L)\\) \\(\\alpha U(\\alpha)g \\g\\)\ngdy \\(\\alpha \\\\infty\\), zbiór \\(\\mathcal{D}(L)\\) jest gęsty w \\(C_0(S)\\).\nuzasadnia (GI1).Dla \\(t > 0\\) oraz \\(f \\\\mathcal{D}(L)\\) zdefiniujmy\n\\[\\begin{equation*}\n    g_t = \\left(1 + \\frac{\\lambda}{t}\\right) f - \\frac{\\lambda}{t} T(t)f = f - \\frac{\\lambda}{t}(T(t)f - f).\n\\end{equation*}\\]\nWtedy \\(\\lim_{t \\downarrow 0} g_t = f - \\lambda \\mathcal{L}f\\) \n\\[\n    (1 + \\lambda/t)\\inf_{x \\S} f(x) \\geq \\frac{\\lambda}{t} \\inf_{x \\S} T(t) f(x) + \\inf_{x \\S} g_t(x)\n    \\geq \\frac{\\lambda}{t} \\inf_{x \\S} f(x) + \\inf_{x \\S} g_t(x),\n\\]\nwięc własność (GI3) z Definicji 8 jest spełniona.\nDrugą nierówność pozostawiamy jako zadanie.Teraz przypuśćmy, że \\(f - \\alpha^{-1} L f = g\\) dla pewnego \\(f \\\\mathcal{D}(L)\\) oraz \\(\\alpha > 0\\).\nPrzez dowiedzioną już implikację w (16), \\(h = \\alpha U(\\alpha) g\\) spełnia\n\\(h - \\alpha^{-1} L h = g\\), więc \\(f = h\\) z (13).\nAby sprawdzić własność (GI5) z Definicji 8,\nprzypuśćmy, że \\(g_n \\C(S)\\) spełniają\n\\(\\sup_n \\|g_n\\| < \\infty\\), oraz że \\(g_n\\) \\(T(t) g_n\\) są zbieżne punktowo \\(1\\) dla każdego \\(t\\).\nZdefiniujmy \\(f_n \\\\mathcal{D}(L)\\) przez\n\\(g_n = f_n - \\lambda L f_n\\).\nWtedy \\(f_n = \\alpha U(\\alpha) g_n\\) z (16).\nPonieważ \\(T(t) g_n \\1\\) punktowo, \n\\(f_n \\1\\) punktowo przez definicję \\(U(\\alpha)\\)\noraz twierdzenie o zbieżności ograniczonej.Aby udowodnić punkt (b) twierdzenia, zauważmy, że\n\\[\n    \\frac{\\mathrm{d}}{\\mathrm{d}t} T(t) f = \\lim_{s \\0} \\frac{T(t+s)f - T(t)f}{s}\n    = \\lim_{s \\0} T(t) \\left( \\frac{T(s)f - f}{s} \\right)\n    = T(t) \\mathcal{L} f = \\mathcal{L} T(t) f,\n\\]\npod warunkiem, że którakolwiek z granic istnieje,\nponieważ wyrażenia w środku granic są identyczne.\nŚrodkowa granica rzeczywiście istnieje, ponieważ \\(f \\\\mathcal{D}(L)\\)\noraz \\(T(t)\\) jest kontrakcją.\nW związku z tym pozostałe granice również istnieją.Skoro istnieje trzecia granica, \\(T(t)f \\\\mathcal{D}(L)\\)\noraz (17) zachodzi.\nŚrodkowe wyrażenie w (17) jest ciągłe względem \\(t\\),\nwięc \\(T(t)f\\) jest ciągłe różniczkowalne.","code":""},{"path":"section-wykład-5-od-generatora-do-półgrupy.html","id":"section-wykład-5-od-generatora-do-półgrupy","chapter":"Wykład 5: od generatora do półgrupy","heading":"Wykład 5: od generatora do półgrupy","text":"2024-10-31Piotr Dyszewski","code":""},{"path":"section-wykład-5-od-generatora-do-półgrupy.html","id":"section-o-notacji-słów-kilka","chapter":"Wykład 5: od generatora do półgrupy","heading":"O notacji słów kilka","text":"Twierdzenie 11  Przy oznaczeniach Twierdzenia 10, dla \\(f \\C_0(S)\\) oraz \\(t > 0\\),\n\\[\n\\lim_{n \\\\infty} \\left(- \\frac{t}{n} L\\right)^{-n} f = T_tf.\n\\]Proof. Sprawdzamy indukcyjnie, że\n\\[\n\\left(- \\alpha^{-1} L\\right)^{-n} f\n= \\alpha^n U^n(\\alpha)f\n= \\int_0^{\\infty} \\alpha^n s^{n-1} e^{-\\alpha s} T(s)f \\, \\mathrm{d}s.\n\\]\nStąd\n\\[\\begin{equation}\n\\left( - \\frac{t}{n} L \\right)^{-n} f\n= \\mathbb{E} T \\left( (\\xi_1 + \\cdots + \\xi_n) t/n \\right) f,\n\\tag{18}\n\\end{equation}\\]\ngdzie \\(\\xi_1, \\xi_2, \\ldots\\) są niezależnymi zmiennymi\no standardowym rozkładzie wykładniczym.\nZauważmy, że funkcja \\((s, x) \\T(s)f(x)\\) jest ciągła,\nwięc wartość oczekiwana w (18) jest dobrze określona.Jeśli \\(f \\\\mathcal{D}(L)\\), \n\\[\\begin{equation*}\n    \\frac{\\mathrm{d}}{\\mathrm{d}t} T_t f = T_t Lf\n\\end{equation*}\\]\nco w notacji całkowej zapisuje się jako\n\\[\\begin{equation*}\n    T_tf-T_sf = \\int_s^t T_r Lf \\mathrm{d}r.\n\\end{equation*}\\]\nSkoro \\(\\|T_r \\| \\leq 1\\), ostatnia nierówność implikuje, że\n\\[\n\\| T(t)f - T(s)f \\| \\leq \\| Lf \\| |t - s|.\n\\]\nWracając teraz (18) otrzymujemy\n\\[\n\\left\\| \\left( - \\frac{t}{n} L \\right)^{-n} f - T(t)f \\right\\|\n\\leq t \\| Lf \\| \\mathbb{E} \\left| \\frac{\\xi_1 + \\cdots + \\xi_n}{n} - 1 \\right|.\n\\]\nRezultat dla \\(f \\\\mathcal{D}(L)\\) wynika teraz z prawa wielkich liczb.\nJest prawdziwy dla wszystkich \\(f \\C_0(S)\\),\nponieważ wszystkie rozważane operatory są kontrakcjami.Remark. Formalnie, \\(T_t = \\exp(tL)\\).\nKiedy \\(L\\) jest ograniczone, istnieją przynajmniej trzy sposoby definiowania tego wykładnika:\n\\[\n\\sum_{k=0}^\\infty \\frac{(tL)^k}{k!}, \\quad\n\\lim_{n \\\\infty} \\left(+ \\frac{t}{n} L\\right)^n,\n\\quad \\text{ } \\quad \\lim_{n \\\\infty} \\left(- \\frac{t}{n} L\\right)^{-n}.\n\\]\nOstatni z nich jest jedynym, który ma sens w przypadku nieograniczonym.Teraz rozważmy kilka przykładów.Zadanie 10  Rozważmy proces Fellera polegający na jednostajnym ruchu w prawo.\nNiech \\(S=\\mathbb{R}\\) niech \\(\\mathbf{P}_x\\) jest punktową\nmasą na ścieżce \\(\\omega_x\\) danej przez \\(\\omega_x(t) = x + t\\),\nlub równoważnie, proces z półgrupą\n\\(T(t)f(x) = f(x + t)\\).\nPokaż, że generatorem \\(L\\) tego procesu jest\nten opisany w Zadaniu~\\(\\ref{zad:3:13}\\).\nUpewnij się, że dziedzina dana jest dokładnie przez dziedzinę \\(L\\).Przykład 4  W przypadku ruchu Browna, można zweryfikować, że\n\\[\nU(\\lambda) f(x) = \\int u_\\lambda(y - x) f(y) \\, dy\n\\]\ngdzie\n\\[\nu_\\lambda(y - x) =\n\\int_0^\\infty (2 \\pi t)^{-1/2} \\exp\\left(-\\frac{|y - x|^2}{2t} - \\lambda t\\right) \\, \\mathrm{d}t\n= \\frac{1}{\\sqrt{2 \\lambda}} \\exp(-|y - x| \\sqrt{2 \\lambda}).\n\\]\nElegancki sposób na uzyskanie tej ostatniej równości polega na użyciu wzoru\n\\(\\mathbb{E}[e^{-\\lambda T_a}] = e^{-\\sqrt{2 \\lambda}}\\) dla transformaty Laplace’\nczasu trafienia \\(> 0\\) przez rzeczywisty ruch Browna rozpoczęty z \\(0\\).\nRóżniczkując względem \\(\\lambda\\),\notrzymujemy\n\\[\n\\mathbb{E}\\left[T_a e^{-\\lambda T_a}\\right] = \\frac{}{\\sqrt{2 \\lambda}} e^{-\\sqrt{2 \\lambda}},\n\\]\nużywając gęstości \\(T_a\\), aby przekształcić \\(\\mathbb{E}[T_a e^{-\\lambda T_a}]\\),\ndokładnie znajdujemy całkę, która pojawia się w obliczeniach \\(u_\\lambda(y - x)\\).Wiemy, że półgrupa operatorów związana z ruchem Browna jest półgrupą Fellera.\nZnajdziemy jej generator \\(L\\).\nWidzieliśmy, że dla każdego \\(\\lambda > 0\\) \\(f \\C_0(\\mathbb{R})\\),\n\\[\nU(\\lambda) f(x) =\n\\int \\frac{1}{\\sqrt{2 \\lambda}} \\exp(-\\sqrt{2 \\lambda} |y - x|) f(y) \\, \\mathrm{d}y.\n\\]\nJeśli \\(h \\\\mathcal{D}(L)\\),\nwiemy, że istnieje \\(f \\C_0(\\mathbb{R})\\) takie,\nże \\(h = U(\\lambda) f\\).\nPrzyjmując \\(\\lambda = 1/2\\), mamy\n\\[\nh(x) = \\int \\exp(-|y - x|) f(y) \\, \\mathrm{d}y.\n\\]\nRóżniczkując pod znakiem całki (pozostawiamy uzasadnienie jako zadanie),\notrzymujemy, że \\(h\\) jest różniczkowalna na \\(\\mathbb{R}\\), \n\\[\nh'(x) = \\int \\operatorname{sgn}(y - x) \\exp(-|y - x|) f(y) \\, \\mathrm{d}y\n\\]\ngdzie \\(\\operatorname{sgn}(z) = 1_{\\{z > 0\\}} - 1_{\\{z < 0\\}}\\)\n(wartość \\(\\operatorname{sgn}(0)\\) jest nieistotna).\nPokażmy również, że \\(h'\\) jest różniczkowalna na \\(\\mathbb{R}\\).\nNiech \\(x_0 \\\\mathbb{R}\\).\nNastępnie, dla \\(x > x_0\\),\n\\[\\begin{align*}\nh'(x) - h'(x_0)  = &\n\\int_{\\mathbb{R}} \\left( \\operatorname{sgn}(y - x) \\exp(-|y - x|) -\n\\operatorname{sgn}(y - x_0) \\exp(-|y - x_0|) \\right) f(y) \\, \\mathrm{d}y \\\\\n=&  \\int_{x_0}^x \\left( -\\exp(-|y - x|) - \\exp(-|y - x_0|) \\right) f(y) \\, \\mathrm{d}y \\\\\n&+ \\int_{\\mathbb{R} \\setminus [x_0, x]} \\operatorname{sgn}(y - x_0)\n\\left( \\exp(-|y - x|) - \\exp(-|y - x_0|) \\right) f(y) \\, \\mathrm{d}y.\n\\end{align*}\\]\nWynika stąd, że\n\\[\n\\frac{h'(x) - h'(x_0)}{x - x_0}\n\\xrightarrow{x \\downarrow x_0} -2f(x_0) + h(x_0).\n\\]\nOtrzymujemy tę samą granicę, gdy \\(x \\uparrow x_0\\),\nstąd uzyskujemy, że \\(h\\) jest dwukrotnie różniczkowalna oraz \\(h'' = -2f + h\\).Z drugiej strony, ponieważ \\(h = U(1/2)f\\),\n\\[\n\\left(\\frac{1}{2} - L\\right) h = f\n\\]\nstąd \\(Lh = -f + h/2 = h''/2\\).\nPodsumowując, uzyskaliśmy, że\n\\[\n\\mathcal{D}(L) \\subset \\{ h \\C^2(\\mathbb{R}) : h \\text{ } h'' \\C_0(\\mathbb{R}) \\}\n\\]\nże, jeśli \\(h \\\\mathcal{D}(L)\\), mamy \\(Lh = h''/2\\).Ostatnie zawieranie jest w rzeczywistości równością.\nAby zobaczyć, możemy postąpić następująco.\nJeśli \\(g\\) jest funkcją dwukrotnie różniczkowalną taką, że \\(g\\) oraz \\(g''\\) należą \\(C_0(\\mathbb{R})\\),\nwtedy przyjmujemy \\(f = \\frac{1}{2}(g - g'') \\C_0(\\mathbb{R})\\),\nwięc \\(h = U(1/2)f \\\\mathcal{D}(L)\\).\nZ poprzedniego argumentu wynika,\nże \\(h\\) jest dwukrotnie różniczkowalna oraz \\(h'' = -2f + h\\).\nStąd \\((h - g)'' = h - g\\).\nPonieważ funkcja \\(h - g\\) należy \\(C_0(\\mathbb{R})\\),\nmusi zanikać tożsamościowo, co daje \\(g = h \\\\mathcal{D}(L)\\).Patrząc na Zadanie 9 Przykład 4,\nmożna się zastanawiać, czy pochodne wyższego rzędu mogą\nbyć generatorami infinitezymalnymi. Odpowiedź brzmi, że nie mogą.\nGłówny problem polega na tym,\nże gdy gładka funkcja osiąga minimum w punkcie wewnętrznym swojej dziedziny,\npierwsza pochodna jest zerowa,\ndruga pochodna jest tam nieujemna.\nNic nie można powiedzieć o znakach innych pochodnych w tym miejscu.Zadanie 11  Pokaż, że nie istnieje generator prawdopodobieństwa,\nktórego ograniczenie gładkich funkcji jest dane przez \\(Lf = f'''\\).","code":""},{"path":"section-wykład-5-od-generatora-do-półgrupy.html","id":"section-od-generatora-do-procesu","chapter":"Wykład 5: od generatora do półgrupy","heading":"Od generatora do procesu","text":"W całym tym rozdziale \\(L\\) będzie generatorem infinitezymalnym.\nNaszym pierwszym zadaniem jest skonstruowanie odpowiadającej półgrupy prawdopodobieństwa.\nAby zrobić, wprowadzamy aproksymację \\(L_\\epsilon\\) \\(L\\) dla małego dodatniego \\(\\epsilon\\) przez\n\\[\nL_\\epsilon = L(- \\epsilon L)^{-1}.\n\\]\nZauważmy, że jest dobrze zdefiniowane z definicji generatora infinitezymalnego,\nponieważ \\(\\mathcal{R}(- \\epsilon L) = \\mathcal{D}(L)\\). łatwo zobaczyć z\n\\[\nf - \\epsilon L f = g \\quad\n\\text{jest równoważne} \\quad\nf = (- \\epsilon L)^{-1} g.\n\\]\nPonadto\n\\[\n\\|L_\\epsilon g\\|\n= \\|L f\\| \\leq \\frac{\\|f\\| + \\|g\\|}{\\epsilon}\n\\leq \\frac{2}{\\epsilon} \\|g\\|,\n\\]\nwięc \\(L_\\epsilon\\) jest ograniczonym operatorem.\npozwala zdefiniować \\(T_\\epsilon(t)\\) przez\n\\[\nT_\\epsilon(t)\n= e^{tL_\\epsilon}\n= \\sum_{n=0}^{\\infty} \\frac{t^n L_\\epsilon^n}{n!}.\n\\]Zadanie 12  () Pokaż, że dla każdego \\(f \\C_0(S)\\),\n\\[\\begin{equation}\n(- \\epsilon L)^{-1} f - \\epsilon L f = f.\n\\tag{19}\n\\end{equation}\\](b) Użyj części (), aby pokazać, że \\(L_\\epsilon\\) jest generatorem infinitezymalnym\noraz że \\(T_\\epsilon(t)\\) jest półgrupą prawdopodobieństwa,\nktórej generatorem jest \\(L_\\epsilon\\) w sensie Twierdzenia 10.Twierdzenie 12  Dla \\(f \\C_0(S)\\),\n\\[\nT(t)f = \\lim_{\\epsilon \\0} T_\\epsilon(t) f\n\\]\njednostajnie na ograniczonych przedziałach \\(t\\).\nDefiniuje półgrupę Fellera, której generatorem jest \\(L\\) w sensie Twierdzenia 10.Najpierw sprawdzamy, że \\(L_\\epsilon\\) \\(L_\\delta\\) komutują dla \\(\\epsilon, \\delta > 0\\).\nWynika z @ref{eq:3-19} oraz\n\\[\n(- \\epsilon L)^{-1} (- \\delta L)^{-1}\n= (- \\delta L)^{-1} (- \\epsilon L)^{-1},\n\\]\nco jest prawdziwe, ponieważ\n\\[\n(- \\epsilon L)^{-1} (- \\delta L)^{-1} f\n= g\n\\quad \\text{jest równoważne}\n\\quad f\n= g - (\\epsilon + \\delta)Lg + \\epsilon \\delta L^2 g,\n\\]\nco jest symetryczne w \\(\\epsilon\\) \\(\\delta\\).","code":""},{"path":"section-wykład-6-od-generatora-przez-półgrupę-do-procesu.html","id":"section-wykład-6-od-generatora-przez-półgrupę-do-procesu","chapter":"Wykład 6: od generatora przez półgrupę do procesu","heading":"Wykład 6: od generatora przez półgrupę do procesu","text":"2024-11-07Piotr DyszewskiW całym tym rozdziale \\((L, \\mathcal{D}(L))\\) będzie generatorem\ninfinitezymalnym. Naszym pierwszym zadaniem jest skonstruowanie\nodpowiadającej mu półgrupy Fellera. Aby zrobić, wprowadzamy\naproksymację \\(L_\\epsilon\\) zadaną przez\n\\[L_\\epsilon = L(- \\epsilon L)^{-1}.\\] Zauważmy, że \\(L_\\epsilon\\) jest\ndobrze określone ponieważ\n\\[f \\\\mathcal{D}(L), \\: f - \\epsilon L f = g \\quad\n        \\text{jest równoważne} \\quad\n        f = (- \\epsilon L)^{-1} g.\\] Ponadto, przy oznaczeniach jak\nwyżej, \\[\\|L_\\epsilon g\\|\n        = \\|L f\\| \\leq \\frac{\\|f\\| + \\|g\\|}{\\epsilon}\n        \\leq \\frac{2}{\\epsilon} \\|g\\|\\],\nwięc \\(L_\\epsilon\\) jest ograniczonym operatorem. pozwala zdefiniować\n\\(T_\\epsilon(t)\\) przez \\[T_\\epsilon(t)\n        = e^{tL_\\epsilon}\n        = \\sum_{n=0}^{\\infty} \\frac{t^n L_\\epsilon^n}{n!}.\\]Zadanie 13  Pokaż, że dla każdego \\(f \\C_0(S)\\),\n\\[\\begin{equation}\n                    (- \\epsilon L)^{-1} f - \\epsilon L_\\epsilon f\n                    = f.\n\\tag{19}\n\\end{equation}\\]Pokaż, że dla każdego \\(f \\C_0(S)\\),\n\\[\\begin{equation}\n                    (- \\epsilon L)^{-1} f - \\epsilon L_\\epsilon f\n                    = f.\n\\tag{19}\n\\end{equation}\\]Użyj części (), aby pokazać, że \\(L_\\epsilon\\) jest generatorem\ninfinitezymalny,, oraz że \\(T_\\epsilon(t)\\) jest półgrupą Fellera,\nktórej generatorem jest \\(L_\\epsilon\\).Użyj części (), aby pokazać, że \\(L_\\epsilon\\) jest generatorem\ninfinitezymalny,, oraz że \\(T_\\epsilon(t)\\) jest półgrupą Fellera,\nktórej generatorem jest \\(L_\\epsilon\\).Twierdzenie 12  Dla \\(f \\C_0(S)\\),\n\\[\n            T(t)f\n            = \\lim_{\\epsilon \\0} T_\\epsilon(t) f\n\\tag{20}\n\\] jednostajnie na\nograniczonych przedziałach \\(t\\). Definiuje półgrupę Fellera, której\ngeneratorem jest \\(L\\).Proof. Najpierw sprawdzamy, że \\(L_\\epsilon\\) \\(L_\\delta\\) komutują dla\n\\(\\epsilon, \\delta > 0\\). Aby sprawdzić zauważmy najpierw, że\n\\[(- \\epsilon L)^{-1} (- \\delta L)^{-1}\n            = (- \\delta L)^{-1} (- \\epsilon L)^{-1},\\] co z kolei\njest prawdziwe, ponieważ \\[(- \\epsilon L)^{-1} (- \\delta L)^{-1} f\n            = g\n            \\quad \\text{jest równoważne}\n            \\quad f\n            = g - (\\epsilon + \\delta)Lg + \\epsilon \\delta L^2 g,\\] co\njest symetryczne w \\(\\epsilon\\) \\(\\delta\\). Korzystając teraz\nz (19)\nnapiszmy \\[\\begin{gathered}\n            \\epsilon\\delta L_\\epsilon L_\\delta =\n            \\left( (-\\epsilon L)^{-1}-\\right) \\left( (-\\delta L)^{-1} -\\right) = \\\\\n            \\left( (-\\delta L)^{-1}-\\right) \\left( (-\\epsilon L)^{-1} -\\right) =\n            \\epsilon\\delta L_\\delta L_\\epsilon.\n        \\end{gathered}\\] Zauważmy, że dzięki temu operatory \\(L_\\delta\\),\n\\(L_\\epsilon\\), \\(T_\\epsilon(t)\\) \\(T_\\delta(t)\\) również komutują.Chcąc pokazać, że \\(T_\\epsilon(t)\\) przy \\(\\epsilon \\0\\) rzeczywiście\nzbiegają będziemy chcieli opierać się na zbieżności \\(L_\\epsilon\\) przy\n\\(\\epsilon \\0\\). Uzasadnimy teraz nierówność, która uzasadni, że takie\nwnioskowanie jest możliwe. Stosując zasadnicze twierdzenie rachunku\nróżniczkowego całkowego otrzymujemy, że dla każdej \\(f \\C_0(S)\\),\n\\[T_\\epsilon(t)f - T_\\delta(t)f\n            = \\int_0^t \\frac{\\mathrm{d}}{\\mathrm{d}s} [T_\\epsilon(s)T_\\delta(t - s)] f \\, \\mathrm{d}s\n            = \\int_0^t T_\\epsilon(s)T_\\delta(t - s) (L_\\epsilon - L_\\delta) f \\, \\mathrm{d}s.\\]\nSkoro \\(\\| T_\\epsilon(s) \\| , \\|T_\\delta(t - s) \\| \\leq 1\\) udowodniona\nwłaśnie nierówność implikuje, że \\[\n            \\|T_\\epsilon(t)f - T_\\delta(t)f\\| \\leq t \\|L_\\epsilon f - L_\\delta f\\| (\\#eq:3-21).\\]\nWystarczy pokazać zatem, że \\(L_\\epsilon\\) zbiegają. Spodziewamy się, że\n\\((-\\epsilon L)^{-1} \\\\), co z kolei powinno implikować, że\n\\(L_\\epsilon \\L\\). Sprawdzimy teraz formalnie rozumowanie. Dla\n\\(f \\\\mathcal{D}(L)\\) mamy \\[(- \\epsilon L)^{-1} f - f\n            = \\epsilon (- \\epsilon L)^{-1} L f\\] Skoro\n\\(\\|(-\\epsilon L)^{-1}\\|\\leq 1\\), \n\\[\\|(- \\epsilon L)^{-1} f - f\\| \\leq \\epsilon \\|L f\\|.\\] W\nszczególności, dla \\(f \\\\mathcal{D}(L)\\),\n\\[\\lim_{\\epsilon \\0} (- \\epsilon L)^{-1} f\n            = f.\\] Skoro \\((-\\epsilon L)^{-1}\\) jest ciągłe \n\\(\\mathcal{D}(L)\\) gęste w \\(C_0(S)\\), powyższa zbieżność ma miejsce dla\nwszystkich \\(f \\C_0(S)\\). Ponieważ\n\\[L_\\epsilon f = (- \\epsilon L)^{-1} L f\\] dla \\(f \\\\mathcal{D}(L)\\),\n\n\\[\\begin{equation}\n            \\lim_{\\epsilon \\0} L_\\epsilon f = L f\n\\tag{21}\n\\end{equation}\\]\ndla\n\\(f \\\\mathcal{D}(L)\\). Tak więc przez @ref\\tag{22},\ngranica w (20) istnieje jednostajnie na ograniczonych zbiorach \\(t\\)\ndla \\(f \\\\mathcal{D}\\). Ponieważ zbiór \\(\\mathcal{D}(L)\\) jest gęsty w\n\\(C_0(S)\\) \\(T_\\epsilon(t)\\) jest kontrakcją, ten sam wniosek jest\nprawdziwy dla wszystkich \\(f \\C_0(S)\\). Z udowodnionej właśnie\nzbieżności wynika, że \\(T(t)\\) spełnia własność półgrupy oraz wszystkie\npostulaty procesu Fellera z wyjątkiem ostatniego.Aby uzasadnić, że \\((T_t)_{t \\\\mathbb{R}_+}\\) spełnia ostatnią własność\npokażemy najpierw, że\n\\[(- \\alpha L)^{-1} = \\alpha \\int_0^\\infty e^{-\\alpha t} T_t \\mathrm{d} t.\\]\nUstalmy w tym celu \\(\\lambda > 0\\) tak małe, że\n\\(\\mathcal{R}(- \\lambda L) = C_0(S)\\), wybierzmy \\(\\alpha\\) tak, że\n\\(\\alpha \\lambda = 1\\). Ustalmy \\(g \\C_0(S)\\). Chcąc uzasadnić\n\\[\n            (- \\alpha L)^{-1}g = \\alpha \\int_0^\\infty e^{-\\alpha t} T_tg \\mathrm{d} t\n\\tag{23}\n\\]\nodwołamy się aproksymacji \\(L\\) \\(T(t)\\) przez odpowiednio \\(L_\\epsilon\\)\noraz \\(T_\\epsilon(t)\\). Niech \\[f_\\epsilon = (-\\alpha L_\\epsilon)^{-1}g  \n            = \\alpha U_\\epsilon(\\alpha)g\n            = \\alpha \\int_0^\\infty e^{-\\alpha t} T_\\epsilon(t)g \\, \\mathrm{d}t.\\]\nZ (20),\n\\[\\lim_{\\epsilon \\0^+} f_\\epsilon\n            = f, \\qquad\n            f= \\alpha \\int_0^\\infty e^{-\\alpha t} T(t)g \\, \\mathrm{d}t.\\]\nPołóżmy \\(h_\\epsilon = (- \\epsilon L)^{-1} f_\\epsilon\\). Wówczas z\n(19),\n\\(\\lim_{\\epsilon \\0} h_\\epsilon = f\\). Niech wreszcie\n\\(h = (-\\lambda L)^{-1} g\\). Naszym aktualnym\ncelem (23) wprowadzonej właśnie notacji jest pokazać, że\n\\(f=h\\). Z definicji \\(L_\\epsilon\\), \\[\\begin{gathered}\n            \\lambda L h_\\epsilon =\n            L_\\epsilon f_\\epsilon =\n            \\alpha\\int_0^\\infty e^{-\\alpha t} L_\\epsilon T_\\epsilon(t) f \\mathrm{d}t=\n            \\alpha\\int_0^\\infty e^{-\\alpha t} \\frac{\\mathrm{d}}{\\mathrm{d}t} T_\\epsilon(t) f \\mathrm{d}t \\\\\n            -\\alpha g+\\alpha^2\\int_0^\\infty e^{-\\alpha t}T_\\epsilon(t) f \\mathrm{d}t = \\alpha(f_\\epsilon-g)\n            \\rightarrow \\alpha (f - g)\n        \\end{gathered}\\] gdy \\(\\epsilon \\0\\). Dlatego,\n\\[\\lim_{\\epsilon \\0} [(h_\\epsilon - h) - \\lambda L(h_\\epsilon - h)] = 0,\\]\nz czego z kolei wynika, że \\(h_\\epsilon - h \\0\\). Stąd \\(f = h\\).\nZastosowanie (23) \\(g_n\\) \\(f_n =(-\\lambda L)^{-1}g_n\\) z\nDefinicji 8 widzimy, że\n\\[\\lim_{n \\\\infty} \\alpha \\int_{0}^{\\infty} e^{-\\alpha t} (T(t)g_n)(x) \\mathrm{d}t\n            =\\lim_{n \\\\infty }f_n(x)= 1\\] dla każdego \\(x \\S\\).\nNiech \\(\\mu_{t,x}\\) będzie miarą zdefiniowaną przez\n\\[T(t)f(x) = \\int f(y) \\mu_{t,x}(\\mathrm{d} y).\\] Skoro \\(T(t)\\) jest\nograniczonym operatorem, miara \\(\\mu_{t,x}\\) jest skończona. Jako, że\n\\(g_n \\1\\) punktowo, twierdzenia o zbieżności ograniczonej mamy więc\n\\[T(t)g_n(x) \\\\mu_{t,x}(S).\\] Stąd\n\\[1 =  \\alpha \\int_{0}^{\\infty} e^{-\\alpha t}  \\mu_{t,x}(S) \\mathrm{d} t.\\]\nInnymi słowy transformatą Laplace’funkcji \\(t \\mapsto \\mu_{t,x}(S)\\)\njest \\(\\alpha \\1/\\alpha\\). Widzimy więc, że \\(\\mu_{t,x}(S) \\equiv 1\\).\nCzyli \\(T(t) g_n \\1\\) punktowo. Oznacza , że spełniona jest ostatnia\nwłasność w Definicji 8.Wreszcie, sprawdzamy, że ta półgrupa Fellera\n\\((T_t)_{t \\\\mathbb{R}_+}\\) ma generator \\(L\\). Mamy\n\\[T_\\epsilon(t)f - f\n            = \\int_{0}^{t} \\frac{\\mathrm{d}}{\\mathrm{d}s} T_\\epsilon(s) \\mathrm{d}s\n            = \\int_{0}^{t} T_\\epsilon(s)L_\\epsilon f \\mathrm{d}s.\\]\nJeśli \\(f \\\\mathcal{D}(L)\\), (20), (21) oraz\nwłasność kontrakcji \\(T_\\epsilon(t)\\) implikują, że można przejść \ngranicy w tym równaniu, aby uzyskać \\[T(t)f - f\n            = \\int_{0}^{t} T(s)Lf \\mathrm{d}s.\\] Zatem\n\\[\\lim_{t \\0} \\frac{T(t)f - f}{t}\n            = Lf \\text{ dla } f \\\\mathcal{D}(L).\\] Pokazaliśmy\nwłaśnie, że \\[\\mathcal{D}(L) \\subseteq \\mathcal{D}'(L) =\n            \\left\\{ f \\C_0(S) \\: :\\: \\lim_{t \\0} (T(t)f - f)/t \\text{ istnieje } \\right\\}.\\]\nWiemy, że obie pary \\((L, \\mathcal{D}(L))\\) oraz \\((L, \\mathcal{D}'(L))\\) są\ngeneratorami infinitezymalnymi. Niech \\(f \\\\mathcal{D}'(L)\\). Rozważmy\n\\(g = (-\\lambda L)f\\). Istnieje \\(h \\\\mathcal{D}(L)\\) takie, że\n\\(g = (-\\lambda L)h\\). Stąd \\((-\\lambda L)(f-h) =0\\) co za tym idzie\n\\(f = h \\\\mathcal{D}(L)\\). Pokazaliśmy właśnie, że\n\\(\\mathcal{D}(L) = \\mathcal{D}'(L)\\).Twierdzenie 13  Jeśli \\((T_t)_{t \\geq 0}\\) jest\npółgrupą Fellera, istnieje proces Fellera \\((\\mathbf{P}, \\mathbb{F})\\)\nspełniający \\[\\mathbf{E}_x f(X(t)) = T_tf(x)\\] dla \\(x \\S, t \\geq 0\\),\noraz \\(f \\C_0(S)\\).Proof. Ustalmy \\(x \\S\\). Pokażemy najpierw, że istnieje proces\n\\(Y(t), t \\\\mathbb{Q}^+\\) na pewnej przestrzeni probabilistycznej,\nktóry ma pożądane rozkłady skończenie wymiarowe oraz \\(Y(0) = x\\).Dla dowolnego \\(n \\\\mathbb{N}\\) dowolnych\n\\(0 \\leq t_1< t_2 <\\ldots < t_n\\) definiujemy rozkład\n\\(\\mu_{t_1, \\ldots, t_n}\\) na \\(S^n\\) poprzez następujący wymóg: dla\ndowolnych \\(f_1, \\ldots, f_n\\) z \\(C_0(S)\\) zachodzi\n\\[\\begin{equation}\n            \\int f_1(y_1) f_2(y_2) \\cdots f_n(y_n) \\mu_{t_1, \\ldots, t_n}(\\mathrm{d}(y_1, \\ldots y_n))\n            = T_{t_1} \\left( f_1 T_{t_2-t_1} \\left( f_2 T_{t_3-t_2} ( \\ldots)  \\right) \\right)(x).\n\\tag{24}\n\\end{equation}\\]\nFunkcje o rozdzielonych zmiennych \\(f_1(y_1) f_2(y_2) \\cdots f_n(y_n)\\) są\nliniowo gęste w \\(C\\left( S^n \\right)\\). Miara \\(\\mu_{t_1, \\ldots, t_n}\\)\njest więc dobrze określona (istnieje dokładnie jedna spełniająca zadany\nwarunek). Aby powołać się na twierdzenie Kołmogorowa o istnieniu procesu\nnależy uzasadnić, że skonstruowane w ten sposób miary są zgodne.\nDokładniej, że dla dowolnego \\(j\\leq n\\), dowolnych borelowskich\n\\(A_1, \\ldots , A_n \\S\\), \\[\\begin{gathered}\n            \\mu_{t_1, \\ldots, t_n}(A_1 \\times \\cdots \\times A_{j-1} \\times\n            S \\times A_{j+1} \\times \\cdots A_n) = \\\\\n            \\mu_{t_1, \\ldots, t_{j-1}, t_{j+1} \\ldots t_n}(A_1 \\times \\cdots \\times A_{j-1} \\times\n            A_{j+1} \\times \\cdots A_n).\n        \\end{gathered}\\] Wynika \nz (24) dla \\(f_j\\equiv 1\\) zastosowaniu własności\npółgrupy \\((T_t)_{t \\\\mathbb{R}_+}\\). Zastosowanie twierdzenia\nKołmogorowa pozwala wywnioskować istnienie procesu o zadanych rozkładach\nskończenie wymiarowych. Jednakże nie daje ono żadnej kontroli nad\nregularnością trajektorii. Dlatego jesteśmy zmuszeni przeprowadzić\ndodatkową konstrukcję.Stosując konstrukcję w poprzedniego paragrafu dla wymiernych \\(t\\)\notrzymujemy proces \\((Y_q^{(x)} \\: : \\: q \\\\mathbb{Q}_+)\\) o rozkładach\nzadanych rozkładach skończenie wymiarowych:\n\\[\\mathbb{P}[Y_{t_1}^{(x)}\\A_1, \\ldots , Y_{t_n}^{(x)} \\A_n] =\n            \\mu_{t_1, \\ldots, t_n}(A_1 \\times \\cdots \\times A_n)\\] oraz\ntaki, że \\(\\mathbb{P}[Y^{(x)}_0=x]=1\\). Proces ten ma własność Markowa. Z\ndefinicji miary \\(\\mu_{t_1, \\ldots, t_n}\\),\n\\[\\mathbb{E} [f_1(Y_{t_1}^{(x)}) \\cdots f_n(Y_{t_n}^{(x)})  ] =\n            \\mathbb{E} [f_1(Y_{t_1}^{(x)}) \\cdots f_{n-1}(Y_{t_{n-1}}^{(x)}) T_{t_{n} - t_{n-1}} f_n (Y_{n-1}^{(x)}) ]\\]\nPowyższe implikuje\n\\[\\mathbb{E} [f_n(Y_{t_n}^{(x)})  | Y_q^{(x)}, q \\leq t_{n-1}] = T_{t_n-t_{n-1}} f_n(Y_{t_{n-1}}^{(x)})\\]\nW analogiczny sposób uzasadniamy własność Markowa.Jeśli \\(f \\C_0(S)\\) jest nieujemna, \n\\[e^{-\\alpha t} T(t) U(\\alpha) f\n            = \\int_{t}^{\\infty} e^{-\\alpha s} T(s) f \\mathrm{d}s\n            \\leq U(\\alpha) f.\\] Stąd wynika, że\n\\(e^{-\\alpha t} U(\\alpha) f(Y(t))\\) jest (ograniczonym) supermartingałem\nwzględem filtracji \\(\\{\\mathcal{F}_t, t \\\\mathbb{Q}^+\\}\\), gdzie\n\\(\\mathcal{F}_t\\) jest generowana przez\n\\[(Y(s), s \\\\mathbb{Q} \\cap [0,t]).\\] Rzeczywiście mamy\n\\[\\mathbb{E}[ e^{-\\alpha(t+s)} U(\\alpha) f(Y_{t+s}) | \\mathcal{F}_s]\n            =  e^{-\\alpha (t+s)} T_t U(\\alpha) f(Y_s) \\leq e^{-\\alpha s} U(\\alpha) f(Y_s).\\]\nWówczas prawdopodobieństwem jeden, prawostronne lewostronne granice\ntego supermartingału wzdłuż liczb wymiernych istnieją w każdym\n\\(t \\[0, \\infty)\\).Dowód twierdzenia o zbieżności nadmartynggałów sprowadza się pokazania,\nże w prawdopodobieństwem jeden liczba przejść w dół przez każdy\nprzedział o końcach wymiernych jest skończona. Każda funkcja zmiennej\nrzeczywistej o tej własności musi mieć lewostronną prawostronną\ngranicę w każdym punkcie.Dla każdej \\(\\alpha\\) każdej nieujemnej \\(f\\) proces\n\\(e^{-\\alpha t} U(\\alpha) f(Y_t)\\) ma lewostronne prawostronne granice.\nOczywiście, zbiór o prawdopodobieństwie zero, na którym się nie\nudaje, może zależeć od \\(f\\) \\(\\alpha\\). Musimy więc ograniczyć naszą\nuwagę gęstego, przeliczalnego zbioru \\(f\\) \\(\\alpha\\), aby stwierdzić\nistnienie prawostronnych lewostronnych granic procesu \\(Y(s)\\) samego w\nsobie. Ponieważ \\(\\alpha U(\\alpha) f \\f\\) gdy \\(\\alpha \\\\infty\\), oraz\n\\(C_0(S)\\) jest ośrodkową przestrzenią metryczną, możemy po prostu użyć\nfunkcji \\(U(\\alpha) f\\), gdzie \\(\\alpha\\) jest liczbą całkowitą, oraz \\(f\\)\njest wzięte z pewnego przeliczalnego gęstego zbioru w \\(C_0(S)\\).Istnieje nadal jeden problem, że te prawostronne lewostronne granice\nmogą być jednopunktową kompaktyfikacją \\(S\\), wykluczenie tego przypadku\nwymaga dodatkowego argumentu. Chodzi o , żeby uzasadnić, że \\(Y\\) nie\nucieka nieskończoności. Niech \\(f \\C_0(S)\\) będzie ściśle dodatnie.\nRozważmy supermartingał \\(M(t) = e^{-tU(1)f(Y(t))}\\). jest ściśle\ndodatnie, więc \\(\\inf_{s \\\\mathbb{Q} \\cap [0,t]} M(s) > 0\\) prawie na\npewno dla każdego \\(t > 0\\) (zadanie). Oznacza , że z\nprawdopodobieństwem jeden \\(Y_s\\) dla \\(s \\leq t\\) są zawarte w zbiorze\nzwartym.Wiemy teraz, że z prawdopodobieństwem jeden, \\(Y(s)\\) ma prawostronne \nlewostronne granice w każdym \\(t \\[0, \\infty)\\). Więc możemy\nzdefiniować\n\\[X(t) = \\lim_{\\substack{s \\downarrow t \\\\ s \\\\mathbb{Q}}} Y(s).\\] \njest automatycznie prawostronnie ciągłe ma lewostronne granice.\nPonieważ skończenie wymiarowe rozkłady odpowiadające \\(T(t)\\) są słabo\nciągłe, \\(X(t)\\) ma poprawne skończenie wymiarowe rozkłady.","code":""},{"path":"section-wykład-7-rozkłady-stacjonarne-zaburzenia-ruchu-browna.html","id":"section-wykład-7-rozkłady-stacjonarne-zaburzenia-ruchu-browna","chapter":"Wykład 7: Rozkłady stacjonarne, zaburzenia ruchu Browna","heading":"Wykład 7: Rozkłady stacjonarne, zaburzenia ruchu Browna","text":"2024-11-14Piotr Dyszewski","code":""},{"path":"section-wykład-7-rozkłady-stacjonarne-zaburzenia-ruchu-browna.html","id":"section-rozkłady-stacjonarne","chapter":"Wykład 7: Rozkłady stacjonarne, zaburzenia ruchu Browna","heading":"Rozkłady stacjonarne","text":"Interesować nas będzie asymptotyczne zachowanie procesów Fellera.\nPodobnie jak w przypadku łańcuchów Markowa w czasie dyskretnym rozkłady\ngraniczne są niezmiennicze ze względu na funkcje przejścia. Przez\n\\(\\Sigma\\) oznaczać będziemy \\(\\sigma\\)-ciało zbiorów borelowskich \\(S\\),\nczyli najmniejsze \\(\\sigma\\)-ciało zawierające wszystkie otwarte podzbiory\n\\(S\\). Skoro \\(S\\) jest ośrodkowa, \\(\\Sigma\\) jest generowane przez\nwszystkie kule otwarte.Dla Procesu Fellera \\((\\mathbf{P}, \\mathbb{F})\\) oraz rozkładu\nprawdopodobieństwa \\(\\mu\\) na \\(S\\) definiujemy miarę probabilistyczną\n\\(\\mathbf{P}_\\mu\\) na \\((S, \\Sigma)\\) wzorem \\[\\mathbf{P}_\\mu[]\n        = \\int_S \\mathbf{P}_x[] \\: \\mu(\\mathrm{d}x),\n        \\qquad \\\\mathcal{F}.\\] W tym miejscu zachęcamy czytelnika\nwprawdzenia, że odwzorowanie \\(x \\mapsto \\mathbf{P}_x[]\\) jest\nmierzalne dla \\(\\\\mathcal{F}\\). Miara \\(\\mathbf{P}_\\mu\\) rozkład\nprocesu Markowa przy rozkładzie początkowym \\(\\mu\\).Definicja 9  Niech \\((\\mathbf{P}, \\mathbb{F})\\) będzie procesem\nFellera Rozkład prawdopodobieństwa \\(\\pi\\) na \\((S, \\Sigma)\\) nazywamy\nrozkładem stacjonarnym jeżeli\n\\[\\mathbf{P}_\\pi \\left[ X(t) \\\\right] = \\pi()\\] dla każdego\n\\(\\\\Sigma\\).Chcielibyśmy wiedzieć, jak określić na podstawie generatora, czy miara\nprawdopodobieństwa na \\(S\\) jest stacjonarna dla procesu Fellera. Z tego\npowodu przepiszemy powyższą definicję w terminach półgrupy. Jeśli \\(\\mu\\)\njest miarą prawdopodobieństwa na \\(S\\), rozkład procesu w czasie \\(t\\), gdy\nrozkład początkowy jest \\(\\mu\\), oznaczamy przez \\(\\mu T(t)\\). Spełnia \nzależność\n\\[\\int f \\, \\mathrm{d}(\\mu T(t)) = \\int T(t)f \\, \\mathrm{d} \\mu = \\mathbf{E}_\\mu \\left[ f(X(t)) \\right]\\]\ndla \\(f \\C_0(S)\\). Tutaj \\(\\mathbf{E}_\\mu\\) wartość oczekiwana\nodpowiadająca \\(\\mathbf{P}_\\mu\\). Równoważnie\n\\[\\mathbf{E}_\\mu[Y] = \\int_X \\mathbf{E}_x[Y] \\: \\mu(\\mathrm{d}x)\\] dla\nkażdej ograniczonej zmiennej losowej \\(Y \\colon \\Omega \\\\mathbb{R}\\).Definicja 10  Miara prawdopodobieństwa \\(\\mu\\) na \\(S\\) jest stacjonarna\ndla procesu Fellera z półgrupą \\(T(t)\\), jeśli \\(\\mu T(t) = \\mu\\) dla\nwszystkich \\(t \\geq 0\\), tzn. jeśli \\[\\label{eq:3:28}\n            \\int T(t)f \\, \\mathrm{d}\\mu\n            = \\int f \\, \\mathrm{d}\\mu\n            \\quad \\text{dla wszystkich}\n            \\ f \\C_0(S) \\ \\text{} \\ t \\geq 0.\\]Zadanie 14  Pokaż, że jeśli \\(\\mu\\) jest miarą prawdopodobieństwa na\n\\(S\\) \\(\\mu T(t) \\Rightarrow \\nu\\), \\(\\nu\\) jest stacjonarna.Twierdzenie 14  Miara prawdopodobieństwa \\(\\mu\\) na \\(S\\) jest\nstacjonarna dla odpowiadającego procesu wtedy tylko wtedy, gdy\n\\[\\int Lf \\, \\mathrm{d}\\mu = 0 \\quad \\text{dla wszystkich} \\ f \\D.\\]Proof. Przypuśćmy, że \\(\\mu\\) jest stacjonarna, weźmy\n\\(f \\\\mathcal{D}(L)\\). Wtedy \\[\\int Lf \\, \\mathrm{d}\\mu\n            = \\int \\lim_{t \\0} \\frac{T(t)f - f}{t} \\, \\mathrm{d}\\mu\n            = \\lim_{t \\0}  \\frac{\\int T(t)f \\mathrm{d}\\mu - \\int f \\, \\mathrm{d}\\mu}{t} = 0.\\]\nPrzeciwnie, przypuśćmy, że \\(\\int Lf \\, \\mathrm{d}\\mu = 0\\) dla wszystkich\n\\(f \\\\mathcal{D}(L)\\) jeśli \\(f \\\\mathcal{D}(L)\\) oraz\n\\(f - \\lambda Lf = g\\), \\(\\int f \\, d\\mu = \\int g \\, d\\mu\\). Iterując ,\notrzymujemy\n\\[\\int (- \\lambda L)^{-n}g \\, \\mathrm{d}\\mu = \\int g \\, \\mathrm{d}\\mu.\\]\nBiorąc \\(\\lambda = t/n\\) przechodząc z \\(n \\\\infty\\) wnioskujemy, że\n\\[\\int T(t)g \\, \\mathrm{d}\\mu = \\int g \\, \\mathrm{d}\\mu.\\]Oto wystarczający warunek na istnienie rozkładu stacjonarnego.Twierdzenie 15  Jeśli \\(S\\) jest przestrzenią zwartą, istnieje miara stacjonarna.Proof. Rozważmy proces Fellera z dowolnym rozkładem początkowym \\(\\mu\\).\nNiech \\(\\nu_n\\) będzie rozkładem zmiennej Zdefiniujmy miarę \\(\\nu_n\\) na \\(S\\)\npoprzez warunek \\[\\int_S f(y) \\nu_n( \\mathrm{d}y) =\n            \\mathbb{E} \\left[ \\mathbf{E}_\\mu \\left[ f(X_{nU}) \\right] \\right]=\n            \\mathbb{E} \\left[ \\int_S T_{nU}f(y) \\: \\mu(\\mathrm{d}y) \\right].\\]\nDla \\(f \\C_0(S)\\), własność półgrupy daje\n\\[\\int T(t)f(y) \\, \\nu_n (\\mathrm{d}y)=\n            \\mathbb{E} \\left[ \\int_S T_{nU+t}f(y) \\: \\mu(\\mathrm{d}y) \\right].\\]\ntakże\n\\[\\begin{multline}\n\\int f \\, \\mathrm{d}\\nu_n - \\int T(t)f \\, \\mathrm{d}\\nu_n\n= \\int f \\, \\mathrm{d}\\nu_n - \\int f \\, \\mathrm{d}(\\nu_n T(t))\\\\\n= \\frac{1}{n} \\left[ \\int_0^t \\int_S T(r)f \\, \\mathrm{d}\\mu \\, \\mathrm{d}r -\n\\int_n^{n+t} \\int_S T(r)f \\, \\mathrm{d}\\mu \\, \\mathrm{d}r \\right].\n\\tag{25}\n\\end{multline}\\]\nPrawa strona (25) dąży\nzera gdy \\(n \\\\infty\\).Teraz, ponieważ \\(S\\) jest zwarty, twierdzenie Prochorowa, implikuje, że\nistnieje podciąg \\(\\nu_{n_k}\\) taki, że \\[\\nu_{n_k} \\Rightarrow \\nu\\] dla\npewnej miary prawdopodobieństwa \\(\\nu\\) na \\(S\\). Zatem, ponieważ\n\\(T(t)f \\C(S)\\), możemy przejść granicy w (25)\nwzdłuż ciągu \\(\\nu_{n_k}\\), aby otrzymać\n\\[\\int f \\, \\mathrm{d}\\nu = \\int T(t)f \\, \\mathrm{d}\\nu.\\] Ponieważ \nzachodzi dla wszystkich \\(f \\C_0(S)\\), wynika stąd, że\n\\(\\nu T(t) = \\nu\\).","code":""},{"path":"section-wykład-7-rozkłady-stacjonarne-zaburzenia-ruchu-browna.html","id":"section-zaburzenia-ruchu-browna","chapter":"Wykład 7: Rozkłady stacjonarne, zaburzenia ruchu Browna","heading":"Zaburzenia ruchu Browna","text":"Przykład 5  Rozważmy ruch Browna na \\([0, \\infty)\\) z absorpcją w 0.Niech \\(\\tau\\) będzie czasem pierwszego uderzenia w 0. Zdefiniujmy\n\\[X_a(t) =\n\\begin{cases}\nX(t) & \\text{jeśli } t < \\tau, \\\\\n0 & \\text{jeśli } t \\ge \\tau,\n\\end{cases}\\] oraz oznaczmy przez \\(L_a\\) \\(T_a(t)\\) odpowiednio generator\npółgrupę. Dla \\(f \\C_0[0, \\infty)\\), niech \\(f_o\\) będzie\n„nieparzystym” przedłużeniem \\(f\\) na \\(\\mathbb{R}\\): \\[f_o(x) =\n\\begin{cases}\nf(x) & \\text{jeśli } x \\ge 0, \\\\\n2f(0) - f(-x) & \\text{jeśli } x < 0.\n\\end{cases}\\] Z zasady odbicia dla każdej \\(g \\C_0[0,\\infty)\\),\n\\[\\mathbf{E}_x \\left[g(X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right] =\n    \\mathbf{E}_x \\left[g(-X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right].\\]\nBiorąc \\(g=f_o\\),\n\\[\\mathbf{E}_x \\left[f_o(X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right]\n    = \\mathbf{E}_x \\left[f_o(-X(t)) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right].\\]\nObie te wielkości są równe\n\\[\\frac{1}{2} \\mathbf{E}_x \\left[(f_o(X(t)) + f_o(-X(t)) ) \\mathbf{1}_{\\{ t \\ge \\tau \\}} \\right].\\]\nOstatnie wyrażenie, z definicji \\(f_o\\) jest równe\n\\[f(0)\\mathbf{P}_x(t \\ge \\tau).\\] Podsumowując dla \\(x \\ge 0\\),\n\\[T_a(t)f(x) = \\mathbf{E}_x \\left[f(X(t)) \\mathbf{1}_{\\{ t < \\tau \\}} \\right] +\n    f(0)\\mathbf{P}_x(t \\ge \\tau) = \\mathbf{E}_x f_o(X(t)).\\] Oczywiście\n\\(f_o \\notin C(\\mathbb{R})\\) o ile \\(f(0) = 0\\). Niemniej jednak, skoro\n\\[f_o''(x) =\n\\begin{cases}\nf''(x) & \\text{jeśli } x > 0, \\\\\n-f''(-x) & \\text{jeśli } x < 0,\n\\end{cases}\\] wtedy \\(f''(0) = 0\\) jest potrzebne, aby \\(f_o''\\) było\nciągłe. Wynika z tego, że\n\\[\\mathcal{D}(L_a) = \\{f \\C_0[0, \\infty) :  f'' \\C[0, \\infty), f''(0) = 0\\},\\]\ndla \\(f \\\\mathcal{D}(L_a)\\), \\(L_a f = \\frac{1}{2} f''\\).Przykład 6  Rozważmy ruch Browna na \\([0, \\infty)\\) z odbiciem w \\(0\\).\nProces ten jest zdefiniowany jako \\[X_r(t) = |X(t)|,\\] jego generator\npółgrupa będą oznaczane odpowiednio przez \\(L_r\\) \\(T_r(t)\\). Jeśli\n\\(f \\C_0[0, \\infty)\\), niech \\(f_e\\) będzie parzystym przedłużeniem \\(f\\)\nna \\(\\mathbb{R}\\): \\[f_e(x) =\n\\begin{cases}\nf(x) & \\text{jeśli } x \\ge 0, \\\\\nf(-x) & \\text{jeśli } x < 0.\n\\end{cases}\\] Wtedy\n\\[T_r(t)f(x) = \\mathbf{E}_x \\left[f(|X(t)|)\\right] = \\mathbf{E}_x f_e(X(t)) \\quad \\text{dla } x \\ge 0.\\]\nZatem, \\[f \\\\mathcal{D}( L_r) \\iff f_e \\\\mathcal{D}(L).\\] Wynika z\ntego, że\n\\[\\mathcal{D}(L_r) = \\{f \\C[0, \\infty) : f', f'' \\C[0, \\infty), f'(0) = 0\\},\\]\ndla \\(f \\\\mathcal{D}(L_r)\\), \\(L_r f = \\frac{1}{2} f''\\).Przykład 7  Zaprezentujemy teraz ruch Browna na \\([0, \\infty)\\) z\nlepkim \\(0\\). Dla \\(c > 0\\), rozważmy operator \\(L_c\\) zdefiniowany jako\n\\(L_c f = \\frac{1}{2} f''\\) na\n\\[\\mathcal{D}(L_c) = \\{f \\C_0[0, \\infty) : f'' \\C[0, \\infty), f'(0) = c f''(0)\\}.\\]\nZauważmy, że graniczne przypadki \\(c \\downarrow 0\\) \\(c \\uparrow \\infty\\)\nodpowiadają odpowiednio odbiciu absorpcji w \\(0\\). Jest generator\nprawdopodobieństwa — dowód jest pozostawiony jako ćwiczenie. Oto\nweryfikacja własności (d) w Definicji 2: Dla \\(g \\C_0[0, \\infty)\\) \\(\\lambda > 0\\), musimy\nrozwiązać \\(f - \\lambda L_c f = g\\) dla \\(f \\\\mathcal{D}(L_c)\\). Niech\n\\(f_a \\\\mathcal{D}(L_a)\\) oraz \\(f_r \\\\mathcal{D}(L_r)\\) będą\nrozwiązaniami\n\\[f_a - \\lambda L_a f_a = g \\quad \\text{oraz} \\quad f_r - \\lambda L_r f_r = g.\\]\nPonieważ wszystkie trzy generatory są równe \\(\\frac{1}{2} f''\\) na swoich\ndziedzinach, \\[f = \\gamma f_a + (1 - \\gamma) f_r\\] jest wymaganym\nrozwiązaniem, pod warunkiem że \\(f'(0) = c f''(0)\\). Ma miejsce, gdy\n\\(\\gamma\\) spełnia \\[\\gamma f_a'(0) = c (1 - \\gamma) f_r''(0).\\] Aby\nznaleźć wartość \\(\\gamma\\), \\(f_a'(0)\\) \\(f_r''(0)\\) muszą mieć ten sam\nznak. Aby sprawdzić, rozważmy \\(h = f_a - f_r\\). Wtedy\n\\(h - \\frac{\\lambda}{2} h'' \\equiv 0\\), więc, ponieważ \\(h\\) jest\nograniczone, \\[h(x) = h(0) e^{-x \\sqrt{2/\\lambda}}.\\] Wynika z tego, że\n\\[f_a'(0) = -\\sqrt{2/\\lambda} h(0)\\] oraz\n\\[f_r''(0) = -\\left(\\frac{2}{\\lambda}\\right) h(0),\\] więc mają ten sam\nznak, \\[\\gamma = \\frac{2c}{2c + \\sqrt{2\\lambda}}.\\] Aby powiedzieć coś\no zachowaniu tego procesu, gdy odwiedza 0, napiszmy","code":""},{"path":"section-wykład-8-układy-spinowe.html","id":"section-wykład-8-układy-spinowe","chapter":"Wykład 8: Układy spinowe","heading":"Wykład 8: Układy spinowe","text":"2024-11-21Piotr Dyszewski","code":""},{"path":"section-wykład-8-układy-spinowe.html","id":"section-motywacje","chapter":"Wykład 8: Układy spinowe","heading":"Motywacje","text":"Aby umotywować nasze przyszłe działania przedyskutujemy kilka\nprzykładów. Od tej pory niech \\(G =(V, E)\\) będzie przeliczalnym grafem\nprostym o ograniczonym stopniu. Dokładniej zakładać będziemy, że zbiór\njego wierzchołków \\(V\\) jest skończony bądź przeliczalny oraz, że\n\\[\\sup_{x \\V} \\mathrm{deg}(x) <\\infty.\\] Rozważmy następujące trzy\nprocesy na \\(G\\).Przykład 8  (Voter model). Przypuśćmy, że na \\(G\\) są dwie wzajemnie\nzwalczające się frakcje (można myśleć o republikanach demokratach). Na\nkażdy wierzchołek jednej frakcji oddziałują (poprzez indoktrynację)\nsąsiedzi z frakcji przeciwnej. Na skutek czego niektóre wierzchołki\nzmieniają frakcję na przeciwną. Dokładniej każdy \\(x \\V\\) zmienia\nfrakcję w intensywnością równą\n\\[c(x) = \\# \\{ y \\V \\: \\: x \\sim y, \\: \\mbox{$y$ jest z innej frakcji niż $x$} \\}.\\]\nJest równoważne z następującym opisem.Dla każdej pary sąsiednich wierzchołków \\(x\\sim y\\) pochodzących z\nróżnych frakcji losujemy niezależnie dwie liczby \\(E_{x\\y}\\) oraz\n\\(E_{y\\x}\\) z rozkładu wykładniczego z parametrem jeden.\n\\(E_{x\\y}\\) interpretujemy jako czas, jaki potrzebuje \\(x\\) aby\nprzekabacić \\(y\\).Dla każdej pary sąsiednich wierzchołków \\(x\\sim y\\) pochodzących z\nróżnych frakcji losujemy niezależnie dwie liczby \\(E_{x\\y}\\) oraz\n\\(E_{y\\x}\\) z rozkładu wykładniczego z parametrem jeden.\n\\(E_{x\\y}\\) interpretujemy jako czas, jaki potrzebuje \\(x\\) aby\nprzekabacić \\(y\\).W momencie, w którym którykolwiek z wierzchołków przeciągnie sąsiada\n(powiedzmy \\(y\\)) na swoją stronę, losujemy nowe wagi dla sąsiadów \\(y\\)\n(według nowego układu obu frakcji).W momencie, w którym którykolwiek z wierzchołków przeciągnie sąsiada\n(powiedzmy \\(y\\)) na swoją stronę, losujemy nowe wagi dla sąsiadów \\(y\\)\n(według nowego układu obu frakcji).Oczywiście powyższa, naiwna konstrukcja ma sens tylko, gdy graf \\(G\\) jest\nskończony. W przypadku grafów nieskończonych wymagana jest odpowiednia\nkonstrukcja, którą opiszemy niebawem.Przykład 9  (Contact process). Załóżmy, że na grafie \\(G\\) panuje\nepidemia. Niech \\(\\lambda >0\\) będzie ustalonym parametrem. Proces rozwija\nsię według następujących zasad.Chore wierzchołki niezależnie zarażają swoich zdrowych sąsiadów z\nczasem wykładniczym z parametrem \\(\\lambda\\).Chore wierzchołki niezależnie zarażają swoich zdrowych sąsiadów z\nczasem wykładniczym z parametrem \\(\\lambda\\).Chore wierzchołki zdrowieją niezależnie z czasem wykładniczym z\nparametrem jeden.Chore wierzchołki zdrowieją niezależnie z czasem wykładniczym z\nparametrem jeden.Ponownie powyższy opis ma sens jedynie w przypadku, gdy \\(G\\) jest\nskończony. Dla nieskończonych grafów \\(G\\) będziemy posługiwać się\nrównoważną charakteryzacjąKażdy chory wierzchołek zdrowieje z intensywnością \\(1\\)Każdy chory wierzchołek zdrowieje z intensywnością \\(1\\)Każdy zdrowy wierzchołek \\(x\\) choruje z intensywnością\n\\[\\lambda \\# \\{ y\\sim x \\: : \\: y \\mbox{ chory}\\}.\\]Każdy zdrowy wierzchołek \\(x\\) choruje z intensywnością\n\\[\\lambda \\# \\{ y\\sim x \\: : \\: y \\mbox{ chory}\\}.\\]Przykład 10  (Exclusion process). Rozważmy zjawisko migracji na grafie\n\\(G\\). Załóżmy, że pewne wierzchołki \\(G\\) są zajmowane przez osobników\nustalonego gatunku (przykładowo krowy). Każdy osobnik czeka niezależnie\nwykładniczy czas z parametrem jeden, po czym podejmuje próbę\nprzemieszczenia się.Jeżeli osobnik zajmuje wierzchołek \\(x \\V\\), losuje wierzchołek\n\\(y\\) z prawdopodobieństwem \\(p(x,y)\\).Jeżeli osobnik zajmuje wierzchołek \\(x \\V\\), losuje wierzchołek\n\\(y\\) z prawdopodobieństwem \\(p(x,y)\\).Jeżeli wierzchołek \\(y\\) nie jest zajęty, osobnik przechodzi \n\\(y\\).Jeżeli wierzchołek \\(y\\) nie jest zajęty, osobnik przechodzi \n\\(y\\).W każdym z powyższych przykładów stan układu w chwili \\(t\\) można\nzakodować przy pomocy \\(\\eta_t \\\\{ 0,1\\}^V\\). Okazuje się, że wówczas\n\\(\\eta = (\\eta_t)_{t \\\\mathbb{R}_+}\\) staje się procesem Fellera. Aby\nsię o tym dokładnie przekonać musimy przeanalizować generatory\nwynikające z przykładów.","code":""},{"path":"section-wykład-8-układy-spinowe.html","id":"section-systemy-spinowe","chapter":"Wykład 8: Układy spinowe","heading":"Systemy spinowe","text":"Właściwością, która odróżnia systemy spinowe od innych procesów Fellera\nna \\(\\{0,1\\}^S\\), jest , że poszczególne przejścia obejmują tylko jedną\nlokalizację. Niech \\(c \\colon V \\times \\{0,1\\}^V \\\\mathbb{R}_+\\) będzie\nograniczoną funkcją taką, że dla każdego \\(x \\V\\),\n\\(c(x, \\cdot) \\colon \\{0,1\\}^V \\\\mathbb{R}_+\\) jest ciągła. \\(c(x,\\eta)\\)\ninterpretować będziemy jako intensywność, z jaką stan \\(\\eta\\) zmienia się\npoprzez zmianę wartości w \\(x\\). Dla \\(x \\V\\) oraz \\(\\eta \\\\{0,1\\}^V\\)\ndefiniujemy \\(\\eta^{(x)} \\\\{0,1\\}^V\\) wzorem \\[\\eta^{(x)}(y) = \\left\\{\n    \\begin{array}{cc} \\eta(y), & y \\neq x \\\\ 1-\\eta(x), & y=x \\end{array}\\right..\\]\nDla \\(f\\) pochodzącego z odpowiedniego podzbioru \\(C_0(\\{0,1\\}^V)\\) chcemy\npołożyć\n\\[\\begin{equation}\n    Lf(\\eta) = \\sum_{x\\V} c(x, \\eta)\\left[f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right].\n\\tag{26}\n\\end{equation}\\]\nOkazuje się, że dokładne napisanie dziedziny jest problematyczne. Aby\nobejść tę trudność rozważmy\n\\[\\begin{equation}\n    D = \\left\\{ f \\C(\\{0,1\\}^V) : \\|f\\|_o := \\sup_{\\eta} \\sum_x \\left|f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right| < \\infty \\right\\}.\n\\tag{27}\n\\end{equation}\\]Naszym celem jest pokazanie, że zdefiniowanie \\(L\\) na \\(D\\) wystarcza \nzdefiniowania procesu.","code":""},{"path":"section-wykład-8-układy-spinowe.html","id":"section-dygresja-analityczna","chapter":"Wykład 8: Układy spinowe","heading":"Dygresja analityczna","text":"Generator infinitezymalny nie należy najprostszych obiektów w teorii\nprocesów Fellera. Jednym z powodów jest konieczność uwzględnienia\ndzieciny która, jak się już przekonaliśmy, ma istotny wpływ na kształt\ngenerowanego procesu. Rzadko się jednak zdarza, że dziedzinę można\nopisać jawnie. Dlatego często definiuje się proponowany generator na\nwygodnej podprzestrzeni dziedziny, następnie bierze się domknięcie.Definicja 11  Operator liniowy \\((L, \\mathcal{D}(L))\\) na \\(C_0(S)\\)\nnazywany jest domkniętym, jeśli jego wykres\n\\[\\Gamma(L)=\\{(f, Lf) : f \\D(L)\\}\\] jest domkniętym podzbiorem\n\\(C_0(S) \\times C_0(S)\\).Operator \\(L\\) jest domknięty wtedy tylko wtedy, gdy\n\\(f_n \\\\mathcal{D}(L)\\) są takie, że \\(f_n \\f\\) oraz \\(Lf_n \\h\\), \n\\(f \\\\mathcal{D}(L)\\) oraz \\(h=Lf\\).Definicja 12  Operator liniowy \\((L, \\mathcal{D}(L))\\) nazywany jest\ndomykalnym, jeżeli domknięcie jego wykresu \\(\\Gamma(L)\\) jest wykresem\noperatora liniowego. W takiej sytuacji definiujemy domknięcie\n\\(\\overline{L}\\) operatora \\(L\\) poprzez\n\\[\\Gamma \\left( \\overline{L} \\right) = \\overline{\\Gamma(L)}.\\]Operator \\(L\\) jest domykalny wtedy tylko wtedy, gdy \\(f_n \\0\\) oraz\n\\(Lf_n \\h\\) implikują \\(h=0\\). Nie każdy operator liniowy ma domknięcie.\nNa przykład, przypuśćmy, że \\(S = [0, 1]\\) \n\\[\\mathcal{D}(L) = \\{ f \\C(S) : f'(0) \\text{ istnieje} \\}\n        \\quad \\text{} \\quad Lf(x) = f'(0)x \\text{ dla } f \\\\mathcal{D}(L).\\]\nWtedy domknięcie wykresu \\(L\\) nie jest wykresem operatora liniowego.\nJednakże w kontekście\nDefinicji 8 taka sytuacja nie występuje.Fakt 1  Niech \\((L, \\mathcal{D}(L))\\) będzie operatorem liniowym na\n\\(C_0(S)\\).Przypuśćmy, że \\(L\\) spełnia (GI1)-(GI3) z\nDefinicji 8. Wtedy \\(L\\) jest domykalny, jego domknięcie\nspełnia (GI1)-(GI3).Przypuśćmy, że \\(L\\) spełnia (GI1)-(GI3) z\nDefinicji 8. Wtedy \\(L\\) jest domykalny, jego domknięcie\nspełnia (GI1)-(GI3).Jeśli \\(L\\) spełnia (GI1)- (GI4) z\nDefinicji 8, wtedy \\(L\\) jest domknięty.Jeśli \\(L\\) spełnia (GI1)- (GI4) z\nDefinicji 8, wtedy \\(L\\) jest domknięty.Jeśli \\(L\\) spełnia (GI3) (GI4), \\[\\mathcal{R}(- \\lambda L)\n                    = C_0(S) \\quad \\text{dla każdego } \\lambda > 0.\\]Jeśli \\(L\\) spełnia (GI3) (GI4), \\[\\mathcal{R}(- \\lambda L)\n                    = C_0(S) \\quad \\text{dla każdego } \\lambda > 0.\\]Jeśli \\(L\\) jest domknięty spełnia (GI3), \n\\(\\mathcal{R}(- \\lambda L)\\) jest domkniętym podzbiorem \\(C_0(S)\\).Jeśli \\(L\\) jest domknięty spełnia (GI3), \n\\(\\mathcal{R}(- \\lambda L)\\) jest domkniętym podzbiorem \\(C_0(S)\\).Proof. Dla pierwszego stwierdzenia, musimy udowodnić, że\n\\(f_n \\\\mathcal{D}(L), f_n \\0\\), oraz \\(Lf_n \\h\\) implikuje\n\\(h = 0\\). Aby zrobić, wybierzmy \\(g \\\\mathcal{D}(L)\\). Korzystając z\n(13),\n\\[\\|(- \\lambda L)(f_n + \\lambda g)\\|\n            \\geq \\|f_n + \\lambda g\\|,\n            \\quad \\lambda > 0.\\] Biorąc \\(n \\\\infty\\) następnie\ndzieląc przez \\(\\lambda\\), dostajemy\n\\[\\|g - h - \\lambda Lg\\| \\geq \\|g\\|.\\] Jeżeli teraz wybierzemy\n\\(\\lambda \\0\\), otrzymamy \\[\\| g-h \\| \\geq \\|g\\|.\\] Skoro\n\\(g\\\\mathcal{D}(L)\\) jest wzięte z gęstego zbioru otrzymujemy \\(h = 0\\).\nDomknięcie \\(\\overline{L}\\) spełnia własności (GI1) (GI2), ponieważ jest\nrozszerzeniem \\(L\\). Aby sprawdzić, czy spełnia własność (GI3),\nprzypuśćmy, że \\(f \\\\mathcal{D}(\\overline{L}), \\lambda \\geq 0\\) \n\\(f - \\lambda \\overline{L}f = g\\). Przez definicję domknięcia, istnieją\n\\(f_n \\\\mathcal{D}(L)\\), takie że \\(f_n \\f\\) \n\\(Lf_n \\\\overline{L}f\\). Przez własność (GI3) dla \\(L\\),\n\\[\\inf_{x \\S} f_n(x) \\geq \\inf_{x \\S} g_n(x),\\] gdzie\n\\(g_n = f_n - \\lambda Lf_n\\). Teraz niech \\(n \\\\infty\\). Dostajemy\n\\[\\inf_{x \\S} f(x) \\geq \\inf_{x \\S} g(x),\\] czyli własność \\((GI3)\\)\ndla \\(\\overline{L}\\).Dla dowodu drugiej części faktu, niech \\(\\overline{L}\\) będzie domknięciem\n\\(L\\). Jeśli \\(f \\\\mathcal{D}(\\overline{L})\\) \\(\\lambda > 0\\) jest małe,\nprzez własność (GI4) istnieje \\(h \\\\mathcal{D}(L)\\) takie że\n\\[\\begin{equation}\n            h - \\lambda Lh = f - \\lambda \\overline{L}f,\\tag{28}\\end{equation}\\] czyli\n\\((h - f) - \\lambda \\overline{L}(h - f) = 0\\). Z (13),\n\\(h = f\\), wtedy \\(\\overline{L}f = Lh\\) z (28).\nWięc, \\(\\overline{L} = L\\) zgodnie z tezą.Przechodząc trzeciego stwierdzenia, wystarczy sprawdzić, że dla\n\\(0 < \\lambda < \\gamma\\) warunek \\(\\mathcal{R}(- \\lambda L) = C_0(S)\\)\nimplikuje \\(\\mathcal{R}(- \\gamma L) = C_0(S)\\). Załóżmy, że\n\\(g \\C_0(S)\\), zdefiniujmy \\(\\Gamma : C(S) \\\\mathcal{D}(L)\\) przez\n\\[\\gamma \\Gamma h\n        = \\lambda (- \\lambda L)^{-1} g + (\\gamma - \\lambda)(- \\lambda L)^{-1} h.\\]\nDefinicja ta jest poprawna, ponieważ założyliśmy\n\\(\\mathcal{R}(- \\lambda L) = C_0(S)\\). Mamy\n\\[\\gamma \\| \\Gamma h_1 - \\Gamma h_2 \\|\n        = (\\gamma - \\lambda) \\| (- \\mathcal{L})^{-1} (h_1 - h_2) \\|\n        \\leq (\\gamma - \\lambda) \\| h_1 - h_2 \\|.\\] Stąd \\(\\Gamma\\) jest\nodwzorowaniem zwężającym, więc z Twierdzenia Banacha o punkcie stałym\nposiada jedyny punkt stały \\(f\\). Wówczas \\(f \\\\mathcal{D}(L)\\) oraz\n\\[\\gamma (- \\lambda \\mathcal{L}) f\n        = \\lambda g + (\\gamma - \\lambda) f.\\] Co można przekształcić \npostaci \\(f - \\gamma L f = g\\). Czyli \\(g \\\\mathcal{R}(- \\gamma L)\\),\nco należało uzasadnić.Aby udowodnić ostatnie stwierdzenie, załóżmy, że\n\\(g_n \\\\mathcal{R}(- \\lambda L)\\) \\(g_n \\g\\). Wtedy możemy\nzdefiniować \\(f_n \\\\mathcal{D}(L)\\) przez\n\\[\\begin{equation}\nf_n - \\lambda L f_n = g_n.\n\\tag{29}\n\\end{equation}\\]\nWówczas\n\\[(f_n - f_m) - \\lambda L (f_n - f_m) = g_n - g_m,\\] zatem\n\\(\\| f_n - f_m \\| \\leq \\| g_n - g_m \\|\\). Ponieważ\n\\(\\{g_n\\}_{n \\\\mathbb{N}}\\) jest ciągiem Cauchy’ego, \n\\(\\{f_n\\}_{n \\\\mathbb{N}}\\) również. Niech\n\\(f = \\lim_{n \\\\infty} f_n\\). Ponieważ \\(f_n \\f\\) \\(g_n \\g\\), z (29)\nwynika, że \\(\\lim_{n \\\\infty} L f_n\\) również istnieje. Ponieważ \\(L\\)\njest domknięte, granicą jest \\(L f\\), więc \\(f - \\lambda L f = g\\), co\noznacza, że \\(g \\\\mathcal{R}(- \\lambda L)\\), czego należało\ndowieść.","code":""},{"path":"section-wykład-8-układy-spinowe.html","id":"section-konstrukcja-systemów-spinowych","chapter":"Wykład 8: Układy spinowe","heading":"Konstrukcja systemów spinowych","text":"Naszym pierwszym celem jest znalezienie naturalnych warunków na\n\\(c(x, \\eta)\\), które gwarantują, że domknięcie \\((L,D)\\), gdzie \\(L\\) \\(D\\)\nsą dane odpowiednio\nprzez (26)\noraz (27) jest operatorem infinitezymalnym. Warunki (GI1),\n(GI2), (GI3) (GI5) są łatwe sprawdzenia nie wymagają dodatkowych\nzałożeń. Prawdziwym wyzwaniem jest (GI4).Dla warunku (GI2), używamy twierdzenia Stone’-Weierstrassa: \\(D\\) jest\nalgebrą funkcji ciągłych na zbiorze zwartym, która rozdziela punkty.\nIstotnie, dla \\(\\eta \\neq \\zeta\\) istnieje \\(x \\S\\) takie, że\n\\(\\eta(x) \\neq \\zeta(x)\\). Funkcja \\(f(\\xi) = \\xi(x)\\) rozdziela \\(\\eta\\) \n\\(\\zeta\\). Algebra \\(D\\) zawiera funkcje stałe, więc\n\\(\\overline{D} = C(\\{0,1\\}^V)\\).Dla (GI3), załóżmy \\(f \\D\\), \\(\\lambda \\geq 0\\), oraz\n\\(f - \\lambda Lf = g\\). Ponieważ \\(\\{0,1\\}^V\\) jest zbiorem zwartym \\(f\\)\njest ciągła, istnieje takie \\(\\eta\\), dla którego \\(f\\) osiąga swoje\nminimum. Wtedy \\(Lf(\\eta) \\geq 0\\), więc \\[\\min_\\zeta f(\\zeta)\n        = f(\\eta) \\geq g(\\eta)\n        \\geq \\min_\\zeta g(\\zeta).\\]Warunek (GI5) wynika z faktu, że \\(1 \\D\\) oraz \\(L1 = 0\\).","code":""},{"path":"section-wykład-9-konstrukcja-systemów-spinowych.html","id":"section-wykład-9-konstrukcja-systemów-spinowych","chapter":"wykład 9: Konstrukcja systemów spinowych","heading":"wykład 9: Konstrukcja systemów spinowych","text":"Piotr Dyszewski28-11-24Dla \\(x \\V\\) oraz \\(\\eta \\\\{0,1\\}^V\\) definiujemy\n\\(\\eta^{(x)} \\\\{0,1\\}^V\\) wzorem \\[\\eta^{(x)}(y) = \\left\\{\n    \\begin{array}{cc} \\eta(y), & y \\neq x \\\\ 1-\\eta(x), & y=x \\end{array}\\right..\\]\nDla \\(f\\) pochodzącego z odpowiedniego podzbioru \\(C_0(\\{0,1\\}^V)\\) chcemy\npołożyć \\[\\label{eq:4:defL}\n    Lf(\\eta) = \\sum_{x\\V} c(x, \\eta)\\left[f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right].\\]\nOkazuje się, że dokładne napisanie dziedziny jest problematyczne. Aby\nobejść tę trudność rozważmy \\[\\label{eq:4:defD}\n    D = \\left\\{ f \\C(\\{0,1\\}^V) : \\|f\\|_o := \\sup_{\\eta} \\sum_x \\left|f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right| < \\infty \\right\\}.\\]","code":""},{"path":"section-wykład-9-konstrukcja-systemów-spinowych.html","id":"section-konstrukcja-systemów-spinowych-cd.","chapter":"wykład 9: Konstrukcja systemów spinowych","heading":"Konstrukcja systemów spinowych cd.","text":"Aby sprawdzić (GI4) musimy wyprowadzić ograniczenie dla rozwiązań\nrównania \\(f - \\lambda Lf = g\\). Niech\n\\[\\epsilon = \\inf_{u, \\eta} [c(u, \\eta) + c(u, \\eta_u)] \\quad \\text{oraz}\n        \\quad \\gamma(x, u) = \\sup_\\eta |c(x, \\eta_u) - c(x, \\eta)|.\\]\nZauważmy, że \\(\\gamma(x,u)\\) mierzy stopień, w jakim intensywność zmiany w\nmiejscu \\(x\\) zależy od konfiguracji w miejscu \\(u\\). Niech \\(\\ell_1(V)\\)\nbędzie przestrzenią Banacha funkcji \\(\\alpha : V \\\\mathbb{R}\\), które\nspełniają \\[||\\alpha|| := \\sum_x |\\alpha(x)| < \\infty.\\] Macierz\n\\(\\gamma\\) definiuje operator \\(\\Gamma\\) na \\(\\ell_1(S)\\) przez\n\\[\\Gamma \\alpha(u) = \\sum_{x: x \\neq u} \\alpha(x) \\gamma(x, u).\\]\nOperator ten jest dobrze zdefiniowany ograniczony, pod warunkiem że\n\\[M := \\sup_x \\sum_{u: u \\neq x} \\gamma(x, u) < \\infty,\\] wtedy\n\\(||\\Gamma|| = M\\).Dla \\(f \\C(\\{0,1\\}^V)\\) \\(x \\S\\), niech\n\\[\\Delta f(x) = \\sup_{\\eta} \\left|f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right|.\\]\nWtedy \\(\\|f\\|_o = ||\\Delta f||_{l_1(V)}\\). Oto oszacowanie, którego\npotrzebujemy.Fakt 2  Załóżmy, że spełniony jest jeden z warunków\\(f \\D\\),\\(f \\D\\),\\(f\\) jest ciągła \n\\[\\begin{equation}\n            c(x, \\cdot) \\equiv 0\n            \\text{ dla wszystkich oprócz skończonej liczby } x \\V.\n\\tag{30}\n\\end{equation}\\]\\(f\\) jest ciągła \n\\[\\begin{equation}\n            c(x, \\cdot) \\equiv 0\n            \\text{ dla wszystkich oprócz skończonej liczby } x \\V.\n\\tag{30}\n\\end{equation}\\]Wówczas jeśli \\(f - \\lambda Lf = g \\D\\), \\(\\lambda > 0\\), oraz\n\\(\\lambda M < 1 + \\lambda \\epsilon\\), \n\\[\\begin{equation}\n        \\Delta f \\leq \\left[ (1 + \\lambda \\epsilon)- \\lambda \\Gamma \\right]^{-1} \\Delta g,\n\\tag{31}\n\\end{equation}\\]\ngdzie nierówność zachodzi współrzędna po współrzędnej, odwrotność jest\nzdefiniowana przez nieskończony szereg\n\\[\\begin{equation}\n        \\left[ (1 + \\lambda \\epsilon)- \\lambda \\Gamma \\right]^{-1} \\alpha\n        = \\frac{1}{1 + \\lambda \\epsilon} \\sum_{k=0}^{\\infty}\n        \\left( \\frac{\\lambda}{1 + \\lambda \\epsilon} \\right)^k \\Gamma^k \\alpha.\n\\tag{32}\n\\end{equation}\\]Proof. Zauważmy, że szereg w\n(32) jest\nzbieżny dla \\(\\alpha \\\\ell_1(V)\\) na mocy założenia\n\\(\\lambda M < 1 + \\lambda \\epsilon\\). Pisząc \\(f - \\lambda Lf = g\\) w\npunktach \\(\\eta\\) oraz \\(\\eta^{(u)}\\), odejmując zauważając że\n\\((\\eta^{(u)})^{(u)} = \\eta\\), otrzymujemy\n\\[\\begin{multline}\n        [f(\\eta^{(u)}) - f(\\eta)][1 + \\lambda c(u, \\eta) + \\lambda c(u, \\eta^{(u)})]\n        = [g(\\eta^{(u)}) - g(\\eta)]\\\\\n        + \\lambda \\sum_{x:x \\neq u} \\left\\{ c(x, \\eta^{(u)}) [f((\\eta^{(u)})^{(x)})\n        - f(\\eta^{(u)})] - c(x, \\eta)[f(\\eta^{(x)}) - f(\\eta)] \\right\\}.\n\\tag{33}\n\\end{multline}\\]\nPonieważ wartości \\(f(\\eta^{(u)}) - f(\\eta)\\), gdy \\(\\eta\\)\nzmienia się \\(u\\) jest ustalone, tworzą zbiór symetryczny, ta różnica\njest funkcją ciągłą \\(\\eta\\), dla każdego \\(u\\) istnieje takie \\(\\eta\\), że\n\\[f(\\eta^{(u)}) - f(\\eta) = \\sup_{\\zeta} |f(\\zeta^{(u)}) - f(\\zeta)| = \\Delta f(u).\\]\nStąd, \\[f(\\zeta^{(u)}) - f(\\zeta) \\leq f(\\eta^{(u)}) - f(\\eta)\\] dla\nkażdej \\(\\zeta\\). Stosując dla \\(\\zeta = \\eta^{(x)}\\) przekształcając,\notrzymujemy \\[f((\\eta^{(u)})^{(x)}) - f(\\eta^{(u)})\n        = f((\\eta^{(x)})^{(u)}) - f(\\eta^{(u)}) \\leq f(\\eta^{(x)}) - f(\\eta),\\]\nUżywając tej nierówności w (33),\n\\[\\begin{multline}\n        \\Delta f(u)(1 + \\lambda \\epsilon) \\leq\n        \\Delta f(u)[1 + \\lambda c(u, \\eta) + \\lambda c(u, \\eta^{(u)})] \\\\\n        \\leq \\Delta g(u) + \\lambda \\sum_{x:x \\neq u}\n        \\left[ c(x, \\eta^{(u)}) - c(x, \\eta) \\right] [f(\\eta^{(x)}) - f(\\eta)]\n        \\\\ \\leq \\Delta g(u) + \\lambda \\sum_{x:x \\neq u} \\gamma(x,u) \\Delta f(x).\n\\tag{34}    \n\\end{multline}\\]\nJeśli (30) zachodzi, tylko skończona liczba wyrazów po\nprawej stronie jest niezerowa, więc przy któregokolwiek z założeń faktu\n\\(\\Gamma \\Delta_f\\) jest dobrze określona. Dlatego (34) można\nzapisać jako\n\\[(1 + \\lambda \\epsilon) \\Delta f \\leq \\Delta g + \\lambda \\Gamma \\Delta f.\\]\nIteracja tej nierówności prowadzi \n\\[\\Delta f \\leq \\frac{1}{1 + \\lambda \\epsilon}\n        \\sum_{k=0}^{n} \\left( \\frac{\\lambda}{1 + \\lambda \\epsilon} \\right)^k\n        \\Gamma^k \\Delta g + \\left( \\frac{\\lambda}{1 + \\lambda \\epsilon} \\right)^{n+1}\n        \\Gamma^{n+1} \\Delta f.\\] Jeżeli rozważymy teraz \\(n \\\\infty\\),\ndostaniemy (31).Twierdzenie 16  Załóżmy, że \\(M < \\infty\\). Wtedy \\(\\overline{L}\\) jest\ngeneratorem infinitezymalnym półgrupy Fellera\n\\(T=(T(t))_{t \\\\mathbb{R}_+}\\). Ponadto,\n\\[\\begin{equation}\n        \\Delta T(t)f \\leq e^{-t \\epsilon} e^{t \\Gamma} \\Delta f.\n\\tag{35}\n\\end{equation}\\]\nW\nszczególności, jeśli \\(f \\D\\), \\(T_tf \\D\\) oraz\n\\[\\begin{equation}\n        \\|T(t)f\\|_o \\leq e^{(M - \\epsilon)t} \\|f\\|_o.\n\\tag{36}\n\\end{equation}\\]Proof. Własności (GI1), (GI2), (GI3) (GI5) z\nDefinicji ?? zachodzą dla \\((L, D)\\) są są dziedziczone przez\n\\(\\overline{L}\\) z Faktu 1. Aby sprawdzić warunek (GI4) weźmy wstępujący ciąg\n\\(V_n\\subseteq V\\) taki, że \\(\\bigcup_nV_n=V\\). Niech \\[\n        L_n f(\\eta) =\n        \\sum_{x \\V_n} c(x, \\eta) \\left[f\\left(\\eta^{(x)}\\right) - f(\\eta)\\right],\n        \\quad f \\C\\left(\\{0,1\\}^V\\right).\n(\\#eq:4.10)\n\\] jest generator dla\nsystemu spinowego, w którym współrzędne \\[(\\eta(x) : x \\notin V_n)\\] są\nstałe w czasie. Ponieważ \\(L_n\\) jest ograniczonym generatorem, spełnia\n\\[\\mathcal{R}(- \\lambda L_n) = C(\\{0,1\\}^V)\\] dla dostatecznie małych\n\\(\\lambda > 0\\). Dla \\(g \\D\\), możemy zdefiniować \\(f_n \\C(\\{0,1\\}^V)\\)\nprzez \\(f_n - \\lambda L_n f_n = g\\). Ponieważ \\(L_n\\) spełnia\n(30), jeśli\n\\(\\lambda\\) jest wystarczająco małe, tak że\n\\(\\lambda M < 1 + \\lambda \\epsilon\\), wtedy \\(f_n \\D\\) zgodnie z\nFaktem 2. W\nzwiązku z tym możemy położyć\n\\[g_n = f_n - \\lambda L f_n \\\\mathcal{R}(- \\lambda L).\\] Niech\n\\(K = \\sup_{x, \\eta} c(x, \\eta) <\\infty\\), wtedy z\nFaktu 2,\n\\[\\begin{multline}\n        \\|g_n - g\\|\n        = \\lambda ||(L - L_n) f_n||\n        \\leq \\lambda K \\sum_{x \\notin V_n} \\Delta f_n(x)\\\\\n        \\leq \\lambda K \\sum_{x \\notin V_n}\n        \\left[ (1 + \\lambda \\epsilon)- \\lambda \\Gamma \\right]^{-1} \\Delta g(x).\n    \\tag{37}\n\\end{multline}\\]\nPonieważ \\(\\Delta g \\\\ell_1(V)\\), prawa strona\n(37) dąży\nzera, gdy \\(n \\\\infty\\), więc \\(g_n \\g\\). Stąd\n\\(g \\\\mathrm{cl}(\\mathcal{R}(- \\lambda L))\\), więc wnioskujemy, że\n\\(D \\subseteq \\mathrm{cl}(\\mathcal{R}(- \\lambda L))\\). Ponieważ \\(D\\) jest\ngęste w \\(C(\\{0,1\\}^V)\\), widzimy, że \\(\\mathcal{R}(- \\lambda L)\\) jest\nrównież gęste. Zgodnie z\nFaktem 1,\n\\(\\mathcal{R}(- \\lambda \\overline{L})\\) musi być\ndomkniętym podzbiorem \\(C(\\{0,1\\}^V)\\). Zatem\n\\[\\mathcal{R}(- \\lambda \\overline{L}) = C(\\{0,1\\}^V)\\] kończy\nweryfikację, że \\(\\overline{L}\\) jest generatorem infinitezymalnym.Przechodząc drugiego stwierdzenia, zapiszmy\n(31) jako\n\\[\\Delta_{(- \\lambda L)^{-1}} g\n        \\leq \\left[ (1 + \\lambda \\epsilon)- \\lambda \\Gamma \\right]^{-1} \\Delta g,\\]\nnastępnie iterujmy, aby uzyskać\n\\[\\Delta_{(- \\frac{t}{n} L)^{-1}} g \\leq\n        \\left[ \\left( 1 + \\frac{t}{n} \\epsilon \\right) - \\frac{t}{n} \\Gamma \\right]^{-n}\n        \\Delta g.\\] Przechodząc granicy otrzymujemy (35).","code":""},{"path":"section-wykład-10-procesy-dualne.html","id":"section-wykład-10-procesy-dualne","chapter":"Wykład 10: Procesy dualne","heading":"Wykład 10: Procesy dualne","text":"Piotr Dyszewski12-12-2014","code":""},{"path":"section-wykład-10-procesy-dualne.html","id":"section-procesy-dualne","chapter":"Wykład 10: Procesy dualne","heading":"Procesy dualne","text":"Często zdarza się, że wartość oczekiwaną związaną z jednym skomplikowany\nprocesem jesteśmy w stanie wyrazić jako wartość oczekiwaną związaną z\ninnym procesem, który jest o wiele poostrzy. Tego typu relacje pozwalają\nna przydatne reprezentacje wielkości występujących w procesach Fellera.Definicja 1. Niech \\(X_1=(X_1(t))_{t \\\\mathbb{R}}\\) \n\\(X_2=(X_2(t))_{t \\\\mathbb{R}}\\) będą procesami Fellera odpowiednio na\nprzestrzeniach \\(S_1\\) \\(S_2\\). Dla mierzalnej ograniczonej funkcji \\(H\\)\nna \\(S_1 \\times S_2\\), procesy te są nazywane dualnymi względem \\(H\\), jeśli\n\\[\n            \\mathbf{E}_{x_1} [H(X_1(t), x_2)] = \\mathbf{E}_{x_2} [H(x_1, X_2(t))]\n\\tag{38}\n\\]\ndla każdego \\(t \\geq 0\\) oraz \\(x_i \\S_i\\).Powyższe pojęcie w zupełnej ogólności jest problematyczne. Naturalnym\njest oczekiwać, że (38) mówi coś o relacji między \\(X_1\\) oraz \\(X_2\\).\nZauważmy, że każde dwa procesy są dualne względem funkcji stałej.\nJednakże charakter relacji między \\(X_1\\) \\(X_2\\) zależy bardzo mocno od\nkonteksty podyktowany jest prze funkcję \\(H\\).Niech \\(X_a\\) oraz \\(X_r\\) będą ruchami Browna na\n\\(S_1=S_2=[0, +\\infty)\\) odpowiednio zabitymi odbitymi w zerze. Procesy\nte są dualne względem funkcji \\[H(x,y) = \\mathbf{1}_{\\{ x \\leq y \\}}.\\]\nWówczas relacja (38) zapisuje się jako\n\\[\\mathbf{P}_{x_1}[X_1(t) \\leq x_2] = \\mathbf{P}_{x_2}[x_1 \\leq X_2(t)].\\]\nW przypadku wspomnianych wersji ruchu Browna\n\\[\\mathbf{P}_{x}[X_a(t) \\leq y] = \\mathbf{P}_{y}[x \\leq X_r(t)].\\] Oba\nprawdopodobieństwa są równe\n\\[\\mathbb{P}[B_t\\geq x-y]+ \\mathbb{P}[B_t\\geq x+y],\\] gdzie\n\\(B=(B_t)_{t \\\\mathbb{R}_+}\\) jest standardowym ruchem Browna na\n\\(\\mathbb{R}\\). Dokładne sprawdzenie wspomnianej równości pozostawiamy\njako zadanie.Załóżmy teraz, że \\(H\\) jest ciągła. W świetle definicji procesy Fellera\n\\(X_1\\) oraz \\(X_2\\) z półgrupami Fellera odpowiednio\n\\(T_1=(T_1(t))_{t \\\\mathbb{R}_+}\\) oraz\n\\(T_2=(T_2(t))_{t \\\\mathbb{R}_+}\\) są \\(H\\)-dualne wtedy tylko wtedy,\ngdy \\[T_1(t)H(\\cdot, s_2)(s_1) = T_2(t)H(s_1, \\cdot) (s_2)\\] dla\nwszystkich \\(t \\geq 0\\), \\(s_1\\S_1\\) oraz \\(s_2 \\S_2\\). W przypadku, gdy\ndefiniujemy procesy Fellera przez ich opis infinitezymalny wygodniejsze\njest kryterium wyrażone w terminach generatorów.Twierdzenie 3. Niech \\(X_1\\) \\(X_2\\) będą generowane odpowiednio przez\n\\((L_1, \\mathcal{D}(L_1))\\) oraz \\((L_2, \\mathcal{D}(L_2)\\), Załóżmy, że dla\nkażdych \\(s_1\\S_1\\) oraz \\(s_2 \\S_2\\)\n\\[H(\\cdot, s_2) \\\\mathcal{D}(L_1) \\quad \\mbox{oraz}\n            \\quad H(s_1, \\cdot) \\\\mathcal{D}(L_2).\\] Jeżeli dodatkowo\n\\[L_1 H(\\cdot, s_2)(s_1) = L_2 H(s_1, \\cdot)(s_2)\\] dla wszystkich\n\\(x_1 \\S_1\\) oraz \\(x_2 \\S_2\\). Wówczas \\(X_1\\) \\(X_2\\) są dualne\nwzględem \\(H\\).Proof. Rozumowanie przeprowadzimy jedynie w przypadku przeliczalnej\n\\(S_2\\) ograniczonego \\(L_2\\). Wówczas \\(X_2\\) jest łańcuchem Markowa z\n\\(q\\)-macierzą \\(q=(q(x,y))_{x,y\\S_2}\\). Przypomnijmy, że wówczas\n\\[T_2(t)f(s_2)=\\mathbf{E}_{s_2}[f(X_2(t)] = \\sum_{y \\S_2} \\mathbf{P}_{s_2}[X_2(t) =y] f(y).\\]\nGenerator \\(L_2\\) zadany jest wówczas przez\n\\[L_2 f(s_2) = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}t} \\right|_{t=0} T_2(t)f(s_2)\n            = \\sum_{y \\S_2}\\left.\\frac{\\mathrm{d}}{\\mathrm{d}t} \\right|_{t=0}\n                \\mathbf{P}_{s_2}[X_2(t) =y] f(y)\n            =\\sum_{y \\S_2} q(s_2, y)f(y).\\] Rozważmy\n\\[\\label{eq:3-31}\n            u(t, x_1, x_2) = \\mathbf{E}_{x_1}H(X_1(t), x_2) = T_1(t) H(\\cdot, x_2)(x_1)\\]\nNa mocy Twierdzenia 10,\n\\[\\begin{multline*}\n            \\frac{\\mathrm{d}}{\\mathrm{d}t} u(t, x_1, x_2)\n            = T_1(t)L_1 H(\\cdot, x_2)(x_1) = T_1(t)L_2 H(x_1, \\cdot)(x_2) \\\\\n            = \\sum_{y\\S_2} q(x_2, y) T_1(t) H(\\cdot, y)(x_1)\n            = \\sum_y q(x_2, y) u(t, x_1, y)\n            = L_2 u(t, x_1, \\cdot)(x_2).\n\\end{multline*}\\]\nDodatkowo \\(u(0, x_1, x_2) = H(x_1, x_2)\\). Z drugiej\nstrony funkcja \\[v(t,x_1, x_2) = T_2(t)H(x_1, \\cdot)(x_2)\\] również\nspełnia \\(v(0,x_1, x_2) = H(x_1, x_2)\\). Wystarczy zatem uzasadnić\njedyność tego zagadnienia. Rozważmy w tym celu \\(h = v-u\\). Wówczas\n\\[h(t,x_1, x_2) = \\int_0^t L_2 h(s, x_1, \\cdot ) \\mathrm{d}s\\] Niech\n\\[h^*(t,x_1) = \\sup_{ s \\leq t, x_2 \\S_2} h(s,x_1, x_2).\\] Wówczas\n\\[h^*(t,x_1) \\leq \\int_0^t \\|L_2\\| h^*(s, x_1) \\mathrm{d}s.\\] Powyższa\nnierówność zwija się \\[\\frac{\\mathrm{d}}{\\mathrm{d}t}\n            \\left(e^{- \\|L_2\\|t} \\int_0^t h^*(s, x_1) \\mathrm{d}s \\right)\\leq 0.\\]\nCo po całkowaniu daje\n\\[e^{- \\|L_2\\|t} \\int_0^t h^*(s, x_1) \\mathrm{d}s\\leq 0.\\] Skoro lewa\nstrona jest niewątpliwie nieujemna, \\(h \\equiv 0\\) za co za tym idzie\n\\(u\\equiv v\\). Ostatnie równość jest równoważna z dowodzoną tezą.Przyklad 4. Niech \\(X_1\\) \\(X_2\\) będą spacerami losowymi na\n\\(\\mathbb{Z}\\) z \\(q\\)-macierzami odpowiednio\n\\[q_1(x, x+1) = \\beta, \\quad q_1(x, x-1) = \\delta\\] oraz\n\\[q_2(x, x+1) = \\delta, \\quad q_1(x, x-1) = \\beta\\]","code":""},{"path":"section-wykład-10-procesy-dualne.html","id":"section-the-voter-model","chapter":"Wykład 10: Procesy dualne","heading":"The Voter model","text":"Niech \\(V\\) będzie przeliczalnym zbiorem z topologia dyskretną. Chcemy\nmodelować proces rozwoju opinii wśród osobników reprezentowanych przez\nelementy \\(V\\). Zakładać będziemy, że w każdej chwili czasu \\(t \\geq 0\\)\nkażdy osobnik \\(x \\V\\) reprezentuje jedną z dwóch opinii\n\\(\\eta_t(x) \\\\{0,1\\}\\) na zadany temat. Załóżmy, że dane są nieujemne\nliczby \\(q(x, y)\\) dla \\(x \\neq y\\). Wielkość \\(q(x,y)\\) będzie intensywnością\nz jaką \\(x\\) przejmuje opinię \\(y\\) o ile oba osobniki reprezentują różne\nopinie. Zakładać będziemy, że\n\\[M = \\sup_{x\\V} \\sum_{u : u \\neq x} q(x, u) < \\infty.\\] Model\ngłosowania (Voter model) \\(\\eta_t\\) system spinowy z\n\\[c(x, \\eta) = \\sum_{y : \\eta(y) \\neq \\eta(x)} q(x, y).\\] Innymi słowy\njest proces Fellera generowany przez\n\\[Lf(\\eta) = \\sum_{x \\V} c(x, \\eta) \\left( f\\left(\\eta^{(x)} \\right) - f(\\eta) \\right).\\]\nTechniczne szczegóły związane z dziedziną \\(L\\) zostały przedyskutowane w\npoprzednim rozdziale. Najważniejsze jest, że z\nTwierdzenia @ref(thm:4.3) wiemy, że proces ten jest dobrze określony\n(domknięcie \\(L\\) jest generatorem infinitezymalnym).Przyklad 5. Załóżmy, że \\(V\\) jest wyposażone w strukturę grafu o\nograniczonym stopniu. Chcemy modelować przypadek w którym każdy z\nwierzchołków \\(x \\V\\) może wchodzić w interakcję jedynie ze swoimi\nbezpośrednimi sąsiadami (od nich zapożycza opinie). Rozważmy\n\\(q(x,y) = \\mathbf{1}_{x \\sim y}\\). Wówczas\n\\[M = \\sup_{x \\V} \\sum_{u: u \\neq x} q(x,u) = \\sup_{x\\V} \\mathrm{deg}(x) <\\infty.\\]Zauważmy, że model głosowania posiada dwa stany stacjonarne\n\\(\\eta \\equiv 1\\) oraz \\(\\eta \\equiv 0\\). Naszym głównym celem jest\nsprawdzenie, czy istnieją inne (nietrywialne) rozkłady stacjonarne.Aby tego dokonać posłużymy się procesem dualnym \\((\\eta_t)_t\\). Ustalmy\n\\(t \\geq 0\\) \\(x \\V\\). Skoro przy zmianach opinia w \\(x\\) jest\nzapożyczana od innych osobników, chcąc zbadać wartość \\(\\eta_t(x)\\)\nrozważmy \\(t_1\\)-moment ostatniej zmiany opinii przez \\(x\\), czyli\n\\[t_1 = \\sup_{s \\leq t} \\{ \\eta_{s-}(x) \\neq \\eta_s(x) \\}\\] Jeżeli zbiór\nczasów pod kresem górnym jest pusty, \\(x\\) nie zmienił zdania na\nodcinku czasu \\([0,t]\\), więc \\(\\eta_t(x) = \\eta_0(x)\\). W chwili \\(t_1\\), \\(x\\)\nprzyjął tę samą opinię co pewien \\(x_1\\) (co się dzieje z intensywnością\n\\(q(x, x_1)\\)), czyli \\(\\eta_t(x) = \\eta_{t_1}(x_1)\\). Chcąc ustalić wartość\n\\(\\eta_{t_1}(x_1)\\) rozważamy ostatni moment, w którym \\(x_1\\) zmienił\nopinię \\[t_2 = \\sup_{s \\leq t} \\{ \\eta_{s-}(x_1) \\neq \\eta_s(x_1) \\}\\]\nJeżeli zbiór pod kresem górnym jest pusty, \\(x_1\\) na przedziale\nczasowym \\([0,t_1]\\) nie zmienił zdania \n\\(\\eta_t(x) = \\eta_{t-1}(x_1) = \\eta_0(x_1)\\). Postępując iteracyjne\ndostajemy ciąg czasów \\(t\\geq t_1 > t_2>\\ldots >t_N\\) taki, że\n\\[\\eta_t(x) =\\eta_{t_1}(x_1) = \\ldots = \\eta_{t_N}(x_N) = \\eta_0(x_N).\\]\nPrzy czym przejście z \\(x_k\\) \\(x_{k+1}\\) dzieje się z intensywnością\n\\(q(x_k, x_{k+1}\\). Skonstruowana w ten sposób ścieżka\n\\((x, x_1, \\ldots, x_N)\\) ma taki sam rozkład jak ścieżka łańcucha Markowa\n\\(Y_x = (Y_x(t))_{t \\\\mathbb{R}_+}\\) z \\(q\\)-macierzą\n\\((q(x,y))_{x,y \\V}\\). Oznacza , że\n\\[\\eta_t(x) \\stackrel{d}{=} \\eta_0(Y_x(t)).\\] Chcąc zbadać teraz rozkład\nłączny \\((\\eta_t(x), \\eta_t(y)\\) dla \\(x,y \\V\\) możemy wykonać podobną\nkonstrukcję na podstawie zmian opinii któregokolwiek z elementów pary.\nDostaniemy w ten sposób ciąg czasów\n\\(t \\geq s_1> s_2 > \\ldots >s_M\\geq 0\\) taki, że\n\\[\\eta_t(x) =\\eta_{s_1}(x_1) = \\ldots = \\eta_{s_M}(x_M) = \\eta_0(x_M).\\]\noraz\n\\[\\eta_t(y) =\\eta_{s_1}(y_1) = \\ldots = \\eta_{s_M}(y_M) = \\eta_0(y_M).\\]\nKluczowa jest następująca własność. Jeżeli \\(x_j=y_j\\) dla pewnego\n\\(j\\geq 1\\), \\(x_k=y_k\\) dla wszystkich \\(k \\geq j\\). Istotnie, jeżeli \\(j\\)\njest najmniejszą taką liczbą, że \\(x_j=y_j\\), oznacza że \\(x_{j-1}\\)\nprzejął opinię \\(x_j\\). Po czym zanim \\(x_j\\) zmienił opinię, \\(y_{j-1}\\)\nprzejął opinię \\(x_j\\). Oznacza , że\n\\[(\\eta_t(x), \\eta_y(y)) \\stackrel{d}{=} (\\eta_0(Y_x(t)), \\eta_0(Y_y(t))),\\]\ngdzie \\(Y_x\\) \\(Y_y\\) są łańcuchami Markowa na \\(V\\) z zadaną \\(q\\)-macierzą\ntakimi, że jeżeli w pewnym momencie się spotkają, od tego momentu\nzaczynają się poruszać się razem. Podobny komentarz możemy napisać dla\nwektora \\(\\eta_x(t)\\) dowolnej długości: jego rozkład będziemy mogli\nwyrazić przez kolekcję spacerów losowych na \\(V\\), które się zlewają w\nmomencie spotkania.Powyżej znajduje się symulacja zlweających się spacerów losowych na torusie dyskretnym.\nCząsteczki poruszają się według symetrycznego spaceru losowego.Aby wprowadzić ten proces bardziej formalnie,\nrozważmy \\(S_2(N)\\) - zbiór wszystkich \\(\\subseteq V\\) o liczebności nie\nwiększej niż \\(N\\). Rozważmy \\(\\{Q(,B)\\}_{,B \\S_2}\\) dane przez\n\\[Q(, (\\setminus \\{x\\}) \\cup \\{y\\}) = q(x, y), \\quad x \\, \\, y \\notin ;\\]\noraz\n\\[Q(, \\setminus \\{x\\}) = \\sum_{y \\, y \\neq x} q(x, y), \\quad x \\.\\]\nTaki wybór instancyjności przejść odpowiada dokładnie zlewającym się\nspacerom losowym. Generator takiego procesu jest ograniczony z normą\n\\[\\sum_{B \\neq } Q(, B) = \\sum_{x \\} \\sum_{y \\neq x} q(x, y) \\leq M N.\\]\nPokażemy, że zlewające się spacery losowe \\(=(A_t)_{t \\\\mathbb{R}}\\)\n(proces o intensywnościach danych powyżej) jest dualny modelu\ngłosowania \\((\\eta_t)_{t \\\\mathbb{R}}\\) z funkcją\n\\[H(\\eta, ) = \\prod_{x \\} \\eta(x) = 1_{\\{\\eta = 1 \\text{ } \\}},\\]\nTrajektorie \\(|A_t|\\) są nierosnące, co czyni go bardzo użytecznym w\nbadaniu modelu głosowania,Twierdzenie 6. Procesy \\((\\eta_t)_t\\) \\((A_t)_t\\) są dualne względem\n\\(H(\\eta, )\\).Proof. Sprawdzimy, że zachodzą założenia\nTwierdzenia 3.\nNiech \\(L\\) będzie generatorem modelu głosowania. Ponieważ \\(H(\\eta, )\\)\nzależy od \\(\\eta\\) tylko poprzez \\(\\{\\eta(x), x \\\\}\\),\n\\[\\begin{multline*}\n        L H(\\cdot, )(\\eta)\n        = \\sum_{x \\, y \\S \\atop \\eta(y) \\neq \\eta(x)}\n            q(x, y) [H(\\eta_x, ) - H(\\eta, )]\\\\\n        = \\sum_{x \\, y \\S \\atop \\eta(y) \\neq \\eta(x)}\n            q(x, y) [1 - 2\\eta(x)] H(\\eta, \\setminus \\{x\\})\\\\\n        = \\sum_{x \\, y \\S}\n            q(x, y) H(\\eta, \\setminus \\{x\\}) [\\eta(y) - \\eta(x)]\\\\\n        = \\sum_{x \\, y \\S}\n            q(x, y) [H(\\eta, (\\setminus \\{x\\}) \\cup \\{y\\}) - H(\\eta, )]\\\\\n        = \\sum_{B} q(, B) [H(\\eta, B) - H(\\eta, )].\n\\end{multline*}\\]","code":""},{"path":"section-wykład-11-the-voter-model-przypadek-rekurencyjny.html","id":"section-wykład-11-the-voter-model-przypadek-rekurencyjny","chapter":"Wykład 11: The voter model: przypadek rekurencyjny","heading":"Wykład 11: The voter model: przypadek rekurencyjny","text":"2024-12-19Piotr DyszewskiZobaczymy teraz korzyści płynące z udowodnionej właśnie dualności.\nJeżeli \\((\\eta_t)_{t \\\\mathbb{R}_+}\\) jest układem spinowym, dla\n\\(\\subseteq V\\) oraz miary probabilistycznej \\(\\mu\\) na \\(\\{0,1\\}\\)\nprawdopodobieństwo\n\\[\\mathbf{P}_\\mu \\left[\\eta_t \\equiv 1 \\text{ na } \\right]\\] zależy od\n\\(\\mu\\) w skomplikowany sposób. W przypadku modelu głosowania, możemy\nwykorzystać proces dualny wyznaczenia wygodnej reprezentacji\n\\[\\begin{multline*}\n        \\mathbf{P}_\\mu\\left[ \\eta_t \\equiv 1 \\text{ na } \\right]\n        = \\int \\mathbf{E}_\\eta H(\\eta_t, ) \\, \\mu (\\mathrm{d}\\eta)\n        = \\int \\mathbf{E}_A H(\\eta, A_t) \\, \\mu(\\mathrm{d} )  \\\\\n        = \\int \\left[\\sum_{B} \\mathbf{P}_A[A_t = B] H(\\eta, B)\\right] \\mu(\\mathrm{d}\\eta)\n        = \\sum_{B} \\mathbf{P}_A[A_t = B] \\mu \\{\\eta : \\eta \\equiv 1 \\text{ na } B\\}.\n\\end{multline*}\\]\nDlatego rozkład łączny \\((\\eta_t(x))_{x \\}\\) zależy\ntylko od rozkładów \\(||\\) współrzędnych względem rozkładu początkowego\n\\(\\mu\\). Załóżmy teraz, że początkowy rozkład dla modelu głosowania \nmiara produktowa \\(\\nu_\\rho\\) dla \\(\\rho \\[0,1]\\) względem której\nwszystkie współrzędne są niezależne mają rozkład\n\\((1-\\rho)\\delta_0 +\\rho\\delta_1\\). Wówczas\n\\[\\nu_\\rho \\{\\eta : \\eta \\equiv 1 \\text{ na } \\} = \\rho^{||}.\\] Mamy\n\\[\\mathbf{P}_{\\nu_\\rho}[\\eta_t(x) = 1] = \\mathbf{P}_{\\nu_\\rho}[\\eta_t \\equiv 1 \\text{ na } \\{x\\} ]\n        = \\sum_{y \\V} \\mathbf{P}_{\\{x\\}} [A_t = \\{y\\}] \\rho = \\rho\\]\noraz\n\\[\\begin{multline*}\n        \\mathbf{P}_{\\nu_\\rho}[\\eta_t(x) = \\eta_t(y) = 1] =\n        \\mathbf{P}_{\\nu_\\rho}(\\eta_t \\equiv 1 \\text{ na } \\{x,y\\})\\\\\n        = \\rho \\mathbf{P}_{\\{x,y\\}}[|A_t|=1] + \\rho^2 \\mathbf{P}_{\\{x,y\\}}[|A_t|=2].\n\\end{multline*}\\]\nOpinie \\(\\eta_t(x)\\) \\(\\eta_t(y)\\) stają się zgodne w\ngranicy \\(t \\\\infty\\) wtedy tylko wtedy, gdy dwie niezależne kopie\nłańcucha spotykają się z prawdopodobieństwem 1. Tę ogólną zasadę\nzilustrujemy na przykładzie \\(V = \\mathbb{Z}^d\\).","code":""},{"path":"section-wykład-11-the-voter-model-przypadek-rekurencyjny.html","id":"section-the-voter-model-na-mathbbzd","chapter":"Wykład 11: The voter model: przypadek rekurencyjny","heading":"The Voter model na \\(\\mathbb{Z}^d\\)","text":"Naszym głównym zastosowaniem dualności będzie znalezienie wszystkich\nrozkładów stacjonarnych dla modelu głosowania. Podobna analiza może być\nużyta określenia jego asymptotycznego zachowania, gdy \\(t \\\\infty\\).\nDla uproszczenia, zakładamy od tej pory, że \\(S = \\mathbb{Z}^d\\). Zakładać\nteż będziemy, że kroki stowarzyszonego łańcucha są niezmiennicze na\nprzesunięcia, czyli przejścia z \\(x_1 \\\\mathbb{Z}^d\\) oraz przejścia z\n\\(x_2\\\\mathbb{Z}^d\\) dzieją się z takimi samymi intensywnościami.\nWówczas \\(q(x, y) = q(0, y - x)\\). Niech\n\\(X_i = (X_i(t)_{t \\\\mathbb{R}_+}\\) dla \\(\\\\mathbb{N}\\) będą\nniezależnymi błądzeniami losowymi z intensywnościami przejść \\(q(x, y)\\) \npunktami początkowymi \\(x_1, x_2, \\dots\\). Analiza dzieli się na dwa\nprzypadki, w zależności od tego, czy symetryzowane błądzenie losowe\n\\(Z(t) = X_1(t) - X_2(t)\\) jest rekurencyjne czy przejściowe, ponieważ\nodpowiada sytuacji, czy \\(X_1(t)\\) \\(X_2(t)\\) spotykają się z\nprawdopodobieństwem jeden.","code":""},{"path":"section-wykład-11-the-voter-model-przypadek-rekurencyjny.html","id":"section-przypadek-rekurencyjny","chapter":"Wykład 11: The voter model: przypadek rekurencyjny","heading":"Przypadek rekurencyjny","text":"Zaczniemy od przypadku rekurencyjnego, w którym model głosowania jest\nprostszy w analizie. Niech \\(\\delta_\\mathbf{0}\\) \\(\\delta_\\mathbf{1}\\)\nbędą miarami probabilistycznymi na układach na \\(\\mathbb{Z}^d\\) skupionymi\nna odpowiednio \\(\\eta \\equiv 0\\) oraz \\(\\eta \\equiv 1\\).Twierdzenie 17  Załóżmy, że symetryzowane błądzenie losowe \\(Z\\) jest\nrekurencyjne.Dla każdego \\(\\eta\\) oraz \\(x_1, x_2 \\\\mathbb{Z}^d\\),\n\\[\\lim_{t \\\\infty} \\mathbf{P}_\\eta[\\eta_t(x_1) \\neq \\eta_t(x_2)] = 0.\\]Dla każdego \\(\\eta\\) oraz \\(x_1, x_2 \\\\mathbb{Z}^d\\),\n\\[\\lim_{t \\\\infty} \\mathbf{P}_\\eta[\\eta_t(x_1) \\neq \\eta_t(x_2)] = 0.\\]Każda miara stacjonarna dla \\((\\eta_t)_{t \\\\mathbb{R}_+}\\) jest\npostaci \\(\\lambda \\delta_\\mathbf{1} + (1-\\lambda) \\delta_\\mathbf{0}\\)\ndla pewnej \\(\\lambda \\[0,1]\\).Każda miara stacjonarna dla \\((\\eta_t)_{t \\\\mathbb{R}_+}\\) jest\npostaci \\(\\lambda \\delta_\\mathbf{1} + (1-\\lambda) \\delta_\\mathbf{0}\\)\ndla pewnej \\(\\lambda \\[0,1]\\).Dla miary probabilistycznej \\(\\mu\\) oraz \\(\\lambda \\[0,1]\\),\n\\[\\begin{equation}\n                    \\lim_{t \\\\infty} \\mathbf{P}_\\mu[(\\eta_t(x_j))_{j \\leq n}\n                    = (\\epsilon_j)_{j \\leq n}]\n                    = \\lambda \\mathbf{1}_{\\{ \\epsilon_1=\\ldots =\\epsilon_n=1\\}}\n                    + (1 - \\lambda)\\mathbf{1}_{\\{\\epsilon_1=\\ldots = \\epsilon_n=0\\}}\n\\tag{39}\n\\end{equation}\\]\ndla każdych \\(x_j\\) \\(\\epsilon_j\\), wtedy tylko wtedy, gdy\n\\[\\begin{equation}\n                        \\lim_{t \\\\infty} \\sum_y p_t(x, y) \\mu \\{\\eta : \\eta(y) = 1\\}\n                    = \\lambda \\text{ dla każdego } x \\\\mathbb{Z}^d,\n\\tag{40}\n\\end{equation}\\]\ngdzie \\(p_t(x, y)\\) są prawdopodobieństwami przejścia dla \\(X_i(t)\\).Dla miary probabilistycznej \\(\\mu\\) oraz \\(\\lambda \\[0,1]\\),\n\\[\\begin{equation}\n                    \\lim_{t \\\\infty} \\mathbf{P}_\\mu[(\\eta_t(x_j))_{j \\leq n}\n                    = (\\epsilon_j)_{j \\leq n}]\n                    = \\lambda \\mathbf{1}_{\\{ \\epsilon_1=\\ldots =\\epsilon_n=1\\}}\n                    + (1 - \\lambda)\\mathbf{1}_{\\{\\epsilon_1=\\ldots = \\epsilon_n=0\\}}\n\\tag{39}\n\\end{equation}\\]\ndla każdych \\(x_j\\) \\(\\epsilon_j\\), wtedy tylko wtedy, gdy\n\\[\\begin{equation}\n                        \\lim_{t \\\\infty} \\sum_y p_t(x, y) \\mu \\{\\eta : \\eta(y) = 1\\}\n                    = \\lambda \\text{ dla każdego } x \\\\mathbb{Z}^d,\n\\tag{40}\n\\end{equation}\\]\ngdzie \\(p_t(x, y)\\) są prawdopodobieństwami przejścia dla \\(X_i(t)\\).Punkt () mówi, że dla dowolnego skończonego \\(\\subseteq \\mathbb{Z}^d\\)\nopinie \\((\\eta_t(x))_{x \\}\\) stają się zgodne z bardzo dużym\nprawdopodobieństwem. Oznacza , że jedyne możliwe układy graniczne są\nstałe (punkt (b)). Prawdopodobieństwa wystąpienia w granicy\nposzczególnych układów stałych są scharakteryzowane w punkcie (c). W\nszczególności pierwsza granica w punkcie (c) istnieje wtedy tylko\nwtedy, gdy istnieje druga.\nWarunek (39) jest równoważny\n\\[\\lim_{t \\\\infty} \\mathbf{P}_\\mu[\\eta_t \\\\cdot ]\n        = \\lambda \\delta_\\mathbf{1}(\\cdot)\n        + (1 - \\lambda)\\delta_\\mathbf{0}(\\cdot)\\] w sensie słabej\nzbieżności miar. Dokładniej dla każdej\n\\(f \\C(\\{ 0,1\\}^{\\mathbb{Z}^d}\\),\n\\[\\lim_{t \\\\infty} \\mathbf{E}_{\\mu}[f(\\eta_t)] = \\lambda f(\\mathbf{1}) +\n        (1-\\lambda)f(\\mathbf{0}).\\] Dzieje się tak, ponieważ topologia\nsłabej zbieżności na przeliczalnym produkcie jest scharakteryzowana\nprzez zbieżność rozkładów skończenie wymiarowych.Proof. Aby uzasadnić punkt () rozważmy\n\\[\\tau = \\inf\\{t > 0 : X_1(t) = X_2(t)\\} = \\inf\\{t > 0 : Z(t) = 0\\}.\\]\nWówczas dla dowolnej konfiguracji początkowej \\(\\eta\\),\n\\[\\mathbf{P}_\\eta[\\eta_t(x_1) \\neq \\eta_t(x_2)]\n        = \\mathbb{P}[\\eta(X_1(t)) \\neq \\eta(X_2(t))), \\tau > t]\n        \\leq \\mathbb{P}[\\tau > t],\\] co zbiega zera, gdy\n\\(t \\\\infty\\), z założenia rekurencyjności \\(Z\\).Dla części (b), weźmy \\(\\pi\\) stacjonarną dla\n\\((\\eta_t)_{t\\\\mathbb{R}}\\). Wtedy\n\\[\\pi [\\eta : \\eta(x_1) \\neq \\eta(x_2)] = \\mathbf{P}_\\pi[ \\eta(x_1) \\neq \\eta(x_2)]\n        = \\int \\mathbf{P}_\\eta[\\eta_t(x_1) \\neq \\eta_t(x_2)] \\mu (\\mathrm{d} \\eta),\\]\nco zbiega zera, gdy \\(t \\\\infty\\), z części (). Dlatego \\(\\eta\\) jest\nstała \\(\\pi\\)-prawie wszędzie. Dla dowodu części (c) weźmy dowolny rozkład\npoczątkowy \\(\\mu\\). Chcąc pokazać, że rozkład graniczny jest stały\nwystarczy porównać go z dowolnym ustalonym punktem. Dla\n\\(x_1 \\\\mathbb{Z}^d\\),\n\\[\\begin{multline}\n        \\mathbf{P}_\\mu[\\eta_t(x_1) = 1] =\n        \\int \\mathbf{P}_\\eta[\\eta_t(x_1) = 1] \\, \\mu(\\mathrm{d}\\eta) = \\\\\n        \\int \\mathbb{P}[\\eta(X_1(t)) = 1] \\, \\mu (\\mathrm{d}\\eta)\n        = \\sum_y p_t(x_1, y) \\mu (\\eta : \\eta(y) = 1).\n\\end{multline}\\]\nDla dowolnego skończonego \\(\\subseteq \\mathbb{Z}^d\\)\ntakiego, że \\(x_1 \\\\),\n\\[\\mathbf{P}_\\mu [\\eta_t(x_1) = 1] - \\mathbf{P}_{\\mu}[\\eta_t \\equiv  1 \\text{ na } ] =\n        \\int \\mathbf{P}_\\eta [\\eta_t(x_1) = 1] -\n        \\mathbf{P}_{\\eta}[\\eta_t \\equiv  1 \\text{ na } ] \\mu(\\mathrm{d}\\eta).\\]\nCo z części () zbiega zera. Mamy zatem\n\\[\\sum_y p_t(x_1, y) \\mu (\\eta : \\eta(y) = 1) = o(1) +\n        \\mathbf{P}_{\\mu}[\\eta_t \\equiv  1 \\text{ na } ].\\]\nStąd (39) oraz (40) istotnie są równoważne.Remark. Biorąc pod uwagę stwierdzenie z części (), można\nprzypuszczać, że \\(\\eta_t(x)\\) zmienia wartości tylko skończoną liczbę\nrazy dla każdego \\(x\\). Nie zawsze jest prawdą. Załóżmy na przykład, że\n\\(d=1\\), \\(q(x, y) = 1\\) jeśli \\(|x - y| = 1\\), \\(q(x, y) = 0\\) jeśli\n\\(|x - y| > 1\\). Jeśli początkowa konfiguracja ma formę\n\\(\\dots 1 1 1 0 0 0 \\dots\\), konfiguracja w czasie \\(t\\) ma ten sam tym,\nz dokładnością lokalizacji najbardziej wysuniętej na prawo jedynki,\nktóra porusz się zgodnie z prostym symetrycznym spacerem losowym na\n\\(\\mathbb{Z}\\). Ponieważ błądzenie losowe jest rekurencyjne,\n\\(\\eta_t(x)\\) zmienia wartości nieskończoną liczbę razy.Symulacja modelu głosowania w jednym wymiarze. Warunek początkowy jest postaci 11110000. W momencie, w którym cały zakres staje się monochromatyczny\njest automatycznie rozszerzany na prawo o 0000 lewo o 111111.","code":""},{"path":"section-wykład-12-the-voter-model-przypadek-tranzytywny.html","id":"section-wykład-12-the-voter-model-przypadek-tranzytywny","chapter":"Wykład 12: The voter model: przypadek tranzytywny","heading":"Wykład 12: The voter model: przypadek tranzytywny","text":"2025-01-16Piotr DyszewskiOd tej pory zakładać będziemy, że symetryzowane błądzenie losowe \\(Z(t)\\)\njest chwilowe. W tym przypadku można łatwo użyć dualności \nskonstruowania jednoparametrowej rodziny nietrywialnych rozkładów\nstacjonarnych. Pokażemy, że wszystkie rozkłady stacjonarne leżą w\notoczce wypukłej wspomnianej rodziny jednoparametrowej.Zacznijmy od miary produktowej \\(\\nu_\\rho\\) na \\(\\{0,1\\}^V\\) o gęstości\n\\(\\rho\\), dokładniej, dla \\(\\rho \\[0,1]\\),\n\\[\\nu_\\rho = \\bigotimes_{x \\V} ((1-\\rho)\\delta_0+\\rho\\delta_1).\\]\nPrzypomnijmy, że zmienne losowe \\(\\eta \\mapsto \\eta(x)\\) dla \\(x \\V\\) na\nprzestrzeni probabilistycznej\n\\((\\{0,1\\}^V, \\mathcal{B}(\\{0,1\\}^V), \\nu_\\rho)\\) są iid z rozkładem\n\\[\\nu_\\rho(\\eta \\: : \\: \\eta(x) =1) = 1-\\nu_\\rho (\\eta \\: : \\: \\eta(x)=0) = \\rho.\\]\nWykorzystując dualność między modelem głosowania (voter model)\n\\((\\eta_t)_{t \\\\mathbb{R}_+}\\), zlewającymi się spacerami losowymi\n\\((A_t)_{t \\\\mathbb{R}_+}\\) dla dowolnego skończonego \\(\\subset V\\)\nmamy\n\\[\\begin{multline}\n    \\mathbf{P}_{\\nu_\\rho}[ \\eta_t \\equiv 1 \\text{ na } ]\n    = \\int \\mathbf{P}_\\eta(\\eta_t \\equiv 1 \\text{ na } ) \\nu_\\rho(\\mathrm{d}\\eta)\n    = \\int \\mathbf{P}_A(\\eta \\equiv 1 \\text{ na } A_t)  \\nu_\\rho(\\mathrm{d}\\eta)\\\\\n    = \\sum_B \\mathbf{P}_A(A_t = B) \\int 1_{\\{\\eta \\equiv 1 \\text{ na } B\\}} \\nu_\\rho(\\mathrm{d}\\eta)\n    = \\mathbf{E}_A \\rho^{|A_t|}\n\\tag{41}\n\\end{multline}\\]\nPonieważ \\(|A_t|\\) jest nierosnące w czasie \\(t\\), \n\\[a_\\infty = \\lim_{t \\\\infty}|A_t|\\] jest dobrze określone.\nPrzechodząc granicy w (41) widzimy, że\n\\[\\lim_{t \\\\infty} \\mathbf{P}_{\\nu_\\rho} \\left[\\eta_t \\equiv 1 \\mbox{ na } \\right]\n    = \\mathbf{E}_{}\\left[ \\rho^{a_\\infty} \\right].\\] Skoro zbieżność\nrozkładów skończenie wymiarowych charakteryzuje słabą zbieżność na\n\\(\\{0,1\\}^V\\) \n\\[\\begin{equation}\n    \\lim_{t \\\\infty} \\mathbf{P}_{\\nu_\\rho} \\left[ \\eta_t \\\\cdot \\right] = \\mu_\\rho(\\cdot)\n\\tag{42}\n\\end{equation}\\]\nsłabo dla pewnej miary \\(\\mu_\\rho\\) na \\(\\{0,1\\}^V\\) takiej, że\n\\[\\begin{equation}\n    \\mu_\\rho \\{\\eta : \\eta \\equiv 1 \\text{ na } \\} = \\mathbf{E}_A \\left[ \\rho^{a_\\infty} \\right]\n\\tag{43}\n\\end{equation}\\]\nPonieważ \\(\\mu_\\rho\\) jest słabą granicą procesu Fellera (z rozkładem\npoczątkowym \\(\\nu_\\rho\\)), jest ona dla niego stacjonarna (zadanie).\nRówność (43) całkowicie charakteryzuje \\(\\mu_\\rho\\) ale nie mówi\nwiele o jej własnościach. Przyjrzyjmy się im teraz nieco dokładniej.\nRozważając (43) dla \\(\\) będącego singletonem otrzymujemy\n\\[\\begin{equation}\n    \\mu_\\rho \\{\\eta : \\eta(x)=1\\} = \\mathbf{E}_{\\{x\\}} \\left[ \\rho^{a_\\infty} \\right]= \\rho\n\\tag{43}\n\\end{equation}\\]\ndla każdego \\(x \\V\\). W szczególności wartość oczekiwana zmiennej\nlosowej \\(\\eta \\mapsto \\eta(x)\\) względem prawdopodobieństwa \\(\\mu_\\rho\\)\nwynosi \\[\\mathbb{E}_{\\mu_\\rho} \\left[\\eta(x) \\right] =\n    \\int_{\\{ 0,1\\}^V} \\eta(x) \\: \\mu_\\rho(\\mathrm{d} \\eta) =\\rho.\\] Dla\n\\(x,y \\V\\) rozważmy kowariancję zmiennych losowych\n\\(\\eta \\mapsto \\eta(x)\\) oraz \\(\\eta \\mapsto \\eta(y)\\) względem miary\n\\(\\mu_\\rho\\),\n\\[\\begin{multline}\n    \\text{Cov}_{\\mu_\\rho}(\\eta(x), \\eta(y))  =\n    \\mathbb{E}_{\\mu_\\rho} [\\eta(x) \\eta(y)] -  \\mathbb{E}_{\\mu_\\rho}[\\eta(x)]\n    \\mathbb{E}_{\\mu_\\rho}[\\eta(y)] \\\\\n    = \\mu_\\rho \\{\\eta : \\eta(x) = \\eta(y) = 1\\} - \\rho^2\n    = \\mathbf{E}_{\\{x,y\\}}\\left[\\rho^{a_\\infty} - \\rho^2\\right]\\\\\n    = \\rho(1 - \\rho) \\mathbf{P}_{\\{x,y\\}}(a_\\infty = 1)\n    = \\rho(1 - \\rho) \\mathbb{P}_{x - y}(Z(t) = 0 \\text{ dla pewnego } t \\geq 0)\n\\end{multline}\\]\nAby poradzić sobie z ostatnim prawdopodobieństwem\nwykorzystamy funkcję Greena\n\\[G(x, y) = \\int_0^\\infty \\mathbb{P}_x[Z(t) = y] \\mathrm{d} t\n    = \\mathbb{E}_x \\left[\\int_0^\\infty \\mathbf{1}_{\\{ Z(t)=y \\}} \\mathrm{d}t \\right].\\]\nJest średni czas jaki spacer zapoczątkowany w \\(x\\) spędza w punkcie\n\\(y\\). Stosując mocną własność Markowa \n\\[\\tau_y = \\inf \\{ t \\geq 0 \\: : \\; Z_t=y \\}\\] czasu pierwszej wizyty w\n\\(y\\) otrzymujemy\n\\[\\begin{equation}\n    G(x,y) = \\mathbb{P}_{x}[\\tau_y<\\infty] G(y,y) = \\mathbb{P}_x[\\tau_y<\\infty] G(0,0),\n\\tag{44}\n\\end{equation}\\]\ngdzie ostatnia równość wynika z niezmienniczości na przesunięcia. Skoro\n\\[\\mathbb{P}_x [\\tau_y< \\infty] =\n    \\mathbb{P}_{x - y}(Z(t) = 0 \\text{ dla pewnego } t \\geq 0)\\] możemy\nstwierdzić, że badana przez nas kowariancja wynosi\n\\[\\text{Cov}_{\\mu_\\rho}(\\eta(x), \\eta(y))  =\n     \\rho(1 - \\rho) \\frac{G(x, y)}{G(0, 0)}.\\]Fakt 3  Załóżmy, że \\(Z = (Z_t)_{t \\\\mathbb{R}_+}\\) jest\ntranzytywny. Wówczas dla każdego \\(x \\V\\),\n\\[\\lim_{|y|\\\\infty}G(x,y) = \\lim_{|y|\\\\infty} \\mathbb{P}_x[\\tau_y <\\infty] =0.\\]Proof. Jeżeli uzasadnimy drugą postulowaną równość, pierwsza\nwyniknie z (44). Z niezmienniczości na przesunięcia\n\\[\\mathbb{P}_x[\\tau_y <\\infty] = \\mathbb{P}_0[\\tau_{y-x}<\\infty].\\]\nMożemy zatem bez zmniejszania ogólności założyć, że \\(x=0\\). Dla\n\\(t \\\\mathbb{R}_+\\) mamy \\[\\mathbb{P}_0[\\tau_y<\\infty] =\n        \\mathbb{P}_0[\\tau_y \\leq t] +\n        \\mathbb{P}_0[t<\\tau_y<\\infty].\\] Pokażemy, że\n\\[\\lim_{t \\\\infty} \\sup_{y \\V}\\mathbb{P}[t < \\tau_y<\\infty]=0.\\]\nRozważmy \\(\\tau_y^t = \\inf\\{ s >t \\: : \\: Z(s)=y\\}\\). Argumentując tak\nsamo jak w przypadku (44),\n\\[\\begin{multline}\n        \\int_t^{\\infty} \\mathbb{P}_0[Z_s=y] \\mathrm{d}s=\n        \\mathbb{E}_0 \\left[ \\int_t^\\infty \\mathbf{1}_{\\{Z(s) = y\\}} \\mathrm{d}s \\right] \\\\\n        = \\mathbb{P}_0\\left( \\tau_y^t<\\infty\\right) G(y, y)\n        = \\mathbb{P}_0\\left( \\tau_y^t<\\infty\\right) G(0, 0).\n\\tag{45}\n\\end{multline}\\]\nLewa strona jest maksymalizowana, gdy \\(y=0\\) ponieważ\njest tak dla funkcji przejścia. Istotnie, wykorzystując równania\nChapmanna-Kołmogorowa, nierówność Cauchy’ego-Schwarza oraz symetryczność\nspaceru otrzymujemy\n\\[\\begin{multline}\n        \\mathbb{P}_0(Z(2t) = y) =\n        \\sum_{z \\V} \\mathbb{P}_0(Z(t) = z)\\mathbb{P}_z(Z(t) = y) \\\\\n        \\leq \\left( \\sum_{z \\V} \\mathbb{P}_0[Z(t) = z]^2 \\right)^{\\frac{1}{2}}\n        \\left( \\sum_{z \\V} \\mathbb{P}_z[Z(t) = y]^2 \\right)^{\\frac{1}{2}} \\\\\n        = \\mathbb{P}_0(Z(2t) = 0]^{\\frac{1}{2}} \\mathbb{P}_y(Z(2t) = y]^{\\frac{1}{2}}\n        = \\mathbb{P}_0[Z(2t) = 0].\n\\end{multline}\\]\nSkoro (45) rzeczywiście jest maksymalizowana dla \\(y=0\\), \n\\[\\sup_{y \\V} \\mathbb{P}_0[\\tau_y^t<\\infty] = \\mathbb{P}_0[\\tau_0^t <\\infty].\\]\nStąd \\[\\sup_{y \\V}\\mathbb{P}_0[t < \\tau_y<\\infty] \\leq\n        \\sup_{y \\V}\\mathbb{P}_0[\\tau_y^{t}<\\infty]=\n        \\mathbb{P}_0[\\tau_0^{t}<\\infty] \\0,\\] gdzie ostatnia\nzbieżność wynika z tranzytywności \\(Z\\). Wystarczy zatem pokazać, że dla\nkażdego \\(t \\\\mathbb{R}_+\\),\n\\[\\lim_{|y| \\\\infty}\\mathbb{P}_0[\\tau_y \\leq t] =0.\\] Z mocnaj\nwłasności Markowa\n\\[\\mathbb{P}_0(Z(\\tau_y + s) = x \\mid \\mathcal{F}_{\\tau_y}) =\n        \\mathbb{P}_y(Z(s) = y) \\geq e^{-2c(y)s} \\geq e^{-2Ms}.\\] gdzie\n\\(M\\) jest zdefiniowane w (??).\nNierówność wynika z faktu, że czas spędzony w\npunkcie \\(x\\) jest rozkładany wykładniczo z parametrem \\(\\leq 2M\\). Mnożąc\nprzez indykator zdarzenia \\(\\{\\tau_x \\leq t\\}\\) biorąc wartości\noczekiwane całkując po \\(s \\[0,1]\\) otrzymujemy\n\\[\\int_0^1 \\mathbb{P}_0(Z(\\tau_y + s) = x, \\tau_x \\leq t) \\mathrm{d}s\n        \\geq \\mathbb{P}_0(\\tau_y \\leq t) \\int_0^1 e^{-2Ms} \\mathrm{d}s.\\]\nLewa strona co najwyżej\n\\[\\mathbb{E}_0 \\left[ \\int_0^{t+1} 1_{\\{Z(s) = y\\}} \\mathrm{d}s \\right].\\]\nSumując po wszystkich \\(y \\V\\) dostajemy\n\\[\\sum_y \\mathbb{P}_0(\\tau_y \\leq t) \\leq \\frac{2M(t + 1)}{1 - e^{-2M}} <\\infty.\\] Zadanie 1. Pokaż, że\n\\[\\mu_\\rho( \\eta(x) =1 | \\eta(y)=1) = (1-\\rho) \\frac{G(x,y)}{G(0,0)} + \\rho.\\]Zauważmy, że jeżeli \\(x \\V\\) jest ustalone \\(|y| \\\\infty\\) \n\\[\\lim_{t \\\\infty}\\text{Cov}_{\\mu_\\rho}(\\eta(x), \\eta(y))  = 0\\]\nponieważ \\(\\mathbb{P}_x[\\tau_y<\\infty] \\0\\). Rachunek z zadania daje\nwówczas\n\\[\\lim_{t \\\\infty}\\mu_\\rho(\\eta(x)= 1 \\: | \\: \\eta(y)=1 )  = \\rho = \\mu_\\rho(\\eta(y) =1).\\]\nCzyli dla dużych wartości \\(|y|\\),\n\\[\\mu_\\rho(\\eta(x)=\\eta(y)=1) \\approx \\mu_{\\rho}(\\eta(x)=1) \\mu_{\\rho}(\\eta(y)=1)\\]\nInnymi słowy \\(\\eta(x)\\) \\(\\eta(y\\) są asymptotycznie niezależne przy\n\\(|x-y| \\\\infty\\).Definicja 13  Niech \\(\\mu\\) będzie niezmienniczą względem translacji\nmiarą prawdopodobieństwa na \\(\\{0, 1\\}^V\\). Mówimy, że \\(\\mu\\) jest\nmieszająca, jeśli spełnia\n\\[\\lim_{x \\\\infty} \\mu \\{\\eta : \\eta \\equiv 1 \\text{ na } \\cup (B + x)\\} =\n        \\mu \\{\\eta : \\eta \\equiv 1 \\text{ na } \\}\n        \\mu \\{\\eta : \\eta \\equiv 1 \\text{ na } B\\}\\] dla dowolnych\nskończonych \\(, B \\subset S\\).Twierdzenie 18  Dla każdego \\(\\rho \\[0,1]\\) miara \\(\\mu_\\rho\\) jest\nmieszająca.Aby udowodnić powyższe twierdzenie będziemy potrzebować własności\nprawdopodobieństwa kolapsu\n\\[\\begin{equation}\ng() = \\mathbf{P}_A(|A_t| < || \\text{ dla pewnego } t > 0).\n\\tag{46}\n\\end{equation}\\]\nWówczas\njeśli \\(\\subset B\\), \\(g() \\leq g(B)\\). Dodatkowo jeżeli \\(|| \\geq 2\\),\n\\[g() \\leq \\sum_{B \\subset , |B| = 2} g(B).\\] Zauważmy, że\n\\[g(\\{0,x\\}) = \\mathbb{P}_0[\\tau_x<\\infty].\\] Stąd\n\\[\\lim_{x \\\\infty} g(\\{0, x\\}) = 0\\] Oznacza , że jeżeli \\(\\) jest\nskończonym zbiorem, który jest rozstrzelony (każde dwa jego elementy są\nod siebie odległe), \\(g()\\) jest bliskie zero.Proof. (Twierdzenia 18)\nZauważmy, że \\(\\nu_\\rho\\) jest niezmiennicza na\nprzesunięcia dla każdego \\(\\rho \\[0,1]\\). Stąd niezmiennicza są również\n\\(\\mathbf{P}_{\\nu_\\rho}[\\eta_t \\\\cdot]\\). Wykorzystując\nzbieżność (42) wnioskujemy niezmienniczość \\(\\mu_\\rho\\).Aby sprawdzić warunek mieszania rozważmy dwie niezależne kopie\nzlewających się spacerów losowych \\((A_t)_{t \\\\mathbb{R}_+}\\) \n\\((B_t)_{t \\\\mathbb{R}_+}\\) zapoczątkowanych odpowiednio w \\(\\) \\(B\\).\nRozważmy \\(\\tau = \\inf \\{ s >0 \\: : \\: A_s \\cap B_s \\neq \\emptyset\\}\\).\nRozważmy \\((C_t)\\) dane przez \\[C_t = A_t \\cup B_t \\qquad t \\leq \\tau.\\]\nPo czasie \\(\\tau\\) niech \\(C_t\\) rozwija się jak zlewający się spacer losowy\nzapoczątkowany w \\(C_\\tau\\) niezależnie od \\(A_t\\) \\(B_t\\). Wówczas \\(C_t\\)\njest zlewającym się spacerem losowym zapoczątkowanym w \\(\\cup B\\). Na\nmocy (43),\n\\[\\begin{multline}\n    |\\mu_\\rho \\{\\eta : \\eta \\equiv 1 \\text{ na } \\cup B\\} -\n    \\mu_\\rho \\{\\eta : \\eta \\equiv 1 \\text{ na } \\}\n    \\mu_\\rho \\{\\eta : \\eta \\equiv 1 \\text{ na } B\\}| \\\\\n    = |\\mathbb{E}\\rho^{c_\\infty} - \\mathbb{E}\\rho^{a_\\infty + b_\\infty}|\n    \\leq \\mathbb{P}(\\tau < \\infty) \\leq \\sum_{u \\, v \\B} g(\\{u, v\\}).\n\\end{multline}\\]\ngdzie \\(a_\\infty\\), \\(b_\\infty\\) \\(c_\\infty\\) granice\nodpowiednio \\(|A_t|\\), \\(|B_t|\\) oraz \\(|C_t|\\) przy \\(t \\\\infty\\).\nZastępując teraz \\(B\\) przez \\(B + x\\) przechodząc z \\(|x| \\\\infty\\)\ndostajemy tezę. Przypomnijmy, że przez \\(\\{X_j(t)\\}_j\\) oznaczamy niezależne spacery\nlosowe na \\(V = \\mathbb{Z}^d\\) o intensywnościach \\(q(x,y)\\).Fakt 4  Niech \\(g\\) będzie funkcją zbioru daną\nprzez (46). WówczasDla każdego \\(k\\), \\[\\lim_{t \\\\infty}\n                g(\\{X_1(t), \\dots, X_k(t)\\}) = 0 \\quad p.w.\\]Dla każdego \\(k\\), \\[\\lim_{t \\\\infty}\n                g(\\{X_1(t), \\dots, X_k(t)\\}) = 0 \\quad p.w.\\]Dla każdego stanu początkowego \\(A_0\\),\n\\[\\lim_{t \\\\infty} g(A_t) = 0 \\quad p.w.\\]Dla każdego stanu początkowego \\(A_0\\),\n\\[\\lim_{t \\\\infty} g(A_t) = 0 \\quad p.w.\\]Proof. Skoro symetryzowany spacer losowy jest tranzytywny, \n\\[\\lim_{t \\\\infty} Z(t) = \\infty \\quad p.w.\\] dla dowolnego punktu\nstartowego. Korzystając zatem z własności prawdopodobieństwa kolapsu\n\\[\\begin{multline}\n    g(\\{X_1(t), \\dots, X_k(t)\\}) \\leq\n        \\sum_{1 \\leq < j \\leq k} g(\\{X_i(t), X_j(t)\\})\\\\\n        = \\sum_{1 \\leq < j \\leq k} g(\\{0, X_j(t) - X_i(t)\\}).\n\\end{multline}\\] Prawa\nstrona zbiega zera p.w.Aby uzasadnić drugi postulat zauważmy, że jeśli \\(|A_0| = k\\), możemy\nokreślić \\(A_t\\) \\(\\{X_1(t), \\dots, X_k(t)\\}\\) na tej samej przestrzeni\nprobabilistycznej, tak aby \\(A_t \\subset \\{X_1(t), \\dots, X_k(t)\\}\\) dla\nwszystkich \\(t\\). Można zrobić, używając tych samych czasów przejść dla\nobu procesów, ale przy każdym zlaniu zachowując tylko jedną z\nkoalizujących cząstek w \\(A_t\\). Wobec monotoniczności \\(g\\),\n\\[g(A_t) \\leq g(\\{X_1(t), \\dots, X_k(t)\\}).\\] Zastosowanie pierwszej\nczęści faktu kończy dowód. Lemma 2  Ciąg \\(\\{c_k\\}_{k \\\\mathbb{N}}\\) jest ciągiem momentów\npewnej miary probabilistycznej na \\([0,1]\\) wtedy tylko wtedy, gdy\n\\(c_0=1\\) oraz \\[\\sum_{k=0}^n {n \\choose k}(-1)^k c_{m+k} \\geq 0\\] dla\nwszystkich naturalnych \\(n, m\\).Proof. Załóżmy najpierw, że \\(c_k\\) są momentami rozkładu pewnej\nzmiennej losowej \\(\\xi\\), dokładniej\n\\[c_k = \\mathbb{E}\\left[\\xi^k\\right]\\] dla każdego \\(k \\\\mathbb{N}\\).\nWówczas \\[\\sum_{k=0}^n {n \\choose k}(-1)^k c_{m+k} =\n        \\mathbb{E} \\left[\\xi^n(1-\\xi)^m \\right] \\geq 0.\\] Dowód\nprzeciwnej implikacji jest bardziej złożony. Można go znaleźć w Thomas\nM. Liggett, Continuous time Markov Processes. Introduction, Theorem\n.32.Twierdzenie 19  Zbiór wszystkich rozkładów stacjonarnych jest\ndomkniętą otoczką wypukłą \\(\\{\\mu_\\rho : 0 \\leq \\rho \\leq 1\\}\\).\nDokładniej, dla każdej miary stacjonarnej \\(\\pi\\) istnieje miara\nprobabilistyczna \\(\\gamma\\) na \\([0,1]\\) taka, że\n\\[\\pi(\\cdot) = \\int_0^1 \\mu_\\rho(\\cdot) \\mathrm{d}\\rho.\\]Proof. Niech \\(\\pi\\) będzie rozkładem stacjonarnym. Przypomnijmy, że\n\\[\\label{eq:4.28}\n        \\pi \\{\\eta : \\eta \\equiv 1 \\text{ na } \\} =\n        \\mathbf{P}_\\pi[\\eta_t \\equiv 1 \\text{ na } ]\n        = \\sum_B \\mathbf{P}_A(A_t = B) \\pi \\{\\eta : \\eta \\equiv 1 \\text{ na } B\\}.\n\\tag{47}\n\\]\nNiech \\(h() = \\pi \\{\\eta : \\eta \\equiv 1 \\text{ na } \\}\\) niech\n\\((V_t)\\), będzie półgrupą Fellera związaną z \\(A_t\\):\n\\(V_tf() = \\mathbf{E}_A f(A_t)\\) dla \\(f\\) określonej na skończonych\npodzbiorach \\(V\\). Wówczas (47) zapisuje się jako \\(h=V_th\\).\nNiech \\(U_t\\) będzie\npółgrupą Fellera związaną z \\(n\\)-niezaleznymi spacerami losowymi na \\(V\\):\n\\[U_td(x_1, \\dots, x_n) = \\mathbb{E}_{x_1, \\dots, x_n} d(X_1(t), \\dots, X_n(t)).\\]\nOperator \\(U_t\\) możemy rozszerzyć funkcji na zbiorach poprzez\n\\[U_tf \\{ x_1, \\ldots , x_n\\} = \\mathbb{E}_{x_1, \\dots, x_n} f(\\{X_1(t), \\dots, X_n(t)\\}).\\]\nMamy \\[\\label{eq:4.29}\n        |V_tf() - U_tf()| \\leq g().\\] Stosując \\(h\\), otrzymujemy\n\\[\\label{eq:4.30}\n        |h() - U_th()| \\leq g().\n\\tag{48}\n\\] Nakładając \\(U_s\\) korzystając z\njego monotoniczności liniowości\n\\[|U_sh() - U_{t+s}h()| \\leq U_sg().\\] Prawa strona zbiega zera,\ngdy \\(s \\\\infty\\) zgodnie z\nFaktem 4. Stąd \\(\\lim_{s \\\\infty} U_sh()\\) istnieje \njest harmoniczna dla nieredukowalnego spaceru losowego\n\\((X_1(t), \\dots, X_n(t))\\) na \\(S^n = (Z^d)^n\\). Takie funkcje harmoniczne\nsą stałe, więc wnioskujemy, że istnieją stałe \\(c(n)\\), takie że\n\\[\\lim_{s \\\\infty} U_sh() = c(||)\\] dla dowolnego \\(\\). Oczywiście\n\\(c(n)\\) zależy od \\(h\\), tym samym od miary stacjonarnej \\(\\pi\\).\nPrzechodząc granicy w (48), wnioskujemy, że\n\\[\\begin{equation}\\label{eq:5:4.31}\n        |h() - c(||)| \\leq g().\n\\tag{49}\n\\end{equation}\\] Jeżeli zbiór \\(\\) jest rozstrzelony,\n\\(h()\\) jest bliskie \\(c(||)\\). Jeżeli\n\\[\\pi(\\cdot) = \\int_0^1 \\mu_\\rho(\\cdot) \\gamma(\\mathrm{d}\\rho)\\] dla\npewnej \\(\\gamma\\), dla rozstrzelonych \\(\\),\n\\[c(||) \\approx h() = \\pi (\\eta \\equiv 1 \\mbox{ na } ) =\n        \\int_0^1 \\mu_\\rho( \\eta \\equiv 1 \\mbox{ na } ) \\gamma(\\mathrm{d}\\rho).\\]\nJeżeli \\(\\) jest rozstrzelone, skoro \\(\\mu_\\rho\\) jest mieszająca\n\\[\\mu_\\rho( \\eta \\equiv 1 \\mbox{ na } ) \\approx \\rho^{||}.\\] Oznacza\n, że szukana przez nas miara \\(\\gamma\\) powinna spełniać\n\\[c(n) = \\int_0^1 \\rho^n \\gamma(d\\rho).\\] Zgodnie z\nLematem 2, koniecznym wystarczającym warunkiem na \njest, aby\n\\[\\begin{equation}\n        \\sum_{k=0}^{n} \\binom{n}{k} (-1)^k c(k + m) \\geq 0\n\\tag{50}\n\\end{equation}\\]\ndla\nwszystkich nieujemnych liczb całkowitych \\(n, m\\). Aby sprawdzić (50),\nustalmy \\(m, n\\), rozważmy, ciąg zbiorów \\(A_i\\) o\nliczebności \\(m + n\\), tak aby \\(g(A_i) \\0\\). Niech \\(A_i = B_i \\cup C_i\\),\ngdzie \\(|B_i| = m\\) \\(|C_i| = n\\). Wówczas \\(g(B_i) \\0\\) oraz\n\\(g(C_i) \\0\\), gdyż \\(B_i\\) \\(C_i\\) są podzbiorami \\(A_i\\). Stosując zasadę\nwłączeń wyłączeń,\n\\[\\pi \\{\\eta : \\eta \\equiv 1 \\text{ na } B_i, \\eta \\equiv 0 \\text{ na } C_i\\} =\n        \\sum_{F \\subseteq C_i} (-1)^{|F|} h(B_i \\cup F).\\] Istotnie, aby\nzobaczyć, że powyższe jest prawdą rozważmy\n\\(\\nu () = \\mu\\{ \\eta\\equiv 1 \\mbox{ na } B_i, \\}\\). Dla \\(x \\V\\) niech\n\\(A_x = \\{\\eta \\: : \\: \\eta(x)=1\\}\\). Wówczas\n\\[\\begin{multline}\n        \\pi \\{\\eta : \\eta \\equiv 1 \\text{ na } B_i, \\eta \\equiv 0 \\text{ na } C_i\\} =\n        \\nu \\left( \\bigcap_{x \\C_i} A_x^c \\right) =\\\\\n        \\int \\prod_{x \\C_i} (1-\\mathbf{1}_{A_x}(\\eta)) \\nu(\\mathrm{d}\\eta) =\n        \\int \\sum_{F \\subseteq C_i} (-1)^{|F|}\n            \\prod_{x \\F}\\mathbf{1}_{A_x}(\\eta) \\nu(\\mathrm{d}\\eta) \\\\\n            = \\sum_{F \\subseteq C_i} (-1)^{|F|} \\nu \\left( \\bigcap_{x \\F}A_x \\right)\n        \\sum_{F \\subseteq C_i} (-1)^{|F|} h(B_i \\cup F).\n\\end{multline}\\]\nPrzechodząc z \\(\\\\infty\\) mamy\n\\(h(B_i\\cup F) \\c(|B_i|+|F|)\\) stąd \\[\\lim_{\\\\infty}\n        \\pi \\{\\eta : \\eta \\equiv 1 \\text{ na } B_i, \\eta \\equiv 0 \\text{ na } C_i\\} =\n        \\sum_{k=0}^{n} \\binom{n}{k} (-1)^k c(k + m),\\] co z kolei daje\ndaje (50). Aby pokazać, że \\(\\gamma\\) jest szukaną przez nas\nmiarą połóżmy\n\\[\\pi^*(\\cdot) = \\int_0^1 \\mu_\\rho(\\cdot) \\gamma(d\\rho) \\quad \\text{oraz}\n        \\quad h^*() = \\pi^* \\{\\eta : \\eta \\equiv 1 \\text{ na } \\}.\\]\nWówczas \\(\\pi^*\\) jest rozkładem stacjonarnym ponieważ jest kombinacją\nwypukłą rozkładów stacjonarnych. Argumentując podobnie jak na początku\ndowodu pokazujemy, że \\(h*=V_th^*\\). Mamy\n\\[0 \\leq \\mu_\\rho(\\eta \\equiv 1 \\mbox{ na } ) -\\rho^{||}\n        = \\mathbf{E}_A[\\rho^{a_\\infty} - \\rho^{||}] \\leq g().\\]\nCałkując względem \\(\\gamma(\\mathrm{d}\\rho)\\) otrzymujemy\n\\[|h^*() - c(||)| \\leq g(),\\] co w połączeniu z\n(49) daje \\[|h^*() - h()| \\leq 2g().\\] Stosując\n\\(V_t\\) tego używając harmoniczności \\(V_t\\) zarówno dla \\(h\\), jak \n\\(h^*\\), otrzymujemy \\[|h^*() - h()| \\leq 2V_tg().\\] Rozważając teraz\n\\(t \\\\infty\\) używając Faktu 4, daje \\(h^* \\equiv h\\), zatem \\(\\pi^* = \\pi\\).\nZatem \\[\\pi = \\int_0^1 \\mu_\\rho \\gamma(\\mathrm{d}\\rho),\\] co kończy\ndowód. ","code":""}]
